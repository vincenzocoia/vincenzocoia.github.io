<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Vincenzo Coia</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 16 Mar 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>The squared error has friends, too!</title>
      <link>/post/20190316-ubc-sfu/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/post/20190316-ubc-sfu/</guid>
      <description>


&lt;p&gt;I was invited to the SFU/UBC Joint Seminar in Spring 2019 where I gave this talk.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;who-am-i&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Who am I?&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;quiz&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quiz&lt;/h1&gt;
&lt;div id=&#34;true-or-false&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;True or False:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The least squares estimator is derived by maximizing the likelihood.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;true-or-false-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;True or False:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The least squares estimator is derived by maximizing the likelihood.&lt;/li&gt;
&lt;li&gt;If errors are not Gaussian, we can’t use least squares to estimate our regression coefficients.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;true-or-false-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;True or False:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The least squares estimator is derived by maximizing the likelihood.&lt;/li&gt;
&lt;li&gt;If errors are not Gaussian, we can’t use least squares to estimate our regression coefficients.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Answers&lt;/strong&gt;: Both FALSE!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question:&lt;/h2&gt;
&lt;p&gt;When we fit a regression model (linear, kNN, random forest, etc.), what is the interpretation of the resulting model function / regression curve?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;question-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question:&lt;/h2&gt;
&lt;p&gt;When we fit a regression model (linear, kNN, random forest, etc.), what is the interpretation of the resulting model function / regression curve?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;: The mean of Y given X.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concept-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concept #1&lt;/h1&gt;
&lt;p&gt;We minimize the SSE in regression because it’s a proper scoring rule for the mean.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;On the board, lets:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Write ybar as a minimization problem.&lt;/li&gt;
&lt;li&gt;Extend to regression&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;concept-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concept #2&lt;/h1&gt;
&lt;p&gt;When doing regression, we ought to consider quantiles, too!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Consider Y = monthly expenditure (in $). Interpretation of quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;median:&lt;/li&gt;
&lt;li&gt;low-quantile:&lt;/li&gt;
&lt;li&gt;high-quantile:&lt;/li&gt;
&lt;li&gt;mean:&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Consider Y = monthly expenditure (in $). Interpretation of quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;median: There’s a 50-50 chance that you’ll have to pay more than this.&lt;/li&gt;
&lt;li&gt;low-quantile:&lt;/li&gt;
&lt;li&gt;high-quantile:&lt;/li&gt;
&lt;li&gt;mean:&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Consider Y = monthly expenditure (in $). Interpretation of quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;median: There’s a 50-50 chance that you’ll have to pay more than this.&lt;/li&gt;
&lt;li&gt;low-quantile: You’ll “at least” have to pay this much.&lt;/li&gt;
&lt;li&gt;high-quantile:&lt;/li&gt;
&lt;li&gt;mean:&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Consider Y = monthly expenditure (in $). Interpretation of quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;median: There’s a 50-50 chance that you’ll have to pay more than this.&lt;/li&gt;
&lt;li&gt;low-quantile: You’ll “at least” have to pay this much.&lt;/li&gt;
&lt;li&gt;high-quantile: You’ll “at most” have to pay this much.&lt;/li&gt;
&lt;li&gt;mean:&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Consider Y = monthly expenditure (in $). Interpretation of quantities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;median: There’s a 50-50 chance that you’ll have to pay more than this.&lt;/li&gt;
&lt;li&gt;low-quantile: You’ll “at least” have to pay this much.&lt;/li&gt;
&lt;li&gt;high-quantile: You’ll “at most” have to pay this much.&lt;/li&gt;
&lt;li&gt;mean: Multiply by &lt;code&gt;m&lt;/code&gt; to estimate total $ after &lt;code&gt;m&lt;/code&gt; months.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;concept-3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concept #3&lt;/h1&gt;
&lt;p&gt;Each quantile has its own proper scoring rule that we can use instead of the squared error.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;On the board:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Write median as an optimization problem&lt;/li&gt;
&lt;li&gt;Extend to generic quantile&lt;/li&gt;
&lt;li&gt;Extend to regression&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;p&gt;The “check function”:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/20190316-ubc-sfu_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;concept-4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Concept #4&lt;/h1&gt;
&lt;p&gt;Make a distributional assumption to reduce estimation uncertainty.&lt;/p&gt;
&lt;div id=&#34;univariate-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Univariate Estimation&lt;/h2&gt;
&lt;p&gt;If you have a univariate sample &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \ldots, Y_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Distributional Assumption?&lt;/th&gt;
&lt;th&gt;Estimation Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-estimation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Univariate Estimation&lt;/h2&gt;
&lt;p&gt;If you have a univariate sample &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \ldots, Y_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Distributional Assumption?&lt;/th&gt;
&lt;th&gt;Estimation Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;“sample versions”: ybar, s^2, &lt;code&gt;quantile()&lt;/code&gt;, …&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;MLE&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-setting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression setting&lt;/h2&gt;
&lt;p&gt;If you have a univariate sample &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \ldots, Y_n\)&lt;/span&gt; AND predictors:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Distributional Assumption?&lt;/th&gt;
&lt;th&gt;Estimation Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-setting-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression setting&lt;/h2&gt;
&lt;p&gt;If you have a univariate sample &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \ldots, Y_n\)&lt;/span&gt; AND predictors:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Distributional Assumption?&lt;/th&gt;
&lt;th&gt;Estimation Method&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Optimize scoring rule for desired quantity.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;MLE&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;return-of-the-quiz&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Return of the Quiz&lt;/h1&gt;
&lt;div id=&#34;return-of-the-quiz-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Return of the Quiz&lt;/h2&gt;
&lt;p&gt;Can we see why these are false?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The least squares estimator is derived by maximizing the likelihood.&lt;/li&gt;
&lt;li&gt;If errors are not Gaussian, we can’t use least squares to estimate our regression coefficients.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;time-left&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Time left?&lt;/h1&gt;
&lt;div id=&#34;time-left-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Time left?&lt;/h2&gt;
&lt;p&gt;Give two ways to estimate the conditional variance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hint&lt;/strong&gt;: Think about the definition of variance.&lt;/p&gt;
&lt;p&gt;Talk to your neighbour for 1 minute&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;p&gt;This talk was inspired by the activity generated by &lt;a href=&#34;https://vincenzocoia.github.io/20180218-mean/&#34;&gt;my blog post “The missing question in supervised learning”&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For proper scoring rules, see &lt;a href=&#34;https://www.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf&#34;&gt;Gneiting, T., and Raftery, A.E. (2007) “Strictly Proper Scoring Rules, Prediction, and Estimation”. Journal of the American Statistical Association, 102:477&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Factor Analysis</title>
      <link>/post/factor_analysis/</link>
      <pubDate>Mon, 21 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/post/factor_analysis/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Here&amp;#39;s a little tutorial on how to use the `factanal` function for Factor Analysis.

## Let&amp;#39;s make a data frame using actual factors:
set.seed(3847)
F1 &amp;lt;- rnorm(100)
F2 &amp;lt;- rnorm(100)
X1 &amp;lt;- F1 + F2 + rnorm(100)/5
X2 &amp;lt;- 4*F1 * rnorm(100)
X3 &amp;lt;- -6*F1 * rnorm(100)/4
X4 &amp;lt;- F2 + rnorm(100)
X5 &amp;lt;- 4*F2 - 0.5*F1 * rnorm(100)
dat &amp;lt;- data.frame(X1=X1, X2=X2, X3=X3, X4=X4, X5=X5)

## Let&amp;#39;s do a factor analysis with k=2 factors. There are two main ways you can do this.
k &amp;lt;- 2

## Method 1: via the data.
## Just indicate the data in the first argument, and specify the number of factors
##  in the `factors` argument.
## Note that loadings that are &amp;quot;essentially zero&amp;quot; are indicated as blanks.
factanal(dat, factors=k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## factanal(x = dat, factors = k)
## 
## Uniquenesses:
##    X1    X2    X3    X4    X5 
## 0.375 0.005 0.947 0.487 0.035 
## 
## Loadings:
##    Factor1 Factor2
## X1  0.790         
## X2  0.160   0.985 
## X3  0.119  -0.198 
## X4  0.715         
## X5  0.976  -0.110 
## 
##                Factor1 Factor2
## SS loadings      2.128   1.024
## Proportion Var   0.426   0.205
## Cumulative Var   0.426   0.630
## 
## Test of the hypothesis that 2 factors are sufficient.
## The chi square statistic is 0.93 on 1 degree of freedom.
## The p-value is 0.336&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Method 2: via the covariance matrix.
## Indicate the covariance matrix in the `covmat` argument. 
## Note: this method doesn&amp;#39;t have as many features, such as hypothesis testing,
##  built into it. But the results are the same.s
factanal(factors=k, covmat=cov(dat))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## factanal(factors = k, covmat = cov(dat))
## 
## Uniquenesses:
##    X1    X2    X3    X4    X5 
## 0.375 0.005 0.947 0.487 0.035 
## 
## Loadings:
##    Factor1 Factor2
## X1  0.790         
## X2  0.160   0.985 
## X3  0.119  -0.198 
## X4  0.715         
## X5  0.976  -0.110 
## 
##                Factor1 Factor2
## SS loadings      2.128   1.024
## Proportion Var   0.426   0.205
## Cumulative Var   0.426   0.630
## 
## The degrees of freedom for the model is 1 and the fit was 0.0097&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Another important thing to note is how to do *rotations*. There are two
##  built-in rotations available via the `rotations` argument -- they are
##  &amp;quot;none&amp;quot; and &amp;quot;varimax&amp;quot; (default). So the plain factor analysis results
##  without rotation is:
factanal(dat, factors=k, rotation = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## factanal(x = dat, factors = k, rotation = &amp;quot;none&amp;quot;)
## 
## Uniquenesses:
##    X1    X2    X3    X4    X5 
## 0.375 0.005 0.947 0.487 0.035 
## 
## Loadings:
##    Factor1 Factor2
## X1  0.784   0.101 
## X2          0.997 
## X3  0.151  -0.175 
## X4  0.712         
## X5  0.981         
## 
##                Factor1 Factor2
## SS loadings      2.106   1.046
## Proportion Var   0.421   0.209
## Cumulative Var   0.421   0.630
## 
## Test of the hypothesis that 2 factors are sufficient.
## The chi square statistic is 0.93 on 1 degree of freedom.
## The p-value is 0.336&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## You can extract the loadings vector by extracting the list component
##  entitled `loadings`. There&amp;#39;s a weird print call associated with the
##  object, but it&amp;#39;s really just a matrix, as the last call indicates.
fit &amp;lt;- factanal(dat, factors=k, rotation = &amp;quot;none&amp;quot;)
fit$loadings&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Loadings:
##    Factor1 Factor2
## X1  0.784   0.101 
## X2          0.997 
## X3  0.151  -0.175 
## X4  0.712         
## X5  0.981         
## 
##                Factor1 Factor2
## SS loadings      2.106   1.046
## Proportion Var   0.421   0.209
## Cumulative Var   0.421   0.630&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit$loadings[1:5, 1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         Factor1     Factor2
## X1  0.783885737  0.10136519
## X2 -0.009679362  0.99744995
## X3  0.151115775 -0.17503806
## X4  0.711852604  0.08077992
## X5  0.980910146  0.05733956&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Contour Plots</title>
      <link>/post/contour_plots/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/contour_plots/</guid>
      <description>


&lt;p&gt;This tutorial introduces contour plots, and how to plot them in &lt;code&gt;ggplot2&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;what-is-a-contour-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is a contour plot?&lt;/h1&gt;
&lt;p&gt;Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?&lt;/p&gt;
&lt;p&gt;The idea is to use &lt;em&gt;contour lines&lt;/em&gt;, which are curves that indicate a constant height.&lt;/p&gt;
&lt;p&gt;Imagine cutting the tops of the mountains off by removing all land above, say, 900 meters altitude. Then trace (on your map) the shapes formed by the new (flat) mountain tops. These curves are contour lines. Choose a differential such as 50 meters, and draw these curves for altitudes …800m, 850m, 900m, 950m, 1000m, … – the result is a &lt;strong&gt;contour plot&lt;/strong&gt; (or topographic map, if it’s a map).&lt;/p&gt;
&lt;p&gt;In general, contour plots are useful for functions of two variables (like a bivariate gaussian density).&lt;/p&gt;
&lt;p&gt;We’ll look at examples in the next section.&lt;/p&gt;
&lt;p&gt;Notes on contours:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They never cross.&lt;/li&gt;
&lt;li&gt;The steepest slope at a point is parallel to the contour line.&lt;/li&gt;
&lt;li&gt;They aren’t entirely ambiguous. For example, you can’t tell whether or not the mountains are actually mountains, or whether they’re holes/valleys! Sometimes you can add colour to indicate depth; other times (like in topographic maps) you can indicate elevation directly as numbers beside contour lines. Other times, this is not required, because the context makes it obvious.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;contour-plots-in-ggplot2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Contour plots in &lt;code&gt;ggplot2&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;There are two ways you can make contour plots in &lt;code&gt;ggplot2&lt;/code&gt; – but they’re both for quite different purposes.&lt;/p&gt;
&lt;div id=&#34;method-1-approximate-a-bivariate-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method 1: Approximate a bivariate density&lt;/h2&gt;
&lt;p&gt;This method approximates a bivariate density &lt;strong&gt;from data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;First, recall how this is done in the univariate case. A little kernel function (like a shrunken bell curve) is placed over each data point, and these are added together to get a density estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(373)
x &amp;lt;- rnorm(1000)
ggplot(data.frame(x=x), aes(x)) + 
    geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/contour_plots_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do the same thing to get a bivariate density, except with little bivariate kernel functions (like shrunken bivariate Gaussian densities). But, we can’t just simply put “density height” on the vertical axis – we need that for the second dimension. Instead, we can use contour plots.&lt;/p&gt;
&lt;p&gt;This is the contour plot that &lt;code&gt;ggplot2&lt;/code&gt;’s &lt;code&gt;geom_density2d()&lt;/code&gt; does: builds a bivariate kernel density estimate (based on data), then makes a contour plot out of it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- rnorm(1000)
ggplot(data.frame(x=x, y=y), aes(x, y)) + 
    geom_density2d()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/contour_plots_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on context (this is a density), we know that this is a “hill” and not a “hole”. If you were to start at some point at the “bottom” of the hill, the steepest way up would be perpendicular to the contours. The highest point on the hill is within the middle-most circle.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-2-general-contour-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method 2: General Contour Plots&lt;/h2&gt;
&lt;p&gt;You can also make contour plots that &lt;em&gt;aren’t&lt;/em&gt; a kernel density estimate (necessarily), using &lt;code&gt;geom_contour()&lt;/code&gt;. This is based off of &lt;strong&gt;any bivariate function&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basics&lt;/h3&gt;
&lt;p&gt;Suppose we want to make a contour plot of the bivariate function &lt;span class=&#34;math display&#34;&gt;\[f(x,y) = x^2 + sin(y)\]&lt;/span&gt; over the rectangle &lt;span class=&#34;math inline&#34;&gt;\(-2&amp;lt;x&amp;lt;2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-5&amp;lt;y&amp;lt;5\)&lt;/span&gt;. First, make a grid over the rectangle (it &lt;em&gt;must&lt;/em&gt; be a grid – &lt;code&gt;geom_contour()&lt;/code&gt; won’t work otherwise). Then, evaluate the function at each of the grid points. Put all this info into a single data frame with three columns (two for the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; coordinates, and one for the function evaluation). Then, indicate the function evaluation in &lt;code&gt;geom_contour()&lt;/code&gt; as the aesthetic &lt;code&gt;z&lt;/code&gt;, and the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; aesthetics are as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f &amp;lt;- function(x) x[1]^2 + sin(x[2])
x &amp;lt;- seq(-2, 2, length.out=100)
y &amp;lt;- seq(-5, 5, length.out=100)
dat &amp;lt;- expand.grid(x=x, y=y)  # Data frame of 100*100=10000 points.
dat$z &amp;lt;- apply(dat, 1, f)
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/contour_plots_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that &lt;code&gt;expand.grid&lt;/code&gt; is useful for making grids. It returns all pairs from the input vectors. But, this also means that it’s easy for the output to explode!&lt;/p&gt;
&lt;p&gt;Note that finer grids yield plots with higher accuracy. Here’s an example of a rough grid, whose contours are jagged:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f &amp;lt;- function(x) x[1]^2 + sin(x[2])
x &amp;lt;- seq(-2, 2, length.out=10)
y &amp;lt;- seq(-5, 5, length.out=10)
dat &amp;lt;- expand.grid(x=x, y=y) # Data frame of 10*10=100 points.
dat$z &amp;lt;- apply(dat, 1, f)
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/contour_plots_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-settings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Additional Settings&lt;/h3&gt;
&lt;p&gt;Here, we’ll look at colouring the plots, and adding more/less contours.&lt;/p&gt;
&lt;p&gt;Here’s another example, with the &lt;code&gt;volcano&lt;/code&gt; data (a matrix of altitudes for a volcano). If you’d like, first take a look at a 3D rendering of the volcano, by running the following code chunk in your R console after un-commenting the last two lines (code taken directly from &lt;code&gt;rgl&lt;/code&gt;’s &lt;code&gt;surface3d()&lt;/code&gt; documentation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(volcano)
z &amp;lt;- 2 * volcano        # Exaggerate the relief
x &amp;lt;- 10 * (1:nrow(z))   # 10 meter spacing (S to N)
y &amp;lt;- 10 * (1:ncol(z))   # 10 meter spacing (E to W)
zlim &amp;lt;- range(y)
zlen &amp;lt;- zlim[2] - zlim[1] + 1
colorlut &amp;lt;- terrain.colors(zlen) # height color lookup table
col &amp;lt;- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point
# open3d()
# surface3d(x, y, z, color = col, back = &amp;quot;lines&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Feel free to move the image around by clicking and dragging. Neat, eh?&lt;/p&gt;
&lt;p&gt;We’ll make a contour plot with this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- expand.grid(x=x, y=y)
dat$z &amp;lt;- as.vector(z)/2   # &amp;quot;De-exaggerate&amp;quot; the relief
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z)) +
    xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/contour_plots_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;528&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But, you can’t tell that the inner circles actually represent a hole (a caldera), not a peak. Let’s add colour by indicating the “variable” &lt;code&gt;..height..&lt;/code&gt; in the &lt;code&gt;colour&lt;/code&gt; aesthetic of &lt;code&gt;geom_cotour()&lt;/code&gt;, which will also indicate height as a legend:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- expand.grid(x=x, y=y)
dat$z &amp;lt;- as.vector(z)/2   # &amp;quot;De-exaggerate&amp;quot; the relief
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z, colour=..level..)) +
    xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank()) +
    scale_color_continuous(&amp;quot;Altitude&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/contour_plots_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can tell that the highest point is within the lightest blue area, to the left of the caldera.&lt;/p&gt;
&lt;p&gt;Now let’s add more contour lines, to get a better sense of the terrain. Do so by indicating the altitudes to make contours for via &lt;code&gt;breaks&lt;/code&gt;. Let’s make 5 unit spacing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- expand.grid(x=x, y=y)
dat$z &amp;lt;- as.vector(z)/2   # &amp;quot;De-exaggerate&amp;quot; the relief
ggplot(dat, aes(x, y)) +
    geom_contour(aes(z=z, colour=..level..),
                 breaks=seq(100, 200, by=5)) +
    xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
    theme(axis.text=element_blank(),
          axis.ticks=element_blank()) +
    scale_color_continuous(&amp;quot;Altitude&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/contour_plots_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although you can change the contours, it’s best practice to keep the (height) spacing between contour lines equal – otherwise, the contour plot becomes harder to read. In the above plot, for example, we know that crossing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; contour lines (that are either increasing or decreasing) results in &lt;span class=&#34;math inline&#34;&gt;\(5n\)&lt;/span&gt; units of elevation gain/loss, because the spacing between contours is always 5 units.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mixture distributions</title>
      <link>/post/mixture_distributions/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 +0000</pubDate>
      <guid>/post/mixture_distributions/</guid>
      <description>


&lt;p&gt;This tutorial introduces the concept of a &lt;em&gt;mixture distribution&lt;/em&gt;. We’ll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.&lt;/p&gt;
&lt;div id=&#34;intuition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intuition&lt;/h2&gt;
&lt;p&gt;Let’s start by looking at a basic experiment:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Flip a coin.&lt;/li&gt;
&lt;li&gt;If the outcome is heads, generate a &lt;code&gt;N(0,1)&lt;/code&gt; random variable. If the outcome is tails, generate a &lt;code&gt;N(4,1)&lt;/code&gt; random variable. We’ll let &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; denote the final result.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable with some distribution (spoiler: it’s a mixture distribution). Let’s perform the experiment 1000 times to get 1000 realizations of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and make a histogram to get a sense of the distribution &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; follows. To make sure the histogram represents an estimate of the density, we’ll make sure the area of the bars add to 1 (with the &lt;code&gt;..density..&lt;/code&gt; option).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(ggplot2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(44)
X &amp;lt;- numeric(0)
coin &amp;lt;- integer(0)
for (i in 1:1000) {
    coin[i] &amp;lt;- rbinom(1, size=1, prob=0.5)  # flip a coin. 0=heads, 1=tails.
    if (coin[i] == 0) {   # heads
        X[i] &amp;lt;- rnorm(1, mean=0, sd=1)
    } else {           # tails
        X[i] &amp;lt;- rnorm(1, mean=4, sd=1)
    }
}
(p &amp;lt;- qplot(X, ..density.., geom=&amp;quot;histogram&amp;quot;, bins=30))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/mixture_distributions_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s try to reason our way to figuring out the overall density. Keep in mind that this density (like all densities) is &lt;em&gt;one curve&lt;/em&gt;. We’ll say we’ve succeeded at finding the density if our density is close to the histogram.&lt;/p&gt;
&lt;p&gt;It looks like the histogram is made up of two normal distributions “superimposed”. These ought to be related to the &lt;code&gt;N(0,1)&lt;/code&gt; and &lt;code&gt;N(4,1)&lt;/code&gt; distributions, so to start, let’s plot these two Gaussian densities overtop of the histogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data.frame(X=X), aes(X)) +
    geom_histogram(aes(y=..density..), bins=30) +
    stat_function(fun=function(x) dnorm(x, mean=0, sd=1), 
                  mapping=aes(colour=&amp;quot;Heads&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=4, sd=1), 
                  mapping=aes(colour=&amp;quot;Tails&amp;quot;)) +
    scale_color_discrete(&amp;quot;Coin Flip&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/mixture_distributions_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, the two Gaussian distributions are in the correct location, and it even looks like they have the correct spread, but they’re too tall.&lt;/p&gt;
&lt;p&gt;Something to note at this point: the two curves plotted above are &lt;em&gt;separate (component) distributions&lt;/em&gt;. We’re trying to figure out the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; – which, again, is a single curve, and is estimated by the histogram. At this point, we only suspect that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is some combination of these two Gaussian distributions.&lt;/p&gt;
&lt;p&gt;So, why are the Gaussian curves too tall? Because each one represents the distribution &lt;em&gt;if we only ever flip either heads or tails&lt;/em&gt; (for example, the red distribution happens when we only ever flip heads). But since we flip heads half of the time, and tails half of the time, these probabilities (more accurately, densities) ought to be reduced by half. Let’s add these “semi” component distributions to the plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(p &amp;lt;- ggplot(data.frame(X=X), aes(X)) +
    geom_histogram(aes(y=..density..), bins=30) +
    stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5, 
                  mapping=aes(colour=&amp;quot;Heads&amp;quot;, linetype=&amp;quot;Semi&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=4, sd=1)*0.5, 
                  mapping=aes(colour=&amp;quot;Tails&amp;quot;, linetype=&amp;quot;Semi&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=0, sd=1), 
                  mapping=aes(colour=&amp;quot;Heads&amp;quot;, linetype=&amp;quot;Full&amp;quot;)) +
    stat_function(fun=function(x) dnorm(x, mean=4, sd=1), 
                  mapping=aes(colour=&amp;quot;Tails&amp;quot;, linetype=&amp;quot;Full&amp;quot;)) +
    scale_color_discrete(&amp;quot;Coin Flip&amp;quot;) +
    scale_linetype_discrete(&amp;quot;Distribution&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/mixture_distributions_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like they line up quite nicely!&lt;/p&gt;
&lt;p&gt;But these two curves are still separate – we need &lt;em&gt;one&lt;/em&gt; overall curve if we are to find the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. So we need to combine them somehow. It might look at first that we can just take the upper-most of the ‘semi’ curves (i.e., the maximum of the two), but looking in between the two curves reveals that the histogram is actually larger than either curve here. It turns out that the two ‘semi’ curves are &lt;em&gt;added&lt;/em&gt; to get the final curve:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5 + 
                      dnorm(x, mean=4, sd=1)*0.5,
                  mapping=aes(linetype=&amp;quot;Full&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/mixture_distributions_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The intuition behind adding the densities is that an outcome for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; comes from &lt;em&gt;both&lt;/em&gt; components, so both contribute some density.&lt;/p&gt;
&lt;p&gt;Even though the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is made up of two components, at the end of the day, it’s still overall just a random variable with some density. And like all densities, the density of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is just one curve. But, this density happens to be &lt;em&gt;made up of&lt;/em&gt; the components, as we’ll see next.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;general-scenario&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;General Scenario&lt;/h2&gt;
&lt;p&gt;The two normal distributions from above are called &lt;em&gt;component distributions&lt;/em&gt;. In general, we can have any number of these (not just two) to make a mixture distribution. And, instead of selecting the component distribution with coin tosses, they’re chosen according to some generic probabilities called the &lt;em&gt;mixture probabilities&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In general, here’s how we make a mixture distribution with &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; component Gaussian distributions with densities &lt;span class=&#34;math inline&#34;&gt;\(\phi_1(x), \ldots, \phi_K(x)\)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose one of the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; components, randomly, with mixture probabilities &lt;span class=&#34;math inline&#34;&gt;\(\pi_1, \ldots, \pi_K\)&lt;/span&gt; (which, by necessity, add to 1).&lt;/li&gt;
&lt;li&gt;Generate a random variable from the selected component distribution. Call the result &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: we can use more than just Gaussian component distributions! But this tutorial won’t demonstrate that.&lt;/p&gt;
&lt;p&gt;That’s how we &lt;em&gt;generate&lt;/em&gt; a random variable with a mixture distribution, but what’s its density? We can derive that by the law of total probability. Let &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; be the selected component number; then the component distributions are actually the distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;em&gt;conditional&lt;/em&gt; on the component number. We get: &lt;span class=&#34;math display&#34;&gt;\[ f_X\left(x\right) = \sum_{k=1}^{K} f_{X|C}\left(x \mid c\right) P\left(C=c\right) = \sum_{k=1}^{K} \phi_k\left(x\right) \pi_k. \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Notes:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The intuition described in the previous section matches up with this result. For &lt;span class=&#34;math inline&#34;&gt;\(K=2\)&lt;/span&gt; components determined by a coin toss &lt;span class=&#34;math inline&#34;&gt;\((\pi_1=\pi_2=0.5),\)&lt;/span&gt; we have &lt;span class=&#34;math display&#34;&gt;\[ f_X\left(x\right) = \phi\left(x\right)0.5 + \phi\left(x-4\right)0.5, \]&lt;/span&gt; which is the black curve in the previous plot.&lt;/li&gt;
&lt;li&gt;This tutorial works with univariate data. But mixture distributions can be multivariate, too. A &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-variate mixture distribution can be made by replacing the component distributions with &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;-variate distributions. Just be sure to distinguish between the dimension of the data &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the number of components &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;We &lt;em&gt;could&lt;/em&gt; just describe a mixture distribution by its density, just like we can describe a normal distribution by its density. But, describing mixture distributions by its &lt;strong&gt;component distributions&lt;/strong&gt; together with the &lt;strong&gt;mixture probabilities&lt;/strong&gt;, we obtain an excellent &lt;em&gt;interpretation&lt;/em&gt; of the mixture distribution. This interpretation is (it’s also called a &lt;em&gt;data generating process&lt;/em&gt;): (1) randomly choose a component, and (2) generate from that component. This interpretation is useful for cluster analysis, because the data clusters can be thought of as being generated by the component distributions, and the proportion of data in each cluster is determined by the mixture probabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;learning-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning Points&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A mixture distribution can be described by its mixing probabilities &lt;span class=&#34;math inline&#34;&gt;\(\pi_1, \ldots, \pi_K\)&lt;/span&gt; and component distributions &lt;span class=&#34;math inline&#34;&gt;\(\phi_1(x), \ldots, \phi_K(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A mixture distribution can also be described by a single density (like all continuous random variables).
&lt;ul&gt;
&lt;li&gt;This density is a single curve if data are univariate; a single “surface” if the data are bivariate; and higher dimensional surfaces if the data are higher dimensional.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;To get the density from the mixing probabilities and component distributions, we can use the formula indicated in the above section (based on the law of total probability).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BAIT 509 Class Meeting 05</title>
      <link>/post/cart/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/cart/</guid>
      <description>


&lt;div id=&#34;outline&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Outline&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Classification and Regression Trees&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Some problems (or at least potential problems) with the local methods introduced last time:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They lack interpretation.
&lt;ul&gt;
&lt;li&gt;It’s not easy to say how the predictors influence the response from the fitted model.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;They typically require a data-rich situation so that the estimation variance is acceptable, without compromising the estimation bias.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ll look at classification and regression trees as another method (sometimes called CART). CARTs are not necessarily better, and in fact, tend to be poor competitors to other methods – but ensemble extensions of these tend to perform well (a topic for next Monday).&lt;/p&gt;
&lt;p&gt;Our setting this time is the usual: we have a response &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (either categorical or numeric), and hope to predict this response using &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; predictors &lt;span class=&#34;math inline&#34;&gt;\(X_1,\ldots,X_p\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When the response is categorical, we aim to estimate the mode and take that as our prediction.&lt;/li&gt;
&lt;li&gt;When the response is numeric, we aim to estimate the mean, and take that as our prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stumps-a-preliminary-concept&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Stumps: A preliminary concept&lt;/h1&gt;
&lt;div id=&#34;what-are-they&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What are they?&lt;/h2&gt;
&lt;p&gt;Let’s say I get an upset stomach once in a while, and I suspect certain foods might be responsible. My response and predictors are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;: sick or not sick (categorical)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;: amount of eggs consumed in a day.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;: amount of milk consumed in a day, in liters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You make a food diary, and record the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Eggs&lt;/th&gt;
&lt;th&gt;Milk&lt;/th&gt;
&lt;th&gt;Sick?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.6&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.7&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;(Example from Mark Schmidt’s CPSC 340)&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;decision/classification stump&lt;/strong&gt; is a decision on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; based on the value of &lt;em&gt;one&lt;/em&gt; of the predictors. You can make one by doing the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose a predictor (in this case, Milk or Eggs?)&lt;/li&gt;
&lt;li&gt;Choose a cutpoint on that predictor:
&lt;ul&gt;
&lt;li&gt;Subset the data having that predictor &lt;em&gt;less than&lt;/em&gt; that cutpoint, and take the mode as your decision/classification.&lt;/li&gt;
&lt;li&gt;Subset the data having that predictor &lt;em&gt;greater than&lt;/em&gt; that cutpoint, and take the mode as your decision/classification.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This will get you a decision/classification stump, like so:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;cm05-trees_files/stump.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;(Image attribution: Hyeju Jang, DSCI 571)&lt;/p&gt;
&lt;p&gt;The same idea applies in the regression case, except we take the average of the subsetted data, and choose the best one according to the mean squared error.&lt;/p&gt;
&lt;p&gt;Some things to note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A decision on a numeric variable is always made based on a threshold (either less than or greater than). Although it makes sense to talk about a stump based on a more complicated decision, allowing for more complicated decisions would increase the search space, and would make fitting these models impractical. Plus, sometimes a more complicated decision can be broken down into several single-threshold decisions.&lt;/li&gt;
&lt;li&gt;It follows that decisions are always binary. Again, although it makes sense to talk about a stump that has more than two options, we can usually write these as a tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-best-stumps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the best stumps&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: What is the error of the above decision stump? If we decide to make a prediction without a decision stump (or any predictors at all), what would the best prediction be, and what’s the error?&lt;/p&gt;
&lt;p&gt;We have many stumps we can choose from – which one is best?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The one that gives the least error.
&lt;ul&gt;
&lt;li&gt;Tends to have a harder time disambiguating several options.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The Gini and Entropy criteria are two of the more commonly used measures.
&lt;ul&gt;
&lt;li&gt;These are measures of &lt;strong&gt;purity&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Regression&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The one that gives the least mean squared error.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;partitioning-of-predictor-space&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Partitioning of predictor space&lt;/h1&gt;
&lt;p&gt;How does a stump partition the predictor space?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trees&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Trees&lt;/h1&gt;
&lt;p&gt;Notice that a tree is just a bunch of stumps!&lt;/p&gt;
&lt;p&gt;This means:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. For numeric predictors, decisions can only involve one threshold.
2. Decisions are always binary.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;optimal-tree-computationally-infeasible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimal tree – computationally infeasible&lt;/h2&gt;
&lt;p&gt;Ideally, we’d examine the error from every possible regression tree, and choose the best one. But there are far too many trees possible! We turn to &lt;em&gt;recursive binary splitting&lt;/em&gt; as the next best thing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recursive-binary-splitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recursive Binary Splitting&lt;/h2&gt;
&lt;p&gt;The idea to making a tree is to take a &lt;strong&gt;greedy&lt;/strong&gt; approach: keep adding decision stumps, each time choosing the best stump.&lt;/p&gt;
&lt;p&gt;This is called “greedy” because this method may not end up with the best tree – it’s not computationally feasible to look many steps ahead to see whether a different path is more feasible.&lt;/p&gt;
&lt;p&gt;Adding “Eggs &amp;lt; 1” to the above stump will be the next best iteration – in this case, resulting in 100% prediction accuracy (on the training data). Notice that every time we add a stump, the error on the training set decreases.&lt;/p&gt;
&lt;p&gt;We keep iterating until we reach some &lt;strong&gt;stopping criterion&lt;/strong&gt;, often based on things such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tree depth&lt;/li&gt;
&lt;li&gt;Number of observations in a leaf becomes too small&lt;/li&gt;
&lt;li&gt;Error is not reduced significantly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Questions&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How does “tree size” affect the bias-variance tradeoff?&lt;/li&gt;
&lt;li&gt;How can you choose these parameters?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pruning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pruning&lt;/h2&gt;
&lt;p&gt;Sometimes, a stopping criterion may cause the tree to stop growing prematurely. For example, perhaps we need an insignificant stump to form before getting a more significant one.&lt;/p&gt;
&lt;p&gt;To avoid this, we take the approach of growing an overly large (overfit) tree, and then pruning it. We won’t get into details of how the tree is pruned back, but one technique is called &lt;em&gt;cost complexity pruning&lt;/em&gt;. The general idea is to control a tuning parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to control how much pruning is done – it can be chosen by cross-validation, or the validation set approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercise&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Exercise&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Coding a decision tree with &lt;code&gt;tree::tree()&lt;/code&gt; in R, using the &lt;a href=&#34;https://raw.githubusercontent.com/vincenzocoia/BAIT509/master/assessments/assignment1/data/titanic.csv&#34;&gt;titanic data&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;predict()&lt;/code&gt; and &lt;code&gt;plot()&lt;/code&gt;, and “S3 generics” in R.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-and-regression-trees-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classification and Regression Trees: in R&lt;/h1&gt;
&lt;p&gt;To fit classification and regression trees in R, we use the package &lt;code&gt;tree&lt;/code&gt; and the function &lt;code&gt;tree()&lt;/code&gt;, which works similarly to &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;loess()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(tree))
suppressPackageStartupMessages(library(tidyverse))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- tree(Sepal.Width ~ ., data=iris)
summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Regression tree:
## tree(formula = Sepal.Width ~ ., data = iris)
## Variables actually used in tree construction:
## [1] &amp;quot;Petal.Length&amp;quot; &amp;quot;Sepal.Length&amp;quot; &amp;quot;Petal.Width&amp;quot; 
## Number of terminal nodes:  10 
## Residual mean deviance:  0.06268 = 8.776 / 140 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.60560 -0.16780  0.03182  0.00000  0.16280  0.63180&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit)  # Plot the tree, without labels
text(fit)  # Add labels to the tree&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/cart_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Messy – would need to modify the predictor names). Make predictions with &lt;code&gt;predict&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit, newdata=iris) %&amp;gt;% 
    head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2        3        4        5        6 
## 3.635294 3.273913 3.273913 3.273913 3.273913 3.635294&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to control the depth of the tree, use the &lt;code&gt;tree.control&lt;/code&gt; function in the &lt;code&gt;control&lt;/code&gt; argument. Arguments of &lt;code&gt;tree.control&lt;/code&gt; that are relevant are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mindev&lt;/code&gt;: The minimum error reduction acceptable before stopping the tree growth.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;minsize&lt;/code&gt;: The minimum number of observations required in a node in order for a tree to keep growing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s fit a tree to the max, and check its MSE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitfull &amp;lt;- tree(Sepal.Width ~ ., data=iris, 
                control=tree.control(nrow(iris), 
                                     mindev=0, minsize=2))
mean((predict(fitfull) - iris$Sepal.Width)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0008666667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can investigate the pruning of a tree via cross validation using the &lt;code&gt;cv.tree&lt;/code&gt; function. Specify &lt;code&gt;FUN=prune.misclass&lt;/code&gt; if you want to prune based on classification error instead of purity measurements, for classification. It returns a list, where the important components are named &lt;code&gt;&amp;quot;size&amp;quot;&lt;/code&gt; (number of terminal nodes) and &lt;code&gt;&amp;quot;dev&amp;quot;&lt;/code&gt; (the error). Let’s plot those:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(4)
fitfull_cv &amp;lt;- cv.tree(fitfull)
plot(fitfull_cv$size, log(fitfull_cv$dev))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/cart_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The x-axis represents the number of terminal nodes present in the subtree. The y-axis is the cross-validation error for that subtree.&lt;/p&gt;
&lt;p&gt;From the plot, it looks like it’s best to prune the tree to have approximately 10 terminal nodes. Use &lt;code&gt;prune.tree(fitfull, best=10)&lt;/code&gt; to prune the tree to 10 terminal nodes.&lt;/p&gt;
&lt;p&gt;*Note: if you encounter an error running &lt;code&gt;prune.tree(fitfull, best=10)&lt;/code&gt;, it’s not a true error (I believe it’s only an error to the &lt;code&gt;print&lt;/code&gt; call, which is called by default). Wrap the code with &lt;code&gt;try&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_pruned &amp;lt;- try(prune.tree(fitfull, best=10))
plot(fit_pruned)
text(fit_pruned)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/cart_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BAIT 509 Class Meeting 07</title>
      <link>/post/ensembles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/ensembles/</guid>
      <description>


&lt;div id=&#34;motivation-why-ensembles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation: why ensembles?&lt;/h1&gt;
&lt;p&gt;Because a classification or regression tree alone tends to be a poor competitor against other machine learning methods. In particular, they tend to be sensitive to the data: if fit to a separate training set, a completely different tree is prone to being fit. This is an embodiment of &lt;em&gt;high variance&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Consider the hypothetical situation: collect &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; data sets (of equal size), and fit a tree to all &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; of them. These &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; models are called an &lt;strong&gt;ensemble&lt;/strong&gt;. Then if we want to predict on a new case (i.e., we’ve observed the predictors but not the response), take the average predictions of the &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; trees in the case of regression, or the popular vote of the &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; trees in the case of classification. This will reduce variance.&lt;/p&gt;
&lt;p&gt;But collecting &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; data sets is not practical, and splitting our single data set into &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; parts would not be a substitute, because we’d only be skyrocketing the variance of each tree before then reducing it in the ensemble.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bagging-bootstrap-aggregating&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bagging (= Bootstrap Aggregating)&lt;/h1&gt;
&lt;p&gt;We can emulate the above hypothetical situation with a technique called the &lt;strong&gt;bootstrap&lt;/strong&gt;. If your data set has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations, then we can draw (for all intents and purposes) any number &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; of data sets we like. To generate one data set, just randomly sample &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations &lt;strong&gt;with replacement&lt;/strong&gt;, and ta-da!&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; data sets are related in some sense, so are not as good as having &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; &lt;em&gt;independent&lt;/em&gt; data sets. But it still gives us something useful – in fact, tremendously useful. Go ahead and fit a tree on each data set, and combine the results – that’s bagging in a nutshell.&lt;/p&gt;
&lt;p&gt;Note that we deliberately tend to &lt;strong&gt;overfit&lt;/strong&gt; each tree in the ensemble, to get trees with low bias and high variance – the variance of which will be reduced in the ensemble.&lt;/p&gt;
&lt;p&gt;This concept of bootstrap is very widespread – it’s not just used for trees, and not even just for machine learning. But for BAIT 509, trees are the only context you’ll see bootstrap in.&lt;/p&gt;
&lt;div id=&#34;size-of-b&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Size of &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Note that we can’t overfit by increasing &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, because this just results in new data sets being generated – not fitting more and more models to a single data set. The error (MSE or error rate) will drop as &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; increases, until it reaches a stable point where it no longer drops. Once this point is reached, increasing &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; does not do us much good. This is a common approach for choosing &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; – no need for cross validation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;out-of-bag-oob-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Out of Bag (OOB) error&lt;/h2&gt;
&lt;p&gt;With bagging comes a unique opportunity to calculate the out-of-sample/test error &lt;em&gt;without having to partition the data&lt;/em&gt; into training and test sets (either through the validation set method, or cross validation). Here’s how.&lt;/p&gt;
&lt;p&gt;Remember how we obtain a single bootstrap data set: sample with replacement &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times. This means that, for each bootstrap sample, there will be some observations that were left out. These are called &lt;strong&gt;out-of-bag&lt;/strong&gt; (OOB) observations, and naturally form a test set!&lt;/p&gt;
&lt;p&gt;Now, obtain an estimate of generalization error (such as MSE or error rate) like so: for each observation in your full data set, consider the trees for which this observation is OOB, and use those to form an ensemble prediction. Compare this against the true value to get the error for this observation. Repeat for all observations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predictor-importance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictor Importance&lt;/h2&gt;
&lt;p&gt;We can use ensembles to determine the &lt;em&gt;importance&lt;/em&gt; of certain predictors over others. Recall that the addition of a stump to a tree reduces the training error. We can set up a “points system”, where a reduction in MSE is “awarded” to the predictor responsible for the stump. Do this for all nodes in a tree to come up with a final score for each predictor – the ones with the largest scores are most important.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Random Forests&lt;/h1&gt;
&lt;p&gt;One problem with Bagging is that the trees in the ensemble tend to be &lt;em&gt;correlated&lt;/em&gt; – that is, they share similarities.&lt;/p&gt;
&lt;p&gt;Random forests attempt to fix this problem by modifying how a single tree in the ensemble is grown. Recall that, when making a new stump to grow a tree, we choose one predictor out of the total &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; predictors. The idea behind random forests is to &lt;em&gt;restrict this choice to some random subset&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; predictors out of the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. A new batch of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; predictors is selected each time a stump is to be made.&lt;/p&gt;
&lt;p&gt;The result is an ensemble of trees that look “more random” – they are said to be &lt;em&gt;decorrelated&lt;/em&gt;. This prevents any one predictor from “dominating” the ensemble. And because the trees are less related, combining their predictions results in an overall better result.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion-questions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Bagging is a special case of random forests under which case?&lt;/li&gt;
&lt;li&gt;What are the hyperparameters we can control for random forests?&lt;/li&gt;
&lt;li&gt;Suppose you have the following paired data of &lt;code&gt;(x,y)&lt;/code&gt;: (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;(1,0), (1,2), (1,5)&lt;/li&gt;
&lt;li&gt;(1,2), (2,0)&lt;/li&gt;
&lt;li&gt;(1,2), (1,2), (1,5)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)?&lt;/li&gt;
&lt;li&gt;You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases?
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Regression: your trees make the following four predictions: 1,1,3,3.&lt;/li&gt;
&lt;li&gt;Classification: your trees make the following four predictions: “A”, “A”, “B”, “C”.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;boosting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Boosting&lt;/h1&gt;
&lt;p&gt;Boosting is another method, different from random forests and bagging, but still involves combining predictions of an ensemble.&lt;/p&gt;
&lt;p&gt;The details are beyond the scope of this course, so we will explain the main ideas. If you truly want a more comprehensive treatment, I suggest reading &lt;a href=&#34;http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/&#34;&gt;this Kaggle blog post&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;basic-boosting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic boosting&lt;/h2&gt;
&lt;p&gt;(Also see the “Motivation” part of the above Kaggle blog – this part is not overly technical).&lt;/p&gt;
&lt;p&gt;Let’s look at a simple two-tree boosting ensemble for regression.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit a tree to your data&lt;/li&gt;
&lt;li&gt;Compute the residuals (actual minus prediction).&lt;/li&gt;
&lt;li&gt;Fit a second tree &lt;em&gt;to the residuals&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To make a prediction on a new observation, do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Feed the predictors into the first tree to get a “preliminary” prediction.&lt;/li&gt;
&lt;li&gt;Feed the predictors into the second tree to get an &lt;em&gt;adjustment&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Obtain a final prediction by adding the adjustment to the preliminary prediction.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This second tree captures patterns in the data that the first tree missed, which is why boosting is so useful.&lt;/p&gt;
&lt;p&gt;Boosting, then, is a continuation of this, fitting trees in sequence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-conglomerate-of-weak-learners&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A conglomerate of “weak learners”&lt;/h2&gt;
&lt;p&gt;Boosting gradually improves predictions by learning on residuals. Because of this, there is no need to build a “strong” model for each iteration – i.e., one that does well at prediction.&lt;/p&gt;
&lt;p&gt;Instead, we deliberately build &lt;em&gt;weak models&lt;/em&gt; to slowly get at the structure underyling the data. We therefore build low-depth trees for each member of the ensemble.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boosting-for-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boosting for Classification&lt;/h2&gt;
&lt;p&gt;The adjustments made here are less interpretable, but do follow a similar logic. Instead of learning on residuals, a consecutive tree leans on classes &lt;em&gt;re-weighted&lt;/em&gt; so that observations that are incorrectly classified get a higher weight. This is called &lt;strong&gt;adaboost&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learning-rate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning Rate&lt;/h2&gt;
&lt;p&gt;It turns out that we obtain a more powerful prediction if we &lt;em&gt;slow down&lt;/em&gt; the “rate of learning”. We introduce a “rate of learning” hyperparameter &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; between 0 and 1. Predictions from trees are multiplied by this amount before adjusting the prediction from the previous tree.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ensembles-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ensembles in R&lt;/h1&gt;
&lt;div id=&#34;randomforest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;randomForest&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We use the &lt;code&gt;randomForest&lt;/code&gt; package and the &lt;code&gt;randomForest&lt;/code&gt; function in R to implement random forests (and thus also bagging for classification and regression trees). The syntax follows R’s regression paradigm with &lt;code&gt;randomForest(response~predictors, data)&lt;/code&gt;. Let’s see an example with the &lt;code&gt;mtcars&lt;/code&gt; data (a default data frame in R), predicting &lt;code&gt;mpg&lt;/code&gt; (a regression problem):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(randomForest))
set.seed(40)
my_fit &amp;lt;- randomForest(mpg ~ ., data=mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;.&lt;/code&gt; stands for “all other variables in the data frame”.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;plot&lt;/code&gt; function is useful for giving a quick-and-dirty plot of error vs. &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(my_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/ensembles_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s stability after around &lt;span class=&#34;math inline&#34;&gt;\(B=200\)&lt;/span&gt; trees.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning!&lt;/strong&gt; The &lt;code&gt;predict&lt;/code&gt; function works differently than usual. Usually, the following two outputs would be the same:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yhat1 &amp;lt;- predict(my_fit)
yhat2 &amp;lt;- predict(my_fit, newdata=mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But they’re not. Take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(yhat1, yhat2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/ensembles_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;288&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What’s going on here? It turns out &lt;code&gt;predict(my_fit)&lt;/code&gt; &lt;em&gt;without&lt;/em&gt; specifying &lt;code&gt;newdata&lt;/code&gt; gives the &lt;em&gt;out of bag predictions&lt;/em&gt;, whereas the entire ensemble is used to make predictions when the &lt;code&gt;newdata&lt;/code&gt; argument is specified. So the OOB error and training errors, respectively, are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean((yhat1 - mtcars$mpg)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.655269&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean((yhat2 - mtcars$mpg)^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.49721&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are two key parameters you can change in the &lt;code&gt;randomForest&lt;/code&gt; function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mtry&lt;/code&gt;: The number of predictors to sample at every stump iteration.
&lt;ul&gt;
&lt;li&gt;Set equal to the total number of predictors to perform bagging.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ntree&lt;/code&gt;: The number of trees to fit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;boosting-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boosting&lt;/h2&gt;
&lt;p&gt;Boosting is not on your Assignment 2. For your reference, I’ll indicate some useful implementations here.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For regression, you can use the &lt;code&gt;gbm&lt;/code&gt; function from the &lt;code&gt;gbm&lt;/code&gt; R package to do boosting. But you can’t do classification beyond 2 classes.&lt;/li&gt;
&lt;li&gt;For classification, I recommend the &lt;code&gt;AdaBoostClassifier&lt;/code&gt; from the &lt;code&gt;sklearn.ensemble&lt;/code&gt; library.
&lt;ul&gt;
&lt;li&gt;This library also contains &lt;code&gt;RandomForestClassifier&lt;/code&gt; for random forest classification.&lt;/li&gt;
&lt;li&gt;For plain decision trees (again for classification), the &lt;code&gt;sklearn.tree&lt;/code&gt; library has a &lt;code&gt;DecisionTreeClassifier&lt;/code&gt; method.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BAIT 509 Class Meeting 09</title>
      <link>/post/svm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/svm/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Maximal Margin Classifier, and hyperplanes&lt;/li&gt;
&lt;li&gt;Support Vector Classifiers (SVC)&lt;/li&gt;
&lt;li&gt;Support Vector Machines (SVM)&lt;/li&gt;
&lt;li&gt;Extensions to multiple classes&lt;/li&gt;
&lt;li&gt;SVM’s in python&lt;/li&gt;
&lt;li&gt;Feedback on Assignment 1&lt;/li&gt;
&lt;li&gt;Lab: work on Assignment 3&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The setup&lt;/h1&gt;
&lt;p&gt;Today, we’ll dicuss a new method for &lt;strong&gt;binary classification&lt;/strong&gt; – that is, classification when there are two categories. The method is called &lt;strong&gt;Support Vector Machines&lt;/strong&gt;. We’ll build up to it by considering two special cases:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The Maximal Margin Classifier (too restrictive to use in practice)&lt;/li&gt;
&lt;li&gt;The Support Vector Classifier (linear version of SVM)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ll demonstrate concepts when there are two predictors, because it’s more difficult to visualize in higher dimensions. But concepts generalize.&lt;/p&gt;
&lt;p&gt;Let’s start by loading some useful R packages to demonstrate concepts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(tidyverse))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::opts_chunk$set(fig.width=6, fig.height=3, fig.align=&amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;maximal-margin-classifier&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Maximal Margin Classifier&lt;/h1&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Consider the following two-predictor example. The response can take on one of two categories: “A” or “B”. Also consider the three classifiers, where above a line we predict “B”, and below, we predict “A”:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/svm_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each line perfectly classifies the training data – which one would you prefer, and why?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Method&lt;/h2&gt;
&lt;p&gt;The Maximal Margin Classifier only applies when the two groups can be perfectly separated on the training set by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a dividing line (if we have 2 predictors),&lt;/li&gt;
&lt;li&gt;a dividing plane (if we have 3 predictors),&lt;/li&gt;
&lt;li&gt;or in general, we need a &lt;strong&gt;dividing hyperplane&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We choose the line/hyperplane so that the observations closest to the line/hyperplane are as far away as possible. This minimizes the chance that a new observation will be misclassified.&lt;/p&gt;
&lt;p&gt;We can say this another way. Notice that, for any given line, we can define the “widest slab” before touching an observation. Here are the widest slabs for the above cases:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/svm_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Maximal Marginal Classifier &lt;em&gt;chooses the line whose slab width is maximal&lt;/em&gt;. Half the “slab width” is called the &lt;strong&gt;margin&lt;/strong&gt;, so this classifier &lt;strong&gt;maximizes the margin&lt;/strong&gt;. There are algorithms to do this maximization.&lt;/p&gt;
&lt;p&gt;With this in mind, we can order the above three lines from best to worst: Line 2 is the worst, and Line 3 is the best.&lt;/p&gt;
&lt;p&gt;Notice that there are only three observations that define the slab for Line 3 – these are called &lt;strong&gt;support vectors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But:&lt;/strong&gt; a perfect linear classification almost never happens with real data, but this is an important setup before moving on to support vector machines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-predictions-and-confidence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Making Predictions, and Confidence&lt;/h2&gt;
&lt;p&gt;We can classify new points as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\((x1,x2)\)&lt;/span&gt; lies above the line, predict “B”.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\((x1,x2)\)&lt;/span&gt; lies below the line, predict “A”.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/svm_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But, we need a way to automate this, since orientation is not always natural with a hyperplane (for example, we have to switch to “left and right” if the dividing line is vertical).&lt;/p&gt;
&lt;p&gt;The idea here is to write the equation of the line/hyperplane as &lt;span class=&#34;math display&#34;&gt;\[ \beta_0 x_1 + \beta_1 x_2 + \beta_3 = 0. \]&lt;/span&gt; We can then classify based on the sign of the left-hand-side of the equation. Note that this can be written in the usual &lt;code&gt;y=mx+b&lt;/code&gt; format as &lt;span class=&#34;math display&#34;&gt;\[ x_2 = \left(-\frac{\beta_0}{\beta_1}\right) x_1 + \left(- \frac{\beta_3}{\beta_1}\right), \]&lt;/span&gt; but this is only useful for plotting.&lt;/p&gt;
&lt;p&gt;For the above classifier, the line is &lt;span class=&#34;math display&#34;&gt;\[ 0.59 x_1 - 0.66 x_2 + 0.46 = 0, \]&lt;/span&gt; and we classify “B” whenever &lt;span class=&#34;math display&#34;&gt;\[ 0.59 x_1 - 0.66 x_2 + 0.46 &amp;lt; 0, \]&lt;/span&gt; and “A” whenever &lt;span class=&#34;math display&#34;&gt;\[ 0.59 x_1 - 0.66 x_2 + 0.46 &amp;gt; 0. \]&lt;/span&gt; You can figure out which is which by just trying a point that you know the classification of.&lt;/p&gt;
&lt;p&gt;This equation also gives us a measure of &lt;strong&gt;confidence&lt;/strong&gt; in our classification – the further the magnitude of the left-hand-side is from 0, the further away from the separating hyperplane it is.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;support-vector-classifiers&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Support Vector Classifiers&lt;/h1&gt;
&lt;p&gt;Support Vector Classifiers (or SVC) are more realistic in the sense that they don’t require perfect separability of the data.&lt;/p&gt;
&lt;p&gt;Consider now the following data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/svm_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A SVC fits a line (or in general, a hyperplane) that “best” separates the data in some way. The idea, as before, is to &lt;em&gt;choose the line that results in the biggest margin&lt;/em&gt;. The only problem is, there’s no such thing as a maximal margin for a given line anymore. Let’s work on defining that.&lt;/p&gt;
&lt;p&gt;Here’s an example of a line and a margin (not necessarily a “maximal” margin – just some random margin that I chose). The dividing line is the middle one, so above this line is a “B” prediction. We’ll call the upper and lower lines the “margin boundaries”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/svm_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The idea is to calculate a &lt;strong&gt;total penalty&lt;/strong&gt; associated with each observation with this line-margin combination, like so:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Observations that are both correctly classified and outside of the “slab” do not receive any penalty.&lt;/li&gt;
&lt;li&gt;Observations that are correctly classified, but within the margin, receive a penalty equal to the &lt;em&gt;proportion of the way through the margin&lt;/em&gt; they are.
&lt;ul&gt;
&lt;li&gt;This means that observations lying on the classification boundary receive a penalty of 1, because they are entirely one margin width away from its margin boundary.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Step (2) can be generalized to observations that are misclassified – they receive a penalty equal to the number of margin widths they are away from their margin boundary.
&lt;ul&gt;
&lt;li&gt;This means that an “A” that’s on B’s margin boundary will receive a penalty of 2.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The numbered observations in the above plot receive a penalty:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Penalty between 0 and 1: observations 2,3,5&lt;/li&gt;
&lt;li&gt;Penalty between 1 and 2: observation 6&lt;/li&gt;
&lt;li&gt;Penalty greater than 2: 1 and 4.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Add up the penalties to obtain a total penalty.&lt;/p&gt;
&lt;p&gt;If we &lt;em&gt;choose&lt;/em&gt; a maximum allowable total penalty, say &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, then for any line, there’s a set of margin widths that result in a penalty less than &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. The maximal margin for that line is the biggest margin.&lt;/p&gt;
&lt;p&gt;Again, the algorithm chooses the line that has the &lt;em&gt;biggest maximal margin&lt;/em&gt;, for a given total penalty &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that, for the above plot, observations 1-6 are called &lt;strong&gt;support vectors&lt;/strong&gt;, because they are the only ones to contribute to a penalty.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-class-exercises&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;In-class exercises&lt;/h1&gt;
&lt;p&gt;Consider the following data, decision boundary, and margin boundaries.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/svm_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Construct the decision rule according to this classification boundary. How would you classify a new observation that has &lt;span class=&#34;math inline&#34;&gt;\(x_1=6\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2=10\)&lt;/span&gt;?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;If &lt;code&gt;x1&amp;lt;7.5&lt;/code&gt;, then “A”. Else “B”. For the above example, since &lt;code&gt;x1=6&amp;lt;7.5&lt;/code&gt;, we would classify “A”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What size is the margin here?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The margin width is 2 units.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which observations receive a penalty? Which observations are the support vectors?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The observations that receive a penalty are 6,7,8,9,10. These are also the support vectors, by definition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is the total penalty here?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The penalty, added in order of observations 6-10, is 0.25+0.25+1.25+1.75+2.25 = 5.75.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Can I choose a bigger margin if my total allowable penalty is 6?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Yes – increasing the margin will eventually lead to an increase in penalty, which is allowable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Are the data separable? If so, what are the support vectors?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;The data are separable – as such, we can apply a maximal margin classifier to it. The support vectors would be 4,8,9.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;support-vector-machines&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Support Vector Machines&lt;/h1&gt;
&lt;p&gt;Quite often, a linear boundary is not useful for classification. Take the below extreme example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/svm_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Recall in linear regression, we can fit other shapes besides lines by transforming the predictors, such as adding powers of the predictor to get polynomials.&lt;/p&gt;
&lt;p&gt;We can do this to get non-linear decision boundaries on the original predictor space, but it’s usually quite computationally expensive. The way to fix the problem is beyond the scope of this course, but the idea is to use &lt;strong&gt;kernel functions&lt;/strong&gt;. The kernel function typically has a hyperparameter associated with it. Two of the most popular examples:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The polynomial kernel, with hyperparameter = the degree of the polynomial.&lt;/li&gt;
&lt;li&gt;The radial kernel, which has hyperparameter typically denoted “gamma” (a positive number).
&lt;ul&gt;
&lt;li&gt;Uses nearby training data for classification, where larger values of gamma allow for data to be further.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that SVC and Maximum Margin Classification are special cases of SVM.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-class-prediction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multi-class Prediction&lt;/h1&gt;
&lt;p&gt;If there are more than two classes to predict, we can use one of two approaches to do the classification. Suppose there are &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; categories.&lt;/p&gt;
&lt;div id=&#34;approach-1-one-vs-one-or-all-pairs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Approach 1: One-vs-one, or all-pairs&lt;/h2&gt;
&lt;p&gt;This approach fits an SVM to all pairs of categories. That is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Subset the data to only include two of the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; categories.&lt;/li&gt;
&lt;li&gt;Fit SVM&lt;/li&gt;
&lt;li&gt;Repeat 1-2 for all possible pairs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Classification is made using the “popular vote”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;approach-2-one-vs-all&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Approach 2: One-vs-all&lt;/h2&gt;
&lt;p&gt;This approach fits an SVM for each category against “other”. That’s &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; SVM’s in total. That is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose a category &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. Lump all other categories into one category called “other”.&lt;/li&gt;
&lt;li&gt;Fit SVM&lt;/li&gt;
&lt;li&gt;Repeat 1-2 for all &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; choices of &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Remember the measure of “confidence” introduced in 4.3, as the absolute value of the left-hand-side of the equation of the hyperplane? We choose the category that results in the highest confidence score.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;svm-in-python&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SVM in python&lt;/h1&gt;
&lt;p&gt;The scikit-learn documentation for running SVM’s in python is available &lt;a href=&#34;http://scikit-learn.org/stable/modules/svm.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In general, the machine learning paradigm in python (at least with scikit-learn, the go-to machine learning library) is to&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Initialize the method.&lt;/li&gt;
&lt;li&gt;Fit the model.&lt;/li&gt;
&lt;li&gt;Query the fit.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can run classification in python with the &lt;code&gt;sklearn.svm&lt;/code&gt; bundle of methods. From this bundle, the method &lt;code&gt;SVC&lt;/code&gt; is useful for SVM’s (despite its name), and &lt;code&gt;LinearSVC&lt;/code&gt; is a special case when the classification boundary is linear (what we’ve been calling SVC).&lt;/p&gt;
&lt;p&gt;Load &lt;code&gt;SVC&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn import svm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use a dataset from the documentation&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from sklearn.datasets import make_classification
X, y = make_classification(n_features=4, random_state=0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;X&lt;/code&gt; is a matrix of the predictors (as a list of lists), with predictors in the columns, and observations in the rows. &lt;code&gt;y&lt;/code&gt; is a list of labels/categories, with length equal to the number of rows of &lt;code&gt;X&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s use scikit-learn.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Initialize the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ll store the initialization in a variable called &lt;code&gt;my_model&lt;/code&gt;. We can use the defaul of SVC, like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my_model = SVC()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or I could have specified hyperparameters here. For example, specify &lt;code&gt;SVC(C=10, kernel=&amp;quot;rbf&amp;quot;)&lt;/code&gt; for a penalty of &lt;span class=&#34;math inline&#34;&gt;\(C=10\)&lt;/span&gt;, using the radial basis function kernel (you can use the &lt;code&gt;gamma&lt;/code&gt; argument to control gamma, too). More details are in the documentation.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fit the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is typically done in scikit-learn by appending &lt;code&gt;.fit(X, y)&lt;/code&gt; to your initialized model, where &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are as above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my_model.fit(X, y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that, with python, the above code modifies the object &lt;code&gt;my_model&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Query the fit&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we can go ahead and do things with the fit, such as make predictions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my_model.predict(X_new)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;X_new&lt;/code&gt; is like &lt;code&gt;X&lt;/code&gt; (possibly with a different number of rows), then predictions will be made on this new data set. Note that this above code &lt;em&gt;does not&lt;/em&gt; modify &lt;code&gt;my_model&lt;/code&gt;, like appending &lt;code&gt;.fit&lt;/code&gt; does.&lt;/p&gt;
&lt;p&gt;We can calculate accuracy:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my_model.score(X_new, y_new)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;y_new&lt;/code&gt; is like &lt;code&gt;y&lt;/code&gt;, but for &lt;code&gt;X_new&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can also calculate the distance to the decision boundary by appending &lt;code&gt;.decision_function&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my_model.decision_function(X)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Collaboration with Version Control: Git and GitHub</title>
      <link>/post/2018-08-14-git_github/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/2018-08-14-git_github/</guid>
      <description>

&lt;p&gt;Does &lt;a href=&#34;http://phdcomics.com/comics/archive_print.php?comicid=1531&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; happen to you? What about bouncing files back and forth amongst collaborators through email? Or even keeping track of your own files between multiple devices?&lt;/p&gt;

&lt;p&gt;Do you currently have a way of handling these sometimes frustrating scenarios? Any horror stories?&lt;/p&gt;

&lt;p&gt;[I developed and delivered this mini-lesson as part of instructional skills training at UBC]&lt;/p&gt;

&lt;h2 id=&#34;learning-outcomes&#34;&gt;Learning Outcomes&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Version control&lt;/strong&gt; is a powerful solution to these regular dilemmas. By the end of this very brief lesson, learners are anticipated to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Understand the central concepts and values of version control&lt;/li&gt;
&lt;li&gt;Identify areas of your own work that can benefit from version control&lt;/li&gt;
&lt;li&gt;Name the prominent version control software, and one prominent cloud-based hosting service&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;what-is-version-control-git&#34;&gt;What is Version Control? Git?&lt;/h2&gt;

&lt;p&gt;Version control is a widely used solution for keeping track of files between devices and collaborators over time. The most prominent software is called &amp;ldquo;git&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I like to think of git as the next level to saving a file. Just like regular &lt;em&gt;file saving&lt;/em&gt; has (hopefully) become a regular part of your workflow, version control requires you to regularly make snapshots of your files. These snapshots are called &lt;em&gt;commits&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Save like crazy. Commit frequently.&lt;/p&gt;

&lt;p&gt;Though, commits aren&amp;rsquo;t really &amp;ldquo;snapshots&amp;rdquo;. Git doesn&amp;rsquo;t actually store an entire copy of your file each time you commit! Only the changes (or &lt;em&gt;diffs&lt;/em&gt;) are stored, so there&amp;rsquo;s no need to sweat any storage demands!&lt;/p&gt;

&lt;h2 id=&#34;what-is-github&#34;&gt;What is GitHub?&lt;/h2&gt;

&lt;p&gt;There are cloud-based services that provide both file storage and a nice interface for git built into them. Perhaps the most prominent of these services is &lt;a href=&#34;https://github.com&#34; target=&#34;_blank&#34;&gt;GitHub&lt;/a&gt; (others are &lt;a href=&#34;https://bitbucket.org/&#34; target=&#34;_blank&#34;&gt;Bitbucket&lt;/a&gt; and &lt;a href=&#34;https://about.gitlab.com/&#34; target=&#34;_blank&#34;&gt;GitLab&lt;/a&gt;). The added step here is to &lt;em&gt;push&lt;/em&gt; changes from your computer up to GitHub.&lt;/p&gt;

&lt;p&gt;Save like crazy. Commit frequently. Push before you&amp;rsquo;re interrupted by a fire.&lt;/p&gt;

&lt;h2 id=&#34;how-exactly-are-my-versioning-and-collaboration-problems-solved&#34;&gt;How exactly are my versioning and collaboration problems solved?&lt;/h2&gt;

&lt;p&gt;In many ways!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Easily identify changes: file changes are highlighted by GitHub. No more detective work to see exactly what was changed in a file.&lt;/li&gt;
&lt;li&gt;Retrievable history: you can easily reconstruct your files as they were at any point along the commit history.

&lt;ul&gt;
&lt;li&gt;No more need to keep loads of file versions!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;No more uncertainty as to who has the most up-to-date files. They&amp;rsquo;re on GitHub!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Personally, I&amp;rsquo;ve been using GitHub more and more, if only for the peace of mind of having a retrievable history, and a record of how my thoughts progress over time. And maybe it&amp;rsquo;s just me, but I also find that dedicating myself to committing keeps me focussed, because they force me to think about what I&amp;rsquo;m doing (instead of just doing it).&lt;/p&gt;

&lt;h2 id=&#34;other-tools&#34;&gt;Other tools&lt;/h2&gt;

&lt;p&gt;Although git and GitHub always work, they work best with plain-text files. If you work with the Office suite of tools like Word, you should first of all &lt;a href=&#34;http://ricardo.ecn.wfu.edu/~cottrell/wp.html&#34; target=&#34;_blank&#34;&gt;consider whether you should really be using this&lt;/a&gt;. If you still want to use Office, then perhaps consider options built into the Office tools, or Google docs.&lt;/p&gt;

&lt;h2 id=&#34;back-to-you&#34;&gt;Back to you&lt;/h2&gt;

&lt;p&gt;What areas of your own life could benefit from version control with Git and GitHub?&lt;/p&gt;

&lt;p&gt;Check your email inbox for emails with attachments to get a sense of the areas of your life that might benefit from version control.&lt;/p&gt;

&lt;p&gt;Next time you go to transfer a file, be mindful of the cognitive load that comes with the file transfer. You might be better off using a version control system like git and GitHub.&lt;/p&gt;

&lt;h2 id=&#34;learning-more&#34;&gt;Learning More&lt;/h2&gt;

&lt;p&gt;If you want to learn more, I recommend starting with a cloud-based service like GitHub. It&amp;rsquo;s a gentle introduction to the subject, and is easier to use than plain git, and has plenty of helpful &lt;a href=&#34;https://guides.github.com/&#34; target=&#34;_blank&#34;&gt;guides&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DSCI 562 Lab 4 Tutorial: GAM</title>
      <link>/post/gam_in_r/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/gam_in_r/</guid>
      <description>


&lt;div id=&#34;generalized-additive-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generalized Additive Models&lt;/h2&gt;
&lt;p&gt;To fit a GAM in R, we could use:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the function &lt;code&gt;gam&lt;/code&gt; in the &lt;code&gt;mgcv&lt;/code&gt; package, or&lt;/li&gt;
&lt;li&gt;the function &lt;code&gt;gam&lt;/code&gt; in the &lt;code&gt;gam&lt;/code&gt; package.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Differences between the two functions are discussed in the “Details” section of the &lt;code&gt;gam&lt;/code&gt; documentation in the &lt;code&gt;mgcv&lt;/code&gt; package. Choose one, but don’t load both! &lt;code&gt;mgcv&lt;/code&gt; tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that’s what is used in this tutorial. But the &lt;code&gt;gam&lt;/code&gt; package has similar workings.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;gam&lt;/code&gt; function works similarly to other regression functions, but the formula specification is different. Let’s go through different formula specifications, doing regression on the &lt;code&gt;mtcars&lt;/code&gt; dataset in R.&lt;/p&gt;
&lt;p&gt;The formula &lt;code&gt;mpg ~ disp + wt&lt;/code&gt; gives you a &lt;em&gt;linear model&lt;/em&gt;. It indicates that &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;wt&lt;/code&gt; both enter the model in a linear fashion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mgcv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: nlme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is mgcv 1.8-24. For overview type &amp;#39;help(&amp;quot;mgcv-package&amp;quot;)&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1 &amp;lt;- gam(mpg ~ disp + wt, data=mtcars)
fit2 &amp;lt;- lm(mpg ~ disp + wt, data=mtcars)
summary(fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ disp + wt
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 34.96055    2.16454  16.151 4.91e-16 ***
## disp        -0.01773    0.00919  -1.929  0.06362 .  
## wt          -3.35082    1.16413  -2.878  0.00743 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## R-sq.(adj) =  0.766   Deviance explained = 78.1%
## GCV = 9.3863  Scale est. = 8.5063    n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4087 -2.3243 -0.7683  1.7721  6.3484 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 34.96055    2.16454  16.151 4.91e-16 ***
## disp        -0.01773    0.00919  -1.929  0.06362 .  
## wt          -3.35082    1.16413  -2.878  0.00743 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.917 on 29 degrees of freedom
## Multiple R-squared:  0.7809, Adjusted R-squared:  0.7658 
## F-statistic: 51.69 on 2 and 29 DF,  p-value: 2.744e-10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the coefficient estimates are the same.&lt;/p&gt;
&lt;p&gt;To make a term non-parametric, wrap the &lt;code&gt;s&lt;/code&gt; function around the term (for &lt;em&gt;splines&lt;/em&gt;; comes with the &lt;code&gt;mgcv&lt;/code&gt; package). The &lt;code&gt;gam&lt;/code&gt; package also has a &lt;code&gt;lo&lt;/code&gt; function, for &lt;em&gt;loess&lt;/em&gt; smoothing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- gam(mpg ~ s(disp) + s(wt), data=mtcars)
summary(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ s(disp) + s(wt)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  20.0906     0.3429   58.59   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##           edf Ref.df     F  p-value    
## s(disp) 6.263  7.386 6.373 0.000164 ***
## s(wt)   1.000  1.000 4.015 0.056434 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.896   Deviance explained = 92.1%
## GCV = 5.0715  Scale est. = 3.762     n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, each predictor enters the model in a non-parametric, additive form. The nonparametric functions can be accessed by calling &lt;code&gt;plot&lt;/code&gt;. For documentation, see &lt;code&gt;?plot.gam&lt;/code&gt;. Let’s plot the “bivariate” scatterplots behind these curves too (these bivariate data actually use partial residuals).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit3, residuals=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/gam_in_r_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;img src=&#34;/tutorials/gam_in_r_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like the “weight” variable (&lt;code&gt;wt&lt;/code&gt;) is quite linear. We can let it be linear, while the &lt;code&gt;disp&lt;/code&gt; variable remains nonparametric. “Wiggliness” of the smoothed fit can be controlled through the &lt;code&gt;k&lt;/code&gt; argument of the &lt;code&gt;s&lt;/code&gt; function, but this is chosen in a “smart” way by default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4 &amp;lt;- gam(mpg ~ s(disp, k=3) + wt, data=mtcars)
summary(fit4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ s(disp, k = 3) + wt
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  31.1110     3.1336   9.928  1.1e-10 ***
## wt           -3.4254     0.9649  -3.550  0.00138 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##          edf Ref.df     F  p-value    
## s(disp) 1.93  1.995 9.724 0.000758 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.839   Deviance explained = 85.4%
## GCV =  6.659  Scale est. = 5.8413    n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(fit4, residuals=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/gam_in_r_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can even combine predictors into a common smooth function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit5 &amp;lt;- gam(mpg ~ s(disp, qsec) + wt, data=mtcars)
summary(fit5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## mpg ~ s(disp, qsec) + wt
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   32.181      4.340   7.415 1.83e-07 ***
## wt            -3.758      1.345  -2.794   0.0105 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Approximate significance of smooth terms:
##                edf Ref.df     F  p-value    
## s(disp,qsec) 7.634  9.694 5.353 0.000312 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## R-sq.(adj) =  0.903   Deviance explained =   93%
## GCV = 5.0335  Scale est. = 3.5181    n = 32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each, the &lt;code&gt;predict&lt;/code&gt; and &lt;code&gt;residuals&lt;/code&gt; functions work in the same old way. Let’s use them to make a residual plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(predict(fit5), residuals(fit5)) +
    geom_abline(intercept=0, slope=0, linetype=&amp;quot;dashed&amp;quot;) +
    xlab(&amp;quot;Prediction (mean)&amp;quot;) + ylab(&amp;quot;Residuals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/gam_in_r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For their documentation, see &lt;code&gt;?predict.gam&lt;/code&gt; and &lt;code&gt;?residuals.gam&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>DSCI 563 Lab1 Tutorial: k-means</title>
      <link>/post/kmeans_in_r/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/kmeans_in_r/</guid>
      <description>


&lt;p&gt;This tutorial discusses the implementation of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means (and variants) in R.&lt;/p&gt;
&lt;p&gt;Let’s use a simulated dataset. Throughout, we’ll take &lt;span class=&#34;math inline&#34;&gt;\(k=3\)&lt;/span&gt; groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(ggplot2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(345)
x &amp;lt;- rnorm(250)
y &amp;lt;- rnorm(250)
## Shift group 2:
x[51:150] &amp;lt;- x[51:150] + 2.5
y[51:150] &amp;lt;- y[51:150] - 1
## Shift group 3:
x[151:250] &amp;lt;- x[151:250] - 1.5
y[151:250] &amp;lt;- y[151:250] - 1.5
## Plot:
dat &amp;lt;- data.frame(x=x, y=y)
qplot(x, y, alpha=I(0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/kmeans_in_r_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;k-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means&lt;/h2&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basics&lt;/h3&gt;
&lt;p&gt;Let’s fit a &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means algorithm to the data with the &lt;code&gt;kmeans&lt;/code&gt; function, to &lt;span class=&#34;math inline&#34;&gt;\(k=3\)&lt;/span&gt; groups. Indicate the data first, then &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; in the second (&lt;code&gt;centers&lt;/code&gt;) argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(22)
(fit1 &amp;lt;- kmeans(dat, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## K-means clustering with 3 clusters of sizes 60, 87, 103
## 
## Cluster means:
##            x          y
## 1  0.3725131  0.2750934
## 2  2.5109499 -1.2199721
## 3 -1.5793637 -1.5174017
## 
## Clustering vector:
##   [1] 1 3 3 3 1 3 3 1 2 1 1 1 1 1 1 1 1 2 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1 3 1
##  [36] 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2 2 2 2 2 2 1 3 2 2 2 2 2 1 2 2 2 2 2 2
##  [71] 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2
## [106] 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2
## [141] 2 2 2 2 2 2 2 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3
## [176] 3 3 1 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3
## [211] 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [246] 3 3 3 3 3
## 
## Within cluster sum of squares by cluster:
## [1] 100.3076 154.1080 170.9162
##  (between_SS / total_SS =  68.4 %)
## 
## Available components:
## 
## [1] &amp;quot;cluster&amp;quot;      &amp;quot;centers&amp;quot;      &amp;quot;totss&amp;quot;        &amp;quot;withinss&amp;quot;    
## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot;    &amp;quot;size&amp;quot;         &amp;quot;iter&amp;quot;        
## [9] &amp;quot;ifault&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output tells you what you can extract, under “Available components”. You can extract them using the &lt;code&gt;$&lt;/code&gt; symbol. Let’s look at some of them.&lt;/p&gt;
&lt;p&gt;Here are the assigned clusters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1$cluster&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] 1 3 3 3 1 3 3 1 2 1 1 1 1 1 1 1 1 2 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1 3 1
##  [36] 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2 2 2 2 2 2 1 3 2 2 2 2 2 1 2 2 2 2 2 2
##  [71] 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2
## [106] 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2
## [141] 2 2 2 2 2 2 2 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3
## [176] 3 3 1 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3
## [211] 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [246] 3 3 3 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(x, y, alpha=I(0.5), colour=factor(fit1$cluster))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/kmeans_in_r_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here are the three means/centers of each group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1$centers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x          y
## 1  0.3725131  0.2750934
## 2  2.5109499 -1.2199721
## 3 -1.5793637 -1.5174017&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also extract the squared distances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1$totss        # Total sum of squares.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1343.956&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1$withinss     # Within group sum of squares.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 100.3076 154.1080 170.9162&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1$tot.withinss # Total within group. **Objective function**&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 425.3318&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1$betweenss    # Between group sum of squares.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 918.6239&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tweaks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweaks&lt;/h3&gt;
&lt;p&gt;In addition to running basic &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means, we can also run &lt;em&gt;multiple&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means with the &lt;code&gt;nstart&lt;/code&gt; argument. For each run, different initial values are used (different centroids). The run with the best fit is the one that is reported. What is the “best fit” anyway? It’s the one with the smallest total &lt;em&gt;within group&lt;/em&gt; sum of squares – that is, &lt;code&gt;$tot.withinss&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take the best of 20 runs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit2 &amp;lt;- kmeans(dat, 3, nstart=20))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## K-means clustering with 3 clusters of sizes 60, 103, 87
## 
## Cluster means:
##            x          y
## 1  0.3725131  0.2750934
## 2 -1.5793637 -1.5174017
## 3  2.5109499 -1.2199721
## 
## Clustering vector:
##   [1] 1 2 2 2 1 2 2 1 3 1 1 1 1 1 1 1 1 3 2 1 1 1 2 2 1 1 2 1 1 1 1 1 1 2 1
##  [36] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 3 3 3 3 3 3 1 2 3 3 3 3 3 1 3 3 3 3 3 3
##  [71] 3 3 3 3 1 3 3 3 3 3 3 3 1 1 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3
## [106] 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 1 3 3 1 1 3 3 3 3 3 3 3 3 3
## [141] 3 3 3 3 3 3 3 1 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2
## [176] 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2
## [211] 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [246] 2 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1] 100.3076 170.9162 154.1080
##  (between_SS / total_SS =  68.4 %)
## 
## Available components:
## 
## [1] &amp;quot;cluster&amp;quot;      &amp;quot;centers&amp;quot;      &amp;quot;totss&amp;quot;        &amp;quot;withinss&amp;quot;    
## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot;    &amp;quot;size&amp;quot;         &amp;quot;iter&amp;quot;        
## [9] &amp;quot;ifault&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure of the output is no different from before.&lt;/p&gt;
&lt;p&gt;You may also choose the centroids to start the algorithm with. To do this, instead of indicating &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; for the &lt;code&gt;centers&lt;/code&gt; argument, use a matrix of the centroids you wish to begin with.&lt;/p&gt;
&lt;p&gt;Let’s start with the true means: (0,0), (2.5,-1), and (-1.5, -1.5):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(xstart &amp;lt;- matrix(c(0,0, 2.5,-1, -1.5,-1.5), ncol=2, byrow=TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]  0.0  0.0
## [2,]  2.5 -1.0
## [3,] -1.5 -1.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit3 &amp;lt;- kmeans(dat, xstart))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## K-means clustering with 3 clusters of sizes 60, 87, 103
## 
## Cluster means:
##            x          y
## 1  0.3725131  0.2750934
## 2  2.5109499 -1.2199721
## 3 -1.5793637 -1.5174017
## 
## Clustering vector:
##   [1] 1 3 3 3 1 3 3 1 2 1 1 1 1 1 1 1 1 2 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1 3 1
##  [36] 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2 2 2 2 2 2 1 3 2 2 2 2 2 1 2 2 2 2 2 2
##  [71] 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2
## [106] 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2
## [141] 2 2 2 2 2 2 2 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3
## [176] 3 3 1 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3
## [211] 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [246] 3 3 3 3 3
## 
## Within cluster sum of squares by cluster:
## [1] 100.3076 154.1080 170.9162
##  (between_SS / total_SS =  68.4 %)
## 
## Available components:
## 
## [1] &amp;quot;cluster&amp;quot;      &amp;quot;centers&amp;quot;      &amp;quot;totss&amp;quot;        &amp;quot;withinss&amp;quot;    
## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot;    &amp;quot;size&amp;quot;         &amp;quot;iter&amp;quot;        
## [9] &amp;quot;ifault&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;k-means-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means++&lt;/h2&gt;
&lt;div id=&#34;basics-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Basics&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;kmeans&lt;/code&gt; function is quite specific – it only looks at squared distances. There’s a package called &lt;code&gt;flexclust&lt;/code&gt; that allows for more flexible cluster analysis. The &lt;code&gt;kcca&lt;/code&gt; function is particularly useful.&lt;/p&gt;
&lt;p&gt;At its basic, &lt;code&gt;kcca&lt;/code&gt; works similarly to &lt;code&gt;kmeans&lt;/code&gt;. In fact, this will run a standard &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means algorithm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(flexclust))
(fit4 &amp;lt;- kcca(dat, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## kcca object of family &amp;#39;kmeans&amp;#39; 
## 
## call:
## kcca(x = dat, k = 3)
## 
## cluster sizes:
## 
##  1  2  3 
## 92 59 99&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But, the output is less friendly than &lt;code&gt;kmeans&lt;/code&gt; (because &lt;code&gt;kcca&lt;/code&gt; uses a formal S4 object-oriented format). The multitude of output can be extracted by &lt;code&gt;@&lt;/code&gt; instead of &lt;code&gt;$&lt;/code&gt;. For example, here are the cluster assignments, and the final three centers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4@cluster&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] 2 3 3 3 2 3 3 2 1 2 1 2 2 2 2 2 2 1 3 2 2 2 3 3 2 2 2 2 2 2 2 2 2 3 2
##  [36] 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 1 1 1 1 1 1 2 3 1 1 1 1 1 2 1 1 1 1 1 1
##  [71] 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1
## [106] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 1 2 2 1 1 1 1 1 1 1 1 1
## [141] 1 1 1 1 1 1 1 2 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3
## [176] 3 3 2 3 3 3 3 3 3 3 3 1 2 3 3 3 3 2 3 3 2 3 3 3 2 3 3 3 3 2 3 3 3 3 3
## [211] 3 3 3 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
## [246] 3 3 3 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4@centers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               x          y
## [1,]  2.4487745 -1.1781922
## [2,]  0.1910489  0.3025144
## [3,] -1.6003047 -1.5694850&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that there is no sum of squares output, like there is in &lt;code&gt;kmeans&lt;/code&gt; – this is because &lt;code&gt;kcca&lt;/code&gt; doesn’t necessarily use the euclidean distance. However, there &lt;em&gt;is&lt;/em&gt; a way to convert a &lt;code&gt;kcca&lt;/code&gt; output to a &lt;code&gt;kmeans&lt;/code&gt; output, though this output is more limited than usual (the only &lt;code&gt;ss&lt;/code&gt; available is &lt;code&gt;$withinss&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4b &amp;lt;- as(fit4, &amp;quot;kmeans&amp;quot;)
fit4b$withinss&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 167.92353  96.14623 161.87687&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tweaks-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweaks&lt;/h3&gt;
&lt;p&gt;The most useful thing about &lt;code&gt;kcca&lt;/code&gt; regarding Lab 1 is &lt;code&gt;kcca&lt;/code&gt;’s ability to do &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means++. This can be done through the &lt;code&gt;control&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;control&lt;/code&gt; argument should be a named list, with each name corresponding to some property you’d like to indicate. See &lt;code&gt;?cclustControl&lt;/code&gt; for the various options. But the one we’re interested in is “initcent”, which controls how the initial centers are chosen. From the documentation, this should be:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Character string, name of function for initial centroids, currently “randomcent” (the default) and “kmeanspp” are available.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So to do &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-means++, this amounts to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit5 &amp;lt;- kcca(dat, 3, control=list(initcent=&amp;quot;kmeanspp&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## kcca object of family &amp;#39;kmeans&amp;#39; 
## 
## call:
## kcca(x = dat, k = 3, control = list(initcent = &amp;quot;kmeanspp&amp;quot;))
## 
## cluster sizes:
## 
##  1  2  3 
## 95 68 87&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another feature of &lt;code&gt;kcca&lt;/code&gt; is that it allows for different distance metrics besides euclidean. This can be indicated through the &lt;code&gt;family&lt;/code&gt; argument. Take a look at the &lt;code&gt;kcca&lt;/code&gt; documentation under “Predefined Families” to see what distance metrics you can use.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>JAGS Tutorial</title>
      <link>/post/jags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/jags/</guid>
      <description>


&lt;p&gt;JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.&lt;/p&gt;
&lt;p&gt;You’ll need to &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;download and install JAGS&lt;/a&gt;. You can interact with JAGS through one of three R packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;runjags&lt;/code&gt; (recommended for this course)
&lt;ul&gt;
&lt;li&gt;Model written as a single string in R; possibly also allows you to input from file.&lt;/li&gt;
&lt;li&gt;Quick-start guide vignette: &lt;code&gt;vignette(&amp;quot;quickjags&amp;quot;, package=&amp;quot;runjags&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Full user guide vignette: &lt;code&gt;vignette(&amp;quot;UserGuide&amp;quot;, package=&amp;quot;runjags&amp;quot;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rjags&lt;/code&gt; (sample code &lt;a href=&#34;http://www.johnmyleswhite.com/notebook/2010/08/20/using-jags-in-r-with-the-rjags-package/&#34;&gt;here&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Model read in from plain text file.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;R2jags&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;this is dependent on &lt;code&gt;R2winbugs&lt;/code&gt;, which I find doesn’t work well outside of Windows machines, so I’m more hesitant to use this package.&lt;/li&gt;
&lt;li&gt;Model written in R as a function, but using JAGS language; or inputted from file.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, the &lt;code&gt;coda&lt;/code&gt; package is useful for working with the output of at least &lt;code&gt;runjags&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;About the JAGS language:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generates samples of parameters based on the prior and statistical model.&lt;/li&gt;
&lt;li&gt;Need to specify which parameters you want to include in the output (aka “track” or “monitor”).&lt;/li&gt;
&lt;li&gt;Specify probability distributions similarly to R, except:
&lt;ul&gt;
&lt;li&gt;Draw samples using calls like &lt;code&gt;dexp&lt;/code&gt; and &lt;code&gt;dnorm&lt;/code&gt;, not &lt;code&gt;rexp&lt;/code&gt; and &lt;code&gt;rnorm&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The JAGS version of &lt;code&gt;rnorm&lt;/code&gt; uses the precision (=1/variance) instead of standard deviation. The documentation of JAGS code is not as nice as R. You have to look things up from a table-of-contents-style search from &lt;a href=&#34;http://www.stats.ox.ac.uk/~nicholls/MScMCMC15/jags_user_manual.pdf&#34;&gt;this&lt;/a&gt; document. Page 29 shows the aliases for various distributions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this week’s lab assignment (3), you’ll only be using it to generate observations from a distribution. Let’s generate data from a N(0,2) distribution (that is, variance=2), and ignore the warning messages for this week.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(runjags))
my_model &amp;lt;- &amp;quot;
model{
    # This is a comment.
    theta ~ dnorm(0, 1/2)
}
&amp;quot;
fit &amp;lt;- run.jags(my_model, monitor=&amp;quot;theta&amp;quot;, n.chains=1, sample=1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: No data was specified or found in the model file so the simulation
## was run withut data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calling the simulation...
## Welcome to JAGS 4.3.0 on Sun Aug 25 20:24:33 2019
## JAGS is free software and comes with ABSOLUTELY NO WARRANTY
## Loading module: basemod: ok
## Loading module: bugs: ok
## . . Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 0
##    Unobserved stochastic nodes: 1
##    Total graph size: 5
## . Initializing model
## . Adaptation skipped: model is not in adaptive mode.
## . Updating 4000
## -------------------------------------------------| 4000
## ************************************************** 100%
## . . Updating 1000
## -------------------------------------------------| 1000
## ************************************************** 100%
## . . . . Updating 0
## . Deleting model
## . 
## Note: the model did not require adaptation
## Simulation complete.  Reading coda files...
## Coda files loaded successfully
## Calculating summary statistics...&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Convergence cannot be assessed with only 1 chain&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Finished running the simulation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theta &amp;lt;- coda::as.mcmc(fit)
head(theta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Markov Chain Monte Carlo (MCMC) output:
## Start = 5001 
## End = 5007 
## Thinning interval = 1 
##           theta
## 5001  0.0876587
## 5002  1.1282000
## 5003 -1.9906600
## 5004  0.1476260
## 5005  0.6665350
## 5006  2.1165500
## 5007 -0.0731193&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(theta)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/jags_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;more-sample-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;More sample code&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.0       ✔ purrr   0.2.5  
## ✔ tibble  2.1.1       ✔ dplyr   0.8.0.1
## ✔ tidyr   0.8.1       ✔ stringr 1.3.1  
## ✔ readr   1.1.1       ✔ forcats 0.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──
## ✖ tidyr::extract() masks runjags::extract()
## ✖ dplyr::filter()  masks stats::filter()
## ✖ dplyr::lag()     masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(runjags)
n &amp;lt;- 50
dat &amp;lt;- tibble(x=rnorm(n),
              y=x + rnorm(n))
jagsdat &amp;lt;- c(as.list(dat), n=nrow(dat))
model &amp;lt;- &amp;quot;model{
    for (i in 1:n) {
        y[i] ~ dnorm(beta*x[i], tau)
    }
    tau &amp;lt;- pow(sigma, -2)
    sigma ~ dunif(0, 100)
    beta ~ dnorm(0, 0.001)
}&amp;quot;
foo &amp;lt;- run.jags(
    model=model, 
    monitor=c(&amp;quot;beta0&amp;quot;, &amp;quot;beta1&amp;quot;, &amp;quot;sigma&amp;quot;), 
    data=jagsdat
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: No initial value blocks found and n.chains not specified: 2 chains
## were used&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: No initial values were provided - JAGS will use the same initial
## values for all chains&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calling the simulation...
## Welcome to JAGS 4.3.0 on Sun Aug 25 20:24:36 2019
## JAGS is free software and comes with ABSOLUTELY NO WARRANTY
## Loading module: basemod: ok
## Loading module: bugs: ok
## . . Reading data file data.txt
## . Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 50
##    Unobserved stochastic nodes: 2
##    Total graph size: 159
## . Initializing model
## . Adapting 1000
## -------------------------------------------------| 1000
## ++++++++++++++++++++++++++++++++++++++++++++++++++ 100%
## Adaptation successful
## . Updating 4000
## -------------------------------------------------| 4000
## ************************************************** 100%
## . Failed to set trace monitor for beta0
## Variable beta0 not found
## . Failed to set trace monitor for beta1
## Variable beta1 not found
## . . Updating 10000
## -------------------------------------------------| 10000
## ************************************************** 100%
## . . . . . Updating 0
## . Deleting model
## . 
## Simulation complete.  Reading coda files...
## Coda files loaded successfully
## Calculating summary statistics...
## Calculating the Gelman-Rubin statistic for 1 variables....
## Finished running the simulation&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(coda::as.mcmc(foo))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in as.mcmc.runjags(foo): Combining the 2 mcmc chains together&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/jags_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lecture 5: Plotting for Humans</title>
      <link>/post/effective_plotting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/effective_plotting/</guid>
      <description>


&lt;blockquote&gt;
&lt;p&gt;Making effective plots can tell you a LOT about data. Its hard! Its an under-rated but very powerful skill to develop.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;- Di Cook&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressPackageStartupMessages(library(tidyverse))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;dplyr&amp;#39; was built under R version 3.5.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gapminder)
knitr::opts_chunk$set(fig.width=5, fig.height=3)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;agenda&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Agenda&lt;/h1&gt;
&lt;p&gt;Tips for effective graphing&lt;/p&gt;
&lt;p&gt;At least two exercises related to content and &lt;a href=&#34;http://viz.wtf/&#34; class=&#34;uri&#34;&gt;http://viz.wtf/&lt;/a&gt; (see the worksheet).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;p&gt;These resources are listed on the syllabus in the lecture table. They provide a good overview of tips for effective plotting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.geckoboard.com/learn/data-literacy/data-visualization-tips/&#34;&gt;Geckoboard’s data vis tips&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jenny’s STAT545 lecture notes: &lt;a href=&#34;http://stat545.com/block015_graph-dos-donts.html&#34;&gt;do’s and don’ts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are some resources that dive a little deeper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Di Cook’s &lt;a href=&#34;http://www.dicook.org/2018/04/14/content/post/2018-04-14-rookie-mistakes/&#34;&gt;Rookie Mistakes&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;Especially focusses on categorical data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Richard Hollins’ &lt;a href=&#34;https://www.richardhollins.com/blog/why-pie-charts-suck/&#34;&gt;Three reasons why pie charts suck&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An entertaining but inspiring resource:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gallery of poor data vis: &lt;a href=&#34;http://viz.wtf/&#34; class=&#34;uri&#34;&gt;http://viz.wtf/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to spend more time on this and/or dig deeper, take a look at the following books:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://webcat1.library.ubc.ca/vwebv/holdingsInfo?bibId=7678980&#34;&gt;Visualization Analysis and Design&lt;/a&gt; – Tamara Munzner&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://resolve.library.ubc.ca/cgi-bin/catsearch?bid=3198934&#34;&gt;Creating more effective graphs&lt;/a&gt; – Naomi Robbins.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preface&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preface&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Effectiveness: how well a graph conveys the information conveyed in data.&lt;/li&gt;
&lt;li&gt;!= publication quality, although sometimes the line between these two can be blurry (no pun intended).&lt;/li&gt;
&lt;li&gt;Take solace in the fact that we can’t make perfectly effective graphics.&lt;/li&gt;
&lt;li&gt;Is a topic in human psychology.&lt;/li&gt;
&lt;li&gt;Don’t know human psychology? Luckily, you’re a human, and have an innate knowledge of the relative effectiveness of a plot. Use that as your guide!
&lt;ul&gt;
&lt;li&gt;From Gelman et al, As a guiding point, ask yourself: how can I modify my graph to better:
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;faciliate comparisons, and&lt;/li&gt;
&lt;li&gt;reveal trends?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The tips you see here and online hold true for most cases. There might be &lt;em&gt;some&lt;/em&gt; rare cases where the tips don’t hold – the key is to be &lt;em&gt;intentional&lt;/em&gt; about every component of the graph.&lt;/p&gt;
&lt;p&gt;“Let’s Practice What We Preach: Turning Tables into Graphs” by Gelman A, Pasarica C, Dodhia R. The American Statistician, Volume 56, Number 2, 1 May 2002 , pp. 121-130(10).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learning-objectives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Learning Objectives&lt;/h1&gt;
&lt;p&gt;From today’s lecture, students are expected to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;be intentional with your choice of graph components.&lt;/li&gt;
&lt;li&gt;be able to spot bad graphs, and avoid making them.&lt;/li&gt;
&lt;li&gt;internalize some tips of plotting effectiveness provided in class.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the quiz, you aren’t expected to know/memorize all of the tips, but you are expected to have internalized some of them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-information-density&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Consider Information Density&lt;/h1&gt;
&lt;p&gt;Sometimes called overplotting.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scatterplot too dense?
&lt;ul&gt;
&lt;li&gt;Do you need a log transform?&lt;/li&gt;
&lt;li&gt;Try alpha transparency&lt;/li&gt;
&lt;li&gt;Change geom: &lt;code&gt;geom_hex()&lt;/code&gt; or &lt;code&gt;geom_bin2d()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Spread the data into separate panels: facet by a grouping variable or two.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapxy &amp;lt;- ggplot(gapminder, aes(lifeExp, gdpPercap)) +
    theme_bw()
gapxy + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapxy &amp;lt;- gapxy + scale_y_log10()
gapxy + geom_point() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapxy + geom_point(alpha=0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-2-3.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapxy + geom_hex() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-2-4.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapxy + geom_density2d()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-2-5.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapxy + facet_wrap(~continent) + geom_point(alpha=0.2) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-2-6.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using too many geom’s? Don’t.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gapminder, aes(continent, lifeExp)) +
    geom_violin(fill=&amp;quot;red&amp;quot;, alpha=0.2) +
    geom_boxplot(fill=&amp;quot;blue&amp;quot;, alpha=0.2) +
    geom_jitter(alpha=0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;find-the-goldilocks-plot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Find the &lt;a href=&#34;https://www.dltk-teach.com/rhymes/goldilocks_story.htm&#34;&gt;Goldilocks&lt;/a&gt; Plot&lt;/h1&gt;
&lt;p&gt;Display just the right amount of content: not too much, not too little.&lt;/p&gt;
&lt;p&gt;In particular: reveal as much relevant information as possible; trim irrelevant and redundant information.&lt;/p&gt;
&lt;div id=&#34;reveal-as-much-relevant-information-as-possible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reveal as much relevant information as possible&lt;/h2&gt;
&lt;p&gt;Because hiding your data is not effective at conveying information!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;jitter + violin, not pinhead plots.&lt;/li&gt;
&lt;li&gt;mosaic plots&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;trim-irrelevant-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trim Irrelevant Information&lt;/h2&gt;
&lt;p&gt;Only use as much data as is required for answering a data analytic question.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Di Cook’s example in &lt;a href=&#34;http://www.dicook.org/2018/04/14/content/post/2018-04-14-rookie-mistakes/#reduce-complexity&#34;&gt;Rookie Mistakes: reduce complexity section&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Remove the special effects (sorry, Excel). Great demo: Darkhorse analytic’s &lt;a href=&#34;https://github.com/STAT545-UBC/STAT545-UBC.github.io/blob/master/img/less-is-more-darkhorse-analytics.gif&#34;&gt;Less is more&lt;/a&gt; gif.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;More examples of extraneous information:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_data(&amp;quot;france&amp;quot;) %&amp;gt;% 
    ggplot(aes(long, lat)) +
    geom_polygon(aes(group=group), fill=NA, colour=&amp;quot;black&amp;quot;) +
    theme_bw() +
    ggtitle(&amp;quot;Are lat and long really needed?&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gapminder, aes(year, lifeExp)) +
    geom_line(aes(group=country, colour=country), alpha=0.2) +
    guides(colour=FALSE) +
    theme_bw() +
    ggtitle(&amp;quot;Is colouring by country really necessary here?\nNevermind fitting the legend!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trim-redundant-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trim Redundant Information&lt;/h2&gt;
&lt;p&gt;Don’t redundantly map variables to aesthetics/facets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Common example: colouring/filling and facetting by the same variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HairEyeColor %&amp;gt;% 
    as_tibble() %&amp;gt;% 
    uncount(n) %&amp;gt;% 
    ggplot(aes(Hair)) +
    facet_wrap(~Sex) +
    geom_bar(aes(fill=Sex)) +
    theme_bw() +
    ggtitle(&amp;quot;Don&amp;#39;t do this.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Really want to use colour? No problem, colours are fun! Try this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HairEyeColor %&amp;gt;% 
    as_tibble() %&amp;gt;% 
    uncount(n) %&amp;gt;% 
    ggplot(aes(Hair)) +
    facet_wrap(~Sex) +
    geom_bar(fill=&amp;quot;#D95F02&amp;quot;) +
    theme_bw() +
    ggtitle(&amp;quot;Do this.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Delegate numeric details to an appendix, not the graph (or omit entirely).&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HairEyeColor %&amp;gt;% 
    as_tibble() %&amp;gt;% 
    uncount(n) %&amp;gt;% 
    count(Hair) %&amp;gt;% 
    ggplot(aes(Hair, n)) +
    geom_col() +
    geom_text(aes(label=n), vjust=-0.1) +
    theme_bw() +
    labs(x=&amp;quot;Hair colour&amp;quot;, y=&amp;quot;count&amp;quot;, 
         title=&amp;quot;Are the bar numbers AND y-axis really needed?&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;choose-human-interpretable-aesthetic-mappings-and-geoms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Choose Human-Interpretable Aesthetic Mappings and Geom’s&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Here’s an iconic fail in &lt;a href=&#34;https://twitter.com/hnrklndbrg/status/886181647003119616?lang=en&#34;&gt;Henrik Lindberg’s tweet&lt;/a&gt;: the “depeche plot”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Don’t use pie charts – use bar charts instead. &lt;a href=&#34;https://www.richardhollins.com/blog/why-pie-charts-suck/&#34;&gt;3 reasons why they suck.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To bar, or not to bar? Not if zero doesn’t matter! As a general rule, I like to err on the side of points over bars.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_beav2 &amp;lt;- bind_rows(
    mutate(beaver1, beaver = &amp;quot;Beaver 1&amp;quot;), 
    mutate(beaver2, beaver = &amp;quot;Beaver 2&amp;quot;)
) %&amp;gt;% 
    group_by(beaver) %&amp;gt;% 
    summarize(med = median(temp)) %&amp;gt;% 
    ggplot(aes(beaver, med)) +
    theme_bw() +
    xlab(&amp;quot;&amp;quot;) +
    ylab(&amp;quot;Body Temperature\n(Celsius)&amp;quot;)
cowplot::plot_grid(
    plot_beav2 +
        geom_col() +
        ggtitle(&amp;quot;Don&amp;#39;t do this.&amp;quot;),
    plot_beav2 +
        geom_point() +
        ggtitle(&amp;quot;Do this.&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Yes, that’s really all the info you’re conveying. Own it.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Colour your groups so that not one group unintentionally stands out.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_iris &amp;lt;- ggplot(iris, aes(Sepal.Width, Sepal.Length)) +
    geom_jitter(aes(colour=Species)) +
    theme_bw() +
    theme(legend.position = &amp;quot;bottom&amp;quot;)
cowplot::plot_grid(
    plot_iris +
        scale_colour_manual(values=c(&amp;quot;brown&amp;quot;, &amp;quot;gray&amp;quot;, &amp;quot;yellow&amp;quot;)) +
        ggtitle(&amp;quot;Don&amp;#39;t do this.&amp;quot;),
    plot_iris +
        scale_colour_brewer(palette=&amp;quot;Dark2&amp;quot;) +
        ggtitle(&amp;quot;Leave it to an expert.\nDo this.&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-zero&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Consider Zero&lt;/h1&gt;
&lt;p&gt;Are you comparing data across groups? Consider what a meaningful &lt;em&gt;distance measure&lt;/em&gt; might be between two groups.&lt;/p&gt;
&lt;p&gt;Are &lt;em&gt;differences&lt;/em&gt; meaningful, and &lt;em&gt;proportions&lt;/em&gt; not? Example: temperature. Zero doesn’t matter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_beav &amp;lt;- bind_rows(
    mutate(beaver1, beaver = &amp;quot;Beaver 1&amp;quot;), 
    mutate(beaver2, beaver = &amp;quot;Beaver 2&amp;quot;)
) %&amp;gt;% 
    ggplot(aes(beaver, temp)) +
    geom_violin() +
    geom_jitter(alpha=0.25) +
    theme_bw() +
    xlab(&amp;quot;&amp;quot;) +
    ylab(&amp;quot;Body Temperature\n(Celsius)&amp;quot;)
cowplot::plot_grid(
    plot_beav + 
        ggtitle(&amp;quot;This.&amp;quot;), 
    plot_beav + 
        ylim(c(0,NA)) +
        ggtitle(&amp;quot;Not This.&amp;quot;)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Are &lt;em&gt;proportions&lt;/em&gt; meaningful, and &lt;em&gt;differences&lt;/em&gt; not? Example: counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HairEyeColor %&amp;gt;% 
    as_tibble() %&amp;gt;% 
    uncount(n) %&amp;gt;% 
    ggplot(aes(Hair)) +
    geom_bar() +
    theme_bw() +
    ggtitle(&amp;quot;Keep this starting from 0.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Want to convey absolute life expectancies, in addition to relative life expectancies? Show 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gapminder, aes(continent, lifeExp)) +
    geom_boxplot() +
    ylim(c(0, NA)) +
    geom_hline(yintercept = 0,
               linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/tutorials/effective_plotting_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;order-factors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Order factors&lt;/h1&gt;
&lt;p&gt;It’s easier to see rankings. &lt;a href=&#34;http://stat545.com/block029_factors.html#change-order-of-the-levels-principled&#34;&gt;See this STAT 545 example by Jenny Bryan&lt;/a&gt;. Use &lt;code&gt;forcats&lt;/code&gt;!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Linear Algebra For DSCI 563 Lab4</title>
      <link>/post/useful_linalg_results/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/useful_linalg_results/</guid>
      <description>


&lt;p&gt;This document outlines some Linear Algebra results that you’ll need to know for the DSCI 563 Lab4 assignment. You can take these as given – you don’t have to prove them.&lt;/p&gt;
&lt;div id=&#34;covariance-rule&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Covariance Rule&lt;/h2&gt;
&lt;p&gt;The variance-covariance matrix (or, just &lt;em&gt;covariance matrix&lt;/em&gt; for short) of a random vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X}\)&lt;/span&gt; is denoted &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\boldsymbol{X})\)&lt;/span&gt;. Now, if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a matrix/vector, then the following rule holds: &lt;span class=&#34;math display&#34;&gt;\[ \text{Cov}(A\boldsymbol{X}) = A \text{Cov}(\boldsymbol{X}) A ^{\top}, \]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(A ^ {\top}\)&lt;/span&gt; denotes the transpose of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;orthogonal-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Orthogonal Matrices&lt;/h2&gt;
&lt;p&gt;An orthogonal matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is defined such that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is square, and &lt;span class=&#34;math display&#34;&gt;\[ A^{\top} A = A A^{\top} = I, \]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is the identity matrix. That is, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{\top}\)&lt;/span&gt; are inverses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spectral-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Spectral Decomposition&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; be a covariance matrix. That is, it’s a square matrix that’s positive definite. Then, &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; can be decomposed/reconstructed by its eigenvalues and eigenvectors. Specifically, if &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is a matrix whose columns are the eigenvectors of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; is a diagonal matrix with the corresponding eigenvectors along the diagonal, then &lt;span class=&#34;math display&#34;&gt;\[ \Sigma = U \Lambda U^{\top}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Further, &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is orthogonal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;diagonal-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagonal Matrices&lt;/h2&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a square diagonal matrix. Then &lt;span class=&#34;math inline&#34;&gt;\(A^{\top} = A\)&lt;/span&gt;. Also, if &lt;span class=&#34;math inline&#34;&gt;\(a \in \mathbb{R}\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(A^a\)&lt;/span&gt; is a diagonal matrix where the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;’th diagonal entry is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;’th diagonal entry of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; raised to the power of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transpose-of-a-matrix-product&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transpose of a Matrix Product&lt;/h2&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(A_1, \ldots, A_k\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; matrices. Then, the transpose of their product can be found by reversing the order of the product, and transposing each matrix: &lt;span class=&#34;math display&#34;&gt;\[ \left(\prod_{i=1}^{k}A_{i}\right)^{\top}=\prod_{i=1}^{k}A_{k-i+1}^{\top}. \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summability-of-powers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summability of Powers&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(a_1, \ldots, a_k\)&lt;/span&gt; be real numbers, and &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; a square matrix. If &lt;span class=&#34;math inline&#34;&gt;\(A^{a_i}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i=1,\ldots,k\)&lt;/span&gt; exist, then &lt;span class=&#34;math display&#34;&gt;\[ \prod_{i=1}^{k}A^{a_{i}}=A^{\sum_{i=1}^{k}a_{i}}. \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Varimax Rotation</title>
      <link>/post/varimax/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/post/varimax/</guid>
      <description>


&lt;div id=&#34;what-is-varimax&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is Varimax?&lt;/h3&gt;
&lt;p&gt;In 1(b), you saw that there are many loadings matrices that we can take so that the Factor Analysis assumptions are still satisfied (i.e., more than one loadings matrix “works”). We explored some of the possible loadings matrices by multiplying by a rotation matrix, denoted &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; in the lab assignment.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;varimax rotation&lt;/em&gt; is one such rotation that leads to loadings vectors that have a desirable property (in terms of interpretation): it “polarizes” the loadings vectors so that the values within a vector are either close to zero or are large in magnitude. A nicely “polarized” loadings vector is useful for interpretation, because features that are relevant to the underlying factor become easier to identify. For example, if the loadings vector for Factor 1 has loadings close to 1 for Features A and B, and close to zero for all other features, then we can look for something common between Features A and B (for example, “bitter flavour”) that describes the factor underlying the data.&lt;/p&gt;
&lt;p&gt;How can one achieve a loadings matrix whose vectors are polarized as much as possible? We need some concrete measure of “polarity” to maximize. &lt;em&gt;One&lt;/em&gt; trick is to maximize the &lt;strong&gt;variance&lt;/strong&gt; of the &lt;em&gt;squared&lt;/em&gt; loadings (squaring the loadings allows us to disregard sign). This is what varimax does.&lt;/p&gt;
&lt;p&gt;The specific formula can be found on Slide 29 in Lecture 6.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;your-task&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Your task&lt;/h3&gt;
&lt;p&gt;Write a function in R that takes a value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and does the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Computes the rotation matrix &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Computes the rotated loadings &lt;span class=&#34;math inline&#34;&gt;\(L_R = LR\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Computes sum of variances of the squared loadings in each column of &lt;span class=&#34;math inline&#34;&gt;\(L_R\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This function returns a measure of “polarity” in your loadings vectors (in terms of the total variance) – the larger the total variance, the more polarized your loadings vectors are. As such, you should find the value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that maximizes this “polarity” (maximizes this function). Use this &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; to compute your rotated loadings.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
