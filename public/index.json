[{"authors":["admin"],"categories":null,"content":"Hi!\nI\u0026rsquo;m a data scientist at the University of British Columbia, Vancouver. My primary focus is the development and delivery of the Master of Data Science program.\nMy hobbies include outdoor recreation such as hiking, birding, skiing, fishing, canoeing, and others.\nPreferred personal pronouns: he/him/his\nSome of the things I do:\n I solve problems using statistics, machine learning, and data visualizations. I interact with students to make data science more approachable. I use R, git, and friends to help me do these things.  I can also identify birds, and other things about nature.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5ed4d2245720b2d43a173bdec4161ed6","permalink":"/authors/vincenzo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vincenzo/","section":"authors","summary":"Hi!\nI\u0026rsquo;m a data scientist at the University of British Columbia, Vancouver. My primary focus is the development and delivery of the Master of Data Science program.\nMy hobbies include outdoor recreation such as hiking, birding, skiing, fishing, canoeing, and others.\nPreferred personal pronouns: he/him/his\nSome of the things I do:\n I solve problems using statistics, machine learning, and data visualizations. I interact with students to make data science more approachable.","tags":null,"title":"Vincenzo Coia","type":"authors"},{"authors":null,"categories":null,"content":" I was invited to the SFU/UBC Joint Seminar in Spring 2019 where I gave this talk.\n## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 Who am I?  Quiz True or False:  The least squares estimator is derived by maximizing the likelihood.   True or False:  The least squares estimator is derived by maximizing the likelihood. If errors are not Gaussian, we can’t use least squares to estimate our regression coefficients.   True or False:  The least squares estimator is derived by maximizing the likelihood. If errors are not Gaussian, we can’t use least squares to estimate our regression coefficients.  Answers: Both FALSE!\n Question: When we fit a regression model (linear, kNN, random forest, etc.), what is the interpretation of the resulting model function / regression curve?\n Question: When we fit a regression model (linear, kNN, random forest, etc.), what is the interpretation of the resulting model function / regression curve?\nAnswer: The mean of Y given X.\n  Concept #1 We minimize the SSE in regression because it’s a proper scoring rule for the mean.\nOn the board, lets:\nWrite ybar as a minimization problem. Extend to regression   Concept #2 When doing regression, we ought to consider quantiles, too!\nConsider Y = monthly expenditure (in $). Interpretation of quantities:\n median: low-quantile: high-quantile: mean:  Consider Y = monthly expenditure (in $). Interpretation of quantities:\n median: There’s a 50-50 chance that you’ll have to pay more than this. low-quantile: high-quantile: mean:  Consider Y = monthly expenditure (in $). Interpretation of quantities:\n median: There’s a 50-50 chance that you’ll have to pay more than this. low-quantile: You’ll “at least” have to pay this much. high-quantile: mean:  Consider Y = monthly expenditure (in $). Interpretation of quantities:\n median: There’s a 50-50 chance that you’ll have to pay more than this. low-quantile: You’ll “at least” have to pay this much. high-quantile: You’ll “at most” have to pay this much. mean:  Consider Y = monthly expenditure (in $). Interpretation of quantities:\n median: There’s a 50-50 chance that you’ll have to pay more than this. low-quantile: You’ll “at least” have to pay this much. high-quantile: You’ll “at most” have to pay this much. mean: Multiply by m to estimate total $ after m months.   Concept #3 Each quantile has its own proper scoring rule that we can use instead of the squared error.\nOn the board:\nWrite median as an optimization problem Extend to generic quantile Extend to regression  The “check function”:\n Concept #4 Make a distributional assumption to reduce estimation uncertainty.\nUnivariate Estimation If you have a univariate sample \\(Y_1, \\ldots, Y_n\\):\n  Distributional Assumption? Estimation Method    No   Yes      Univariate Estimation If you have a univariate sample \\(Y_1, \\ldots, Y_n\\):\n  Distributional Assumption? Estimation Method    No “sample versions”: ybar, s^2, quantile(), …  Yes MLE     Regression setting If you have a univariate sample \\(Y_1, \\ldots, Y_n\\) AND predictors:\n  Distributional Assumption? Estimation Method    No   Yes      Regression setting If you have a univariate sample \\(Y_1, \\ldots, Y_n\\) AND predictors:\n  Distributional Assumption? Estimation Method    No Optimize scoring rule for desired quantity.  Yes MLE      Return of the Quiz Return of the Quiz Can we see why these are false?\n The least squares estimator is derived by maximizing the likelihood. If errors are not Gaussian, we can’t use least squares to estimate our regression coefficients.    Time left? Time left? Give two ways to estimate the conditional variance.\nHint: Think about the definition of variance.\nTalk to your neighbour for 1 minute\n  Resources This talk was inspired by the activity generated by my blog post “The missing question in supervised learning”.\nFor proper scoring rules, see Gneiting, T., and Raftery, A.E. (2007) “Strictly Proper Scoring Rules, Prediction, and Estimation”. Journal of the American Statistical Association, 102:477\n ","date":1552694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552694400,"objectID":"31656fe1973ea97015446ec0d16d0a18","permalink":"/post/20190316-ubc-sfu/","publishdate":"2019-03-16T00:00:00Z","relpermalink":"/post/20190316-ubc-sfu/","section":"post","summary":"I was invited to the SFU/UBC Joint Seminar in Spring 2019 where I gave this talk.\n## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 Who am I?  Quiz True or False:  The least squares estimator is derived by maximizing the likelihood.   True or False:  The least squares estimator is derived by maximizing the likelihood.","tags":null,"title":"The squared error has friends, too!","type":"post"},{"authors":null,"categories":null,"content":" ## Here\u0026#39;s a little tutorial on how to use the `factanal` function for Factor Analysis. ## Let\u0026#39;s make a data frame using actual factors: set.seed(3847) F1 \u0026lt;- rnorm(100) F2 \u0026lt;- rnorm(100) X1 \u0026lt;- F1 + F2 + rnorm(100)/5 X2 \u0026lt;- 4*F1 * rnorm(100) X3 \u0026lt;- -6*F1 * rnorm(100)/4 X4 \u0026lt;- F2 + rnorm(100) X5 \u0026lt;- 4*F2 - 0.5*F1 * rnorm(100) dat \u0026lt;- data.frame(X1=X1, X2=X2, X3=X3, X4=X4, X5=X5) ## Let\u0026#39;s do a factor analysis with k=2 factors. There are two main ways you can do this. k \u0026lt;- 2 ## Method 1: via the data. ## Just indicate the data in the first argument, and specify the number of factors ## in the `factors` argument. ## Note that loadings that are \u0026quot;essentially zero\u0026quot; are indicated as blanks. factanal(dat, factors=k) ## ## Call: ## factanal(x = dat, factors = k) ## ## Uniquenesses: ## X1 X2 X3 X4 X5 ## 0.375 0.005 0.947 0.487 0.035 ## ## Loadings: ## Factor1 Factor2 ## X1 0.790 ## X2 0.160 0.985 ## X3 0.119 -0.198 ## X4 0.715 ## X5 0.976 -0.110 ## ## Factor1 Factor2 ## SS loadings 2.128 1.024 ## Proportion Var 0.426 0.205 ## Cumulative Var 0.426 0.630 ## ## Test of the hypothesis that 2 factors are sufficient. ## The chi square statistic is 0.93 on 1 degree of freedom. ## The p-value is 0.336 ## Method 2: via the covariance matrix. ## Indicate the covariance matrix in the `covmat` argument. ## Note: this method doesn\u0026#39;t have as many features, such as hypothesis testing, ## built into it. But the results are the same.s factanal(factors=k, covmat=cov(dat)) ## ## Call: ## factanal(factors = k, covmat = cov(dat)) ## ## Uniquenesses: ## X1 X2 X3 X4 X5 ## 0.375 0.005 0.947 0.487 0.035 ## ## Loadings: ## Factor1 Factor2 ## X1 0.790 ## X2 0.160 0.985 ## X3 0.119 -0.198 ## X4 0.715 ## X5 0.976 -0.110 ## ## Factor1 Factor2 ## SS loadings 2.128 1.024 ## Proportion Var 0.426 0.205 ## Cumulative Var 0.426 0.630 ## ## The degrees of freedom for the model is 1 and the fit was 0.0097 ## Another important thing to note is how to do *rotations*. There are two ## built-in rotations available via the `rotations` argument -- they are ## \u0026quot;none\u0026quot; and \u0026quot;varimax\u0026quot; (default). So the plain factor analysis results ## without rotation is: factanal(dat, factors=k, rotation = \u0026quot;none\u0026quot;) ## ## Call: ## factanal(x = dat, factors = k, rotation = \u0026quot;none\u0026quot;) ## ## Uniquenesses: ## X1 X2 X3 X4 X5 ## 0.375 0.005 0.947 0.487 0.035 ## ## Loadings: ## Factor1 Factor2 ## X1 0.784 0.101 ## X2 0.997 ## X3 0.151 -0.175 ## X4 0.712 ## X5 0.981 ## ## Factor1 Factor2 ## SS loadings 2.106 1.046 ## Proportion Var 0.421 0.209 ## Cumulative Var 0.421 0.630 ## ## Test of the hypothesis that 2 factors are sufficient. ## The chi square statistic is 0.93 on 1 degree of freedom. ## The p-value is 0.336 ## You can extract the loadings vector by extracting the list component ## entitled `loadings`. There\u0026#39;s a weird print call associated with the ## object, but it\u0026#39;s really just a matrix, as the last call indicates. fit \u0026lt;- factanal(dat, factors=k, rotation = \u0026quot;none\u0026quot;) fit$loadings ## ## Loadings: ## Factor1 Factor2 ## X1 0.784 0.101 ## X2 0.997 ## X3 0.151 -0.175 ## X4 0.712 ## X5 0.981 ## ## Factor1 Factor2 ## SS loadings 2.106 1.046 ## Proportion Var 0.421 0.209 ## Cumulative Var 0.421 0.630 fit$loadings[1:5, 1:2] ## Factor1 Factor2 ## X1 0.783885737 0.10136519 ## X2 -0.009679362 0.99744995 ## X3 0.151115775 -0.17503806 ## X4 0.711852604 0.08077992 ## X5 0.980910146 0.05733956 ","date":1548028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548028800,"objectID":"30ca78a73fb1412cf15f5d8ea04e3e0b","permalink":"/post/factor_analysis/","publishdate":"2019-01-21T00:00:00Z","relpermalink":"/post/factor_analysis/","section":"post","summary":"## Here\u0026#39;s a little tutorial on how to use the `factanal` function for Factor Analysis. ## Let\u0026#39;s make a data frame using actual factors: set.seed(3847) F1 \u0026lt;- rnorm(100) F2 \u0026lt;- rnorm(100) X1 \u0026lt;- F1 + F2 + rnorm(100)/5 X2 \u0026lt;- 4*F1 * rnorm(100) X3 \u0026lt;- -6*F1 * rnorm(100)/4 X4 \u0026lt;- F2 + rnorm(100) X5 \u0026lt;- 4*F2 - 0.5*F1 * rnorm(100) dat \u0026lt;- data.frame(X1=X1, X2=X2, X3=X3, X4=X4, X5=X5) ## Let\u0026#39;s do a factor analysis with k=2 factors.","tags":null,"title":"Factor Analysis","type":"post"},{"authors":null,"categories":null,"content":" You all know the drill \u0026ndash; you\u0026rsquo;re asked to make predictions of a continuous variable, so you turn to your favourite supervised learning method to do the trick. But have you ever suspected that you could be after the wrong type of output before you even begin?\nRegression trees, loess, linear regression\u0026hellip; you name it, they\u0026rsquo;re all in pursuit of the mean (well, almost all). But the true outcome is random. It has a distribution. Are you sure you want the mean of that distribution?\nYou might say \u0026ldquo;Yes! It ensures my prediction is as close as possible to the outcome!\u0026rdquo; If this is indeed what you want, the mean still might not be your best choice \u0026ndash; it only ensures the mean squared error is minimized.\nThere are a suite of other options that might be more appropriate than the mean. The good thing is, your favourite supervised learning method probably has a natural extension for estimating these alternatives. Let\u0026rsquo;s investigate the quantities you might care about.\nThe Median No, the median isn\u0026rsquo;t just an inferior version of the mean, to be used under the unfortunate presence of outliers.\nIf I randomly pick a data scientist, what do you think their salary would be? This distribution has a right-skew, so chances are, your data scientist earns less than the mean. Predict the median, and you\u0026rsquo;ll have a 50% chance that your data scientist does earn at least what you predict.\nIn short, use the median when you want your prediction to be exceeded with a coin toss.\nMinimize the mean absolute error to get this prediction.\nHigher (or lower) Quantiles Want to make it to an interview on time? You add some \u0026ldquo;buffer time\u0026rdquo; to the expected travel time, right? What you\u0026rsquo;re after is a high quantile of travel time \u0026ndash; something like the 0.99-quantile, so that there is only a small chance you\u0026rsquo;ll be late (1% in this case).\nUse a high (or low) quantile if you want a conservative (or liberal) prediction \u0026ndash; or both, if you want a prediction interval.\nMinimize the mean rho function to get this prediction.\nThe Mean The mean is useful when we care about totals. Want to know how much gas a vehicle uses? You\u0026rsquo;re after the mean, because the total quantity drawn out over time is what matters.\nMinimize the mean squared error to get this prediction.\nOther Options Do you really need to distill your prediction down to a single number? Consider looking at the entire distribution of the outcome as your prediction (typically conditional on predictors) \u0026ndash; after all, this conveys the entire uncertainty about the outcome. This is known as probabilistic forecasting.\nThere are other measures, too. Expected shortfall is useful for risk analysis, or even expectiles. Maybe you care about variance or skewness for some reason. Whatever you want to get at, just make sure you ask yourself what you actually care about. You have an entire distribution to distill!\n(Photo from Pexels)\n","date":1518912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518912000,"objectID":"33b65142d22950dbc10e553d4d63fa91","permalink":"/project/2018-02-18-mean/","publishdate":"2018-02-18T00:00:00Z","relpermalink":"/project/2018-02-18-mean/","section":"project","summary":"You all know the drill \u0026ndash; you\u0026rsquo;re asked to make predictions of a continuous variable, so you turn to your favourite supervised learning method to do the trick. But have you ever suspected that you could be after the wrong type of output before you even begin?\nRegression trees, loess, linear regression\u0026hellip; you name it, they\u0026rsquo;re all in pursuit of the mean (well, almost all). But the true outcome is random.","tags":["supervised learning","mean","quantile","median","probabilistic forecasting"],"title":"The missing question in supervised learning","type":"project"},{"authors":null,"categories":null,"content":" I love backpacks. They are my go-to way of carrying stuff around. But many backpacks cause fuss that we\u0026rsquo;ve resigned to deal with. Look for these top eight features that will make your backpack work for you instead of against you.\n8. Loops for Carabiners Having these on the outside of your pack can be tremendously useful. They allow for quick, easy-access \u0026ldquo;storage\u0026rdquo; of miscellaneous things like a water bottle or handbag. More useful than you might think!\n7. Size I like to err on the side of too big, because cramming stuff isn\u0026rsquo;t fun. Plus, you can get some large packs that are also lightweight.\n6. Rain Protector A rain protector not only helps by keeping your stuff dry inside the backpack (like your laptop), but the bag gets heavier as it gets saturated with water.\nThink an umbrella will do the trick? Just feel the top of your bag after using an umbrella on a rainy day.\n5. Wide and Thin Straps It\u0026rsquo;s not the padding that makes a pack sit softly on your shoulders, it\u0026rsquo;s the width. In fact, I look for thinner padding to prevent stench and bacteria buildup. After all, the straps go under the armpits!\n4. Top-loading I prefer top-loading packs. It\u0026rsquo;s just easier to load and access your stuff than front-loading packs are.\nI only see front-loading packs to be useful if you intend to access your stuff like you would a suitcase \u0026ndash; by putting the back down on a flat surface, and opening the entire front. I almost never do this \u0026ndash; I almost always access the pack in an up-right position.\n3. No zippers Zippers are trouble. They almost always fail. Any of these scenarios sound familiar?\n Your zipper loses connection with other side. The bindings come apart. Zipper bites on fabric.  Instead, I look for a drawstring. Or just recently, I\u0026rsquo;ve taken to the style present on dry bags that involve folding over the top, and buckling the ends together.\n2. Simplicity Where did I put that thingamajig? Perhaps this pocket here\u0026hellip; no, maybe it\u0026rsquo;s in the zippered pocket inside this other pocket.\nSimplify your life, and stick to one or two compartments. If you do encounter the need for further organization (such as separating little things from big), consider using smaller packs instead of compartments. I like dry bags) for this purpose, because you can also compress them.\n1. Airflow The flow of air between your back and the pack is a feature that I think is far too overlooked. If the pack sits flat up against your back, you can expect to sweat, especially if you\u0026rsquo;re a fast walker like I am. I even look for this feature in my everyday backpack.\nDo your back a favour and allow it to breathe!\nDo you share these sentiments? Or perhaps disagree? Have something to add? Share your thoughts in the comments section below!\n(Photo by Sassu anas from Pexels)\n","date":1515110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515110400,"objectID":"924d7589801daf6ac2fe43b65cc562e8","permalink":"/project/2018-01-05-backpack/","publishdate":"2018-01-05T00:00:00Z","relpermalink":"/project/2018-01-05-backpack/","section":"project","summary":"I love backpacks. They are my go-to way of carrying stuff around. But many backpacks cause fuss that we\u0026rsquo;ve resigned to deal with. Look for these top eight features that will make your backpack work for you instead of against you.\n8. Loops for Carabiners Having these on the outside of your pack can be tremendously useful. They allow for quick, easy-access \u0026ldquo;storage\u0026rdquo; of miscellaneous things like a water bottle or handbag.","tags":["backpack","efficiency","no-fuss","outdoors"],"title":"Is your backpack fuss-proof? Look for these eight game-changing features.","type":"project"},{"authors":null,"categories":null,"content":"I find myself writing a lot. Usually, I squirrel my notes away in my local file management system. But that\u0026rsquo;s not very useful for you, and it\u0026rsquo;s not ideal for me, either.\nI\u0026rsquo;m hoping to write for the public, for three main reasons:\n You (hopefully) gain something of value. And that makes me happy, too. It forces me to clarify my thoughts. Even though I write regularly, putting content online forces me to make my notes clearer. It encourages me to write more often. Because when a note is just for me, there\u0026rsquo;s less incentive for me to write it in the first place.  I write about my passions \u0026ndash; naturally, because I think about them a lot. They can be summarized as three main themes that I think will make up the contents of this website:\n Data science Systems for personal effectiveness Nature  I\u0026rsquo;m not entirely sure how often this website will be updated, because regular public writing is new for me. But I\u0026rsquo;m quite confident it will be somewhat active, since my vision for this is fairly clear. And, I have lots of things to say.\nSome things you might expect to see on this site are:\n How to organize project files \u0026ldquo;Change Management\u0026rdquo; is a misnomer. Here\u0026rsquo;s the real change management. Understanding principal components analysis Stories about my hikes in British Columbia How and why to use copulas for regression/machine learning  I hope you\u0026rsquo;ll find this website/blog useful. Have you had the itch to write for the public? Please share your experience and your feedback in the comments section below!\n","date":1514678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514678400,"objectID":"5207695e74cc4320cb9526e607f388f7","permalink":"/project/2017-12-31-hello_world/","publishdate":"2017-12-31T00:00:00Z","relpermalink":"/project/2017-12-31-hello_world/","section":"project","summary":"Here's why I'm writing for you.","tags":["hello world","blogging"],"title":"Hello, World!","type":"project"},{"authors":null,"categories":null,"content":" This tutorial introduces contour plots, and how to plot them in ggplot2.\nWhat is a contour plot? Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?\nThe idea is to use contour lines, which are curves that indicate a constant height.\nImagine cutting the tops of the mountains off by removing all land above, say, 900 meters altitude. Then trace (on your map) the shapes formed by the new (flat) mountain tops. These curves are contour lines. Choose a differential such as 50 meters, and draw these curves for altitudes …800m, 850m, 900m, 950m, 1000m, … – the result is a contour plot (or topographic map, if it’s a map).\nIn general, contour plots are useful for functions of two variables (like a bivariate gaussian density).\nWe’ll look at examples in the next section.\nNotes on contours:\n They never cross. The steepest slope at a point is parallel to the contour line. They aren’t entirely ambiguous. For example, you can’t tell whether or not the mountains are actually mountains, or whether they’re holes/valleys! Sometimes you can add colour to indicate depth; other times (like in topographic maps) you can indicate elevation directly as numbers beside contour lines. Other times, this is not required, because the context makes it obvious.   Contour plots in ggplot2 There are two ways you can make contour plots in ggplot2 – but they’re both for quite different purposes.\nMethod 1: Approximate a bivariate density This method approximates a bivariate density from data.\nFirst, recall how this is done in the univariate case. A little kernel function (like a shrunken bell curve) is placed over each data point, and these are added together to get a density estimate:\nlibrary(ggplot2) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 set.seed(373) x \u0026lt;- rnorm(1000) ggplot(data.frame(x=x), aes(x)) + geom_density() We can do the same thing to get a bivariate density, except with little bivariate kernel functions (like shrunken bivariate Gaussian densities). But, we can’t just simply put “density height” on the vertical axis – we need that for the second dimension. Instead, we can use contour plots.\nThis is the contour plot that ggplot2’s geom_density2d() does: builds a bivariate kernel density estimate (based on data), then makes a contour plot out of it:\ny \u0026lt;- rnorm(1000) ggplot(data.frame(x=x, y=y), aes(x, y)) + geom_density2d() Based on context (this is a density), we know that this is a “hill” and not a “hole”. If you were to start at some point at the “bottom” of the hill, the steepest way up would be perpendicular to the contours. The highest point on the hill is within the middle-most circle.\n Method 2: General Contour Plots You can also make contour plots that aren’t a kernel density estimate (necessarily), using geom_contour(). This is based off of any bivariate function.\nBasics Suppose we want to make a contour plot of the bivariate function \\[f(x,y) = x^2 + sin(y)\\] over the rectangle \\(-2\u0026lt;x\u0026lt;2\\) and \\(-5\u0026lt;y\u0026lt;5\\). First, make a grid over the rectangle (it must be a grid – geom_contour() won’t work otherwise). Then, evaluate the function at each of the grid points. Put all this info into a single data frame with three columns (two for the \\(x\\) and \\(y\\) coordinates, and one for the function evaluation). Then, indicate the function evaluation in geom_contour() as the aesthetic z, and the x and y aesthetics are as usual.\nf \u0026lt;- function(x) x[1]^2 + sin(x[2]) x \u0026lt;- seq(-2, 2, length.out=100) y \u0026lt;- seq(-5, 5, length.out=100) dat \u0026lt;- expand.grid(x=x, y=y) # Data frame of 100*100=10000 points. dat$z \u0026lt;- apply(dat, 1, f) ggplot(dat, aes(x, y)) + geom_contour(aes(z=z)) Notice that expand.grid is useful for making grids. It returns all pairs from the input vectors. But, this also means that it’s easy for the output to explode!\nNote that finer grids yield plots with higher accuracy. Here’s an example of a rough grid, whose contours are jagged:\nf \u0026lt;- function(x) x[1]^2 + sin(x[2]) x \u0026lt;- seq(-2, 2, length.out=10) y \u0026lt;- seq(-5, 5, length.out=10) dat \u0026lt;- expand.grid(x=x, y=y) # Data frame of 10*10=100 points. dat$z \u0026lt;- apply(dat, 1, f) ggplot(dat, aes(x, y)) + geom_contour(aes(z=z))  Additional Settings Here, we’ll look at colouring the plots, and adding more/less contours.\nHere’s another example, with the volcano data (a matrix of altitudes for a volcano). If you’d like, first take a look at a 3D rendering of the volcano, by running the following code chunk in your R console after un-commenting the last two lines (code taken directly from rgl’s surface3d() documentation):\ndata(volcano) z \u0026lt;- 2 * volcano # Exaggerate the relief x \u0026lt;- 10 * (1:nrow(z)) # 10 meter spacing (S to N) y \u0026lt;- 10 * (1:ncol(z)) # 10 meter spacing (E to W) zlim \u0026lt;- range(y) zlen \u0026lt;- zlim[2] - zlim[1] + 1 colorlut \u0026lt;- terrain.colors(zlen) # height color lookup table col \u0026lt;- colorlut[ z - zlim[1] + 1 ] # assign colors to heights for each point # open3d() # surface3d(x, y, z, color = col, back = \u0026quot;lines\u0026quot;) Feel free to move the image around by clicking and dragging. Neat, eh?\nWe’ll make a contour plot with this.\ndat \u0026lt;- expand.grid(x=x, y=y) dat$z \u0026lt;- as.vector(z)/2 # \u0026quot;De-exaggerate\u0026quot; the relief ggplot(dat, aes(x, y)) + geom_contour(aes(z=z)) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + theme(axis.text=element_blank(), axis.ticks=element_blank()) But, you can’t tell that the inner circles actually represent a hole (a caldera), not a peak. Let’s add colour by indicating the “variable” ..height.. in the colour aesthetic of geom_cotour(), which will also indicate height as a legend:\ndat \u0026lt;- expand.grid(x=x, y=y) dat$z \u0026lt;- as.vector(z)/2 # \u0026quot;De-exaggerate\u0026quot; the relief ggplot(dat, aes(x, y)) + geom_contour(aes(z=z, colour=..level..)) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + theme(axis.text=element_blank(), axis.ticks=element_blank()) + scale_color_continuous(\u0026quot;Altitude\u0026quot;) Now we can tell that the highest point is within the lightest blue area, to the left of the caldera.\nNow let’s add more contour lines, to get a better sense of the terrain. Do so by indicating the altitudes to make contours for via breaks. Let’s make 5 unit spacing:\ndat \u0026lt;- expand.grid(x=x, y=y) dat$z \u0026lt;- as.vector(z)/2 # \u0026quot;De-exaggerate\u0026quot; the relief ggplot(dat, aes(x, y)) + geom_contour(aes(z=z, colour=..level..), breaks=seq(100, 200, by=5)) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + theme(axis.text=element_blank(), axis.ticks=element_blank()) + scale_color_continuous(\u0026quot;Altitude\u0026quot;) Although you can change the contours, it’s best practice to keep the (height) spacing between contour lines equal – otherwise, the contour plot becomes harder to read. In the above plot, for example, we know that crossing \\(n\\) contour lines (that are either increasing or decreasing) results in \\(5n\\) units of elevation gain/loss, because the spacing between contours is always 5 units.\n   ","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"1cde67e5128905fad57b7e912e8832de","permalink":"/post/contour_plots/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/post/contour_plots/","section":"post","summary":"This tutorial introduces contour plots, and how to plot them in ggplot2.\nWhat is a contour plot? Suppose you have a map of a mountainous region. How might you indicate elevation on that map, so that you get to see the shape of the landscape?\nThe idea is to use contour lines, which are curves that indicate a constant height.\nImagine cutting the tops of the mountains off by removing all land above, say, 900 meters altitude.","tags":null,"title":"Contour Plots","type":"post"},{"authors":null,"categories":null,"content":" This tutorial introduces the concept of a mixture distribution. We’ll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.\nIntuition Let’s start by looking at a basic experiment:\nFlip a coin. If the outcome is heads, generate a N(0,1) random variable. If the outcome is tails, generate a N(4,1) random variable. We’ll let \\(X\\) denote the final result.  \\(X\\) is a random variable with some distribution (spoiler: it’s a mixture distribution). Let’s perform the experiment 1000 times to get 1000 realizations of \\(X\\), and make a histogram to get a sense of the distribution \\(X\\) follows. To make sure the histogram represents an estimate of the density, we’ll make sure the area of the bars add to 1 (with the ..density.. option).\nsuppressMessages(library(ggplot2)) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 set.seed(44) X \u0026lt;- numeric(0) coin \u0026lt;- integer(0) for (i in 1:1000) { coin[i] \u0026lt;- rbinom(1, size=1, prob=0.5) # flip a coin. 0=heads, 1=tails. if (coin[i] == 0) { # heads X[i] \u0026lt;- rnorm(1, mean=0, sd=1) } else { # tails X[i] \u0026lt;- rnorm(1, mean=4, sd=1) } } (p \u0026lt;- qplot(X, ..density.., geom=\u0026quot;histogram\u0026quot;, bins=30)) Let’s try to reason our way to figuring out the overall density. Keep in mind that this density (like all densities) is one curve. We’ll say we’ve succeeded at finding the density if our density is close to the histogram.\nIt looks like the histogram is made up of two normal distributions “superimposed”. These ought to be related to the N(0,1) and N(4,1) distributions, so to start, let’s plot these two Gaussian densities overtop of the histogram.\nggplot(data.frame(X=X), aes(X)) + geom_histogram(aes(y=..density..), bins=30) + stat_function(fun=function(x) dnorm(x, mean=0, sd=1), mapping=aes(colour=\u0026quot;Heads\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=4, sd=1), mapping=aes(colour=\u0026quot;Tails\u0026quot;)) + scale_color_discrete(\u0026quot;Coin Flip\u0026quot;) Well, the two Gaussian distributions are in the correct location, and it even looks like they have the correct spread, but they’re too tall.\nSomething to note at this point: the two curves plotted above are separate (component) distributions. We’re trying to figure out the distribution of \\(X\\) – which, again, is a single curve, and is estimated by the histogram. At this point, we only suspect that the distribution of \\(X\\) is some combination of these two Gaussian distributions.\nSo, why are the Gaussian curves too tall? Because each one represents the distribution if we only ever flip either heads or tails (for example, the red distribution happens when we only ever flip heads). But since we flip heads half of the time, and tails half of the time, these probabilities (more accurately, densities) ought to be reduced by half. Let’s add these “semi” component distributions to the plot:\n(p \u0026lt;- ggplot(data.frame(X=X), aes(X)) + geom_histogram(aes(y=..density..), bins=30) + stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5, mapping=aes(colour=\u0026quot;Heads\u0026quot;, linetype=\u0026quot;Semi\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=4, sd=1)*0.5, mapping=aes(colour=\u0026quot;Tails\u0026quot;, linetype=\u0026quot;Semi\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=0, sd=1), mapping=aes(colour=\u0026quot;Heads\u0026quot;, linetype=\u0026quot;Full\u0026quot;)) + stat_function(fun=function(x) dnorm(x, mean=4, sd=1), mapping=aes(colour=\u0026quot;Tails\u0026quot;, linetype=\u0026quot;Full\u0026quot;)) + scale_color_discrete(\u0026quot;Coin Flip\u0026quot;) + scale_linetype_discrete(\u0026quot;Distribution\u0026quot;)) Looks like they line up quite nicely!\nBut these two curves are still separate – we need one overall curve if we are to find the distribution of \\(X\\). So we need to combine them somehow. It might look at first that we can just take the upper-most of the ‘semi’ curves (i.e., the maximum of the two), but looking in between the two curves reveals that the histogram is actually larger than either curve here. It turns out that the two ‘semi’ curves are added to get the final curve:\np + stat_function(fun=function(x) dnorm(x, mean=0, sd=1)*0.5 + dnorm(x, mean=4, sd=1)*0.5, mapping=aes(linetype=\u0026quot;Full\u0026quot;)) The intuition behind adding the densities is that an outcome for \\(X\\) comes from both components, so both contribute some density.\nEven though the random variable \\(X\\) is made up of two components, at the end of the day, it’s still overall just a random variable with some density. And like all densities, the density of \\(X\\) is just one curve. But, this density happens to be made up of the components, as we’ll see next.\n General Scenario The two normal distributions from above are called component distributions. In general, we can have any number of these (not just two) to make a mixture distribution. And, instead of selecting the component distribution with coin tosses, they’re chosen according to some generic probabilities called the mixture probabilities.\nIn general, here’s how we make a mixture distribution with \\(K\\) component Gaussian distributions with densities \\(\\phi_1(x), \\ldots, \\phi_K(x)\\):\nChoose one of the \\(K\\) components, randomly, with mixture probabilities \\(\\pi_1, \\ldots, \\pi_K\\) (which, by necessity, add to 1). Generate a random variable from the selected component distribution. Call the result \\(X\\).  Note: we can use more than just Gaussian component distributions! But this tutorial won’t demonstrate that.\nThat’s how we generate a random variable with a mixture distribution, but what’s its density? We can derive that by the law of total probability. Let \\(C\\) be the selected component number; then the component distributions are actually the distribution of \\(X\\) conditional on the component number. We get: \\[ f_X\\left(x\\right) = \\sum_{k=1}^{K} f_{X|C}\\left(x \\mid c\\right) P\\left(C=c\\right) = \\sum_{k=1}^{K} \\phi_k\\left(x\\right) \\pi_k. \\]\nNotes:  The intuition described in the previous section matches up with this result. For \\(K=2\\) components determined by a coin toss \\((\\pi_1=\\pi_2=0.5),\\) we have \\[ f_X\\left(x\\right) = \\phi\\left(x\\right)0.5 + \\phi\\left(x-4\\right)0.5, \\] which is the black curve in the previous plot. This tutorial works with univariate data. But mixture distributions can be multivariate, too. A \\(d\\)-variate mixture distribution can be made by replacing the component distributions with \\(d\\)-variate distributions. Just be sure to distinguish between the dimension of the data \\(d\\) and the number of components \\(K\\). We could just describe a mixture distribution by its density, just like we can describe a normal distribution by its density. But, describing mixture distributions by its component distributions together with the mixture probabilities, we obtain an excellent interpretation of the mixture distribution. This interpretation is (it’s also called a data generating process): (1) randomly choose a component, and (2) generate from that component. This interpretation is useful for cluster analysis, because the data clusters can be thought of as being generated by the component distributions, and the proportion of data in each cluster is determined by the mixture probabilities.    Learning Points  A mixture distribution can be described by its mixing probabilities \\(\\pi_1, \\ldots, \\pi_K\\) and component distributions \\(\\phi_1(x), \\ldots, \\phi_K(x)\\). A mixture distribution can also be described by a single density (like all continuous random variables).  This density is a single curve if data are univariate; a single “surface” if the data are bivariate; and higher dimensional surfaces if the data are higher dimensional.  To get the density from the mixing probabilities and component distributions, we can use the formula indicated in the above section (based on the law of total probability).   ","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"5700d987d2a4eafe2a512a48d71a74b3","permalink":"/post/mixture_distributions/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/post/mixture_distributions/","section":"post","summary":"This tutorial introduces the concept of a mixture distribution. We’ll look at a basic example first, using intuition, and then describe mixture distributions mathematically. See the very end for a summary of the learning points.\nIntuition Let’s start by looking at a basic experiment:\nFlip a coin. If the outcome is heads, generate a N(0,1) random variable. If the outcome is tails, generate a N(4,1) random variable. We’ll let \\(X\\) denote the final result.","tags":null,"title":"Mixture distributions","type":"post"},{"authors":null,"categories":null,"content":" Getting Started with GitHub and git \u0026ndash; with RStudio September 12, 2017\nToday, we\u0026rsquo;ll aim to:\n have a github repo set up have that repo cloned locally get fluent in RStudio\u0026rsquo;s git client if time permits: RMarkdown  Working on GitHub  Let\u0026rsquo;s start with a survey of github. Let\u0026rsquo;s use the MDS public repo as an example. Notice\u0026hellip;  Directory: You can see a directory of files, along with a README (in other folders, too).  Can\u0026rsquo;t make folders on gh\u0026hellip; need to make locally.  File Rendering: GitHub renders certain file types nicely.  Markdown! csv/tsv! pdf! Not html :( Basically get a free website. Make it browsable!.  Editing: Use the \u0026ldquo;pen\u0026rdquo; icon. Then \u0026ldquo;commit\u0026rdquo;. Not so good for big changes (can\u0026rsquo;t save) \u0026ndash; we\u0026rsquo;ll see how to make changes locally later. Commits: List that shows the entire \u0026ldquo;history\u0026rdquo; of the project since you initiated git. See at the repo\u0026rsquo;s \u0026ldquo;home\u0026rdquo; directory. diffs: Click on a commit to see what was changed! File History and Blame: History shows the commit history of the file; blame shows who did what. Issues: A way for open dialogue to happen amongst your team.  Exercise 1:  Create a new public github repo. Initiate with a README file. Edit the README file by adding some new text.  Exercise 2: Navigate to the github repo that makes the stat545.com website. Investigate the file cm001_course-intro-sw-install-account-signup.md in the main repo.  When was the last change made? By whom? What was added on line 10 in the most recent change? Who was the last person to change line 17? When was the first commit made? By whom? What did the file look like after that change?  Exercise 3: Collaboration  Add your neighbour as a collaborator to your github repo. Go to Settings -\u0026gt; Collaborators, and type in their github username. Modify your neighbour\u0026rsquo;s README by adding some text, like \u0026ldquo;was here.\u0026rdquo; Add an issue titled something like \u0026ldquo;README should be reviewed\u0026rdquo; and tag the repo owner by preceding their username with @. Repo owners: check your email. You should have a notification. Close the issue (or comment+close).  NOTE: Click \u0026ldquo;Watch\u0026rdquo; to be notified of issues even if your aren\u0026rsquo;t tagged!   Exercise 4: If there\u0026rsquo;s enough time: forking.  Fork your other neighbour\u0026rsquo;s repo by navigating to their repo and clicking \u0026ldquo;fork\u0026rdquo;.  You\u0026rsquo;ll have your own copy of their repo on your account.  Add a new file called testing.md, and add some text to it. Make a pull request by clicking pull request. Be sure to leave a comment. Owners of the repo: Accept the pull request.  What happened in Exercise 4? A separate copy of the repo was modified, and merged to the main repo.\n Some plain markdown\n  Working Locally  Clone! Or, you could start new. See JB\u0026rsquo;s Happy book for different orderings.  git init if you\u0026rsquo;re starting from an existing project!! Otherwise, you\u0026rsquo;ll try to version everything!  Set up git with RStudio. Cache credentials. Link to gh.  git is a program with no interface. RStudio/others provide an interface, on your computer. install.packages(\u0026quot;knitr\u0026quot;)  New R script; compile notebook. Stage, commit, pull, push. Advanced stuff:  FYI: Alternatives to git: bare repo on your own server. Branch. Revert. Merge. Merge conflicts. Do via source tree, or bash.   R Markdown A start, if we get to it!\n Differences between markdown and R markdown.  latex (what is? Super useful for theses.) code chunks!  How to render: rmarkdown::render or click \u0026ldquo;knit\u0026rdquo;. What gets created? Rmd -\u0026gt; md -\u0026gt; html/pdf. this slide  output_format: github_document. Others are available\u0026hellip;  YAML header.  Lessons  Working with git:  local repository: a self-contained project on your computer remote repository: that project on some \u0026ldquo;cloud\u0026rdquo;, like GitHub, BitBucket, your own server, \u0026hellip; git client: program on your computer that helps you use git (commit, push, pull, \u0026hellip;)  Adopt git in your workflow! This means:  Dedicate a self-contained directory to your project. Set up an R project for the directory. Set up git for that directory. Save perpetually, commit often, pull/push (to github) periodically!  git can be painful.  Problems? Often best to just burn it down and start fresh\u0026hellip; Documentation difficult to read. Check out [this parody]((https://git-man-page-generator.lokaltog.net/) of github documentation. Use a git client to make it easier to work with git. RStudio, Source Tree, GitHub desktop, \u0026hellip;   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4c942575c10ea01a208684cd09f7d7ff","permalink":"/notes/stat545cm003/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/stat545cm003/","section":"notes","summary":"Getting Started with GitHub and git \u0026ndash; with RStudio September 12, 2017\nToday, we\u0026rsquo;ll aim to:\n have a github repo set up have that repo cloned locally get fluent in RStudio\u0026rsquo;s git client if time permits: RMarkdown  Working on GitHub  Let\u0026rsquo;s start with a survey of github. Let\u0026rsquo;s use the MDS public repo as an example. Notice\u0026hellip;  Directory: You can see a directory of files, along with a README (in other folders, too).","tags":null,"title":"","type":"notes"},{"authors":null,"categories":null,"content":" The Binomial Distribution [ ] Materials: Paper and pen/pencil.\nPre-quiz  Suppose you have 6-sided and fair dice. What\u0026rsquo;s the probability of rolling:  A 3 on the roll of a die? Snake-eyes? (i.e., two 1\u0026rsquo;s after rolling two dice).  On a given day, there\u0026rsquo;s a 50% chance that your toaster works. What\u0026rsquo;s the probability that\u0026hellip;  your toaster works exactly one day in a given week? BONUS: your toaster works at least one day in a given week?   [ ] Pre-quiz done!\nShare your answers How did you come up with those numbers?\nThese can all be calculated using the __Binomial distribution_!\nAnyone familiar?\n[ ] Sharing done!\nLearning Objectives By the end of the lesson, when faced with the task of computing a probability, learners are anticipated to be able to:\n Identify whether the probability can be computed using the binomial distribution. Apply the binomial distribution formula to compute the probability.  [ ] Objectives done!\n Can the Binomial distribution be used to compute a probability? \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;  You need two ingredients.\nIngredient #1: Outcome = number of \u0026ldquo;wins\u0026rdquo; of independent \u0026ldquo;games\u0026rdquo;.\nQuestion: What are the \u0026ldquo;games\u0026rdquo; in the pre-quiz questions? What\u0026rsquo;s a \u0026ldquo;win\u0026rdquo;?\nI\u0026rsquo;ll do Q1 Can you do Q2?  [ ] Ingredient 1 done!\nIngredient #2: The probability of winning a game stays the same across games.\n[ ] Question: What are the probabilities of winning a game for Q1 and Q2? Are they the same across games?\n[ ] Ingredient 2 done! All ingredients done!\n How to use the Binomial distribution to compute a probability \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-  Formula for x wins out of n games. Probability of success p:\n$$\\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$$\nActivity:\nLet\u0026rsquo;s work through Q1 together You do Q2 on your own. Write it down! Share/revise answers with peer. Communicate answers  [ ] Using the formula = done!\nSummary  Use the binomial distribution to compute probability of number of \u0026ldquo;wins\u0026rdquo;. A \u0026ldquo;win\u0026rdquo; can be so many things!  rain (vs. not rain) getting to work in less than 30 minutes (vs. not) passing a course (vs. not) etc\u0026hellip;   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"721f0e913444969776fe7d3bbfe070bb","permalink":"/notes/binomial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/binomial/","section":"notes","summary":"The Binomial Distribution [ ] Materials: Paper and pen/pencil.\nPre-quiz  Suppose you have 6-sided and fair dice. What\u0026rsquo;s the probability of rolling:  A 3 on the roll of a die? Snake-eyes? (i.e., two 1\u0026rsquo;s after rolling two dice).  On a given day, there\u0026rsquo;s a 50% chance that your toaster works. What\u0026rsquo;s the probability that\u0026hellip;  your toaster works exactly one day in a given week? BONUS: your toaster works at least one day in a given week?","tags":null,"title":"","type":"notes"},{"authors":null,"categories":null,"content":"For many years, I\u0026rsquo;ve been looking for a clean and sensible way to organize my analytical project files and folders. But being mindful of file organization isn\u0026rsquo;t enough, as my half-organized PhD-related files an attest.\nWhat is a \u0026ldquo;good\u0026rdquo; organization system, anyway? Your project folder should have the following traits.\n Navigable. Someone going to your project folder should know what\u0026rsquo;s there and where to find it. Reproducible. Someone that\u0026rsquo;s not current-you should be able to reproduce the analysis. Ideally, with the stroke of a button. Self-containted. All files related to the analysis should be in your project folder.  Your project folder is essentially all files that are necessary (and useful) for creating the final manuscript.\nBut usually, there are other files that are specific to you, and not required by the manuscript, such as research and meeting notes. I\u0026rsquo;ve therefore found that it\u0026rsquo;s useful to keep two folders: your project file, and a companion developer\u0026rsquo;s folder personal to yourself.\nDon\u0026rsquo;t be afraid to duplicate files! For example, your BibTeX documentary should live in the project folder \u0026ndash; even if you only ever draw on one central bibliography. But, this file should be considered as output, not a file to be mofidied in itself. For example, make an automator script to update the bibliography from your central bibiography, and put that script in your developer\u0026rsquo;s folder.\nhttps://github.ubc.ca/ubc-mds-2017/general/blob/master/general_lab_instructions.md\n Code files contributing to my analysis go in the src directory. Project organization files go in the root directory. This includes:  A Makefile or shell scripts This does not include except for the driver scripts (Shell script and Makefile which call your analysis scripts).  all rendered documents and visualizations you create live in the results directory any data present goes in the data directory answers to written questions live in the doc directory.  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e52d6d7b0ca4694a138e5e8c6cb488ce","permalink":"/notes/organizing_proj_dir/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/organizing_proj_dir/","section":"notes","summary":"For many years, I\u0026rsquo;ve been looking for a clean and sensible way to organize my analytical project files and folders. But being mindful of file organization isn\u0026rsquo;t enough, as my half-organized PhD-related files an attest.\nWhat is a \u0026ldquo;good\u0026rdquo; organization system, anyway? Your project folder should have the following traits.\n Navigable. Someone going to your project folder should know what\u0026rsquo;s there and where to find it. Reproducible. Someone that\u0026rsquo;s not current-you should be able to reproduce the analysis.","tags":null,"title":"","type":"notes"},{"authors":null,"categories":null,"content":" Improv and Life Who\u0026rsquo;s seen improv shows? Knows lessons?\nLearning Objective By the end of today\u0026rsquo;s lesson, using two activities based on improv theatre, learners are anticipated to\n Demonstrate how saying yes to ideas opens up new possibilities in work and life.  Activity #1: Yes, and\u0026hellip;; Yeah, but\u0026hellip; Part 1:\n Pair up. Someone starts: \u0026ldquo;Hey! Let\u0026rsquo;s [activity]!!\u0026rdquo; Go back and forth: \u0026ldquo;Yes! And, [\u0026hellip;]\u0026rdquo;. I\u0026rsquo;ll stop you after 1 minute. Rule: you must give an overly enthusiastic \u0026ldquo;yes\u0026rdquo;.  Part 2:\nSame thing, but say \u0026ldquo;yeah, but\u0026hellip;\u0026rdquo;\n Rule: appear displeased with the others\u0026rsquo; suggestion.  Discussion:\n Any take-aways? When is either useful? Which type of person are you?  Activity #2: Story time Teams: one big team of 5.\nGoal: Build a cohesive story in a team, using 5 parts.\nEach person gets a component of the story:\n Setting characters problem stakes solution  Discussion:\n Any take-aways?  Summary Saying yes in group settings leads to new opportunities!\nUse \u0026ldquo;yeah, but\u0026hellip;\u0026rdquo; to refine ideas.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7cc5b74ce2717611f95f726bb29d5f40","permalink":"/notes/yes_and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/yes_and/","section":"notes","summary":"Improv and Life Who\u0026rsquo;s seen improv shows? Knows lessons?\nLearning Objective By the end of today\u0026rsquo;s lesson, using two activities based on improv theatre, learners are anticipated to\n Demonstrate how saying yes to ideas opens up new possibilities in work and life.  Activity #1: Yes, and\u0026hellip;; Yeah, but\u0026hellip; Part 1:\n Pair up. Someone starts: \u0026ldquo;Hey! Let\u0026rsquo;s [activity]!!\u0026rdquo; Go back and forth: \u0026ldquo;Yes! And, [\u0026hellip;]\u0026rdquo;. I\u0026rsquo;ll stop you after 1 minute.","tags":null,"title":"","type":"notes"},{"authors":null,"categories":null,"content":" Outline  Classification and Regression Trees   Motivation Some problems (or at least potential problems) with the local methods introduced last time:\nThey lack interpretation.  It’s not easy to say how the predictors influence the response from the fitted model.  They typically require a data-rich situation so that the estimation variance is acceptable, without compromising the estimation bias.  We’ll look at classification and regression trees as another method (sometimes called CART). CARTs are not necessarily better, and in fact, tend to be poor competitors to other methods – but ensemble extensions of these tend to perform well (a topic for next Monday).\nOur setting this time is the usual: we have a response \\(Y\\) (either categorical or numeric), and hope to predict this response using \\(p\\) predictors \\(X_1,\\ldots,X_p\\).\n When the response is categorical, we aim to estimate the mode and take that as our prediction. When the response is numeric, we aim to estimate the mean, and take that as our prediction.   Stumps: A preliminary concept What are they? Let’s say I get an upset stomach once in a while, and I suspect certain foods might be responsible. My response and predictors are:\n \\(Y\\): sick or not sick (categorical) \\(X_1\\): amount of eggs consumed in a day. \\(X_2\\): amount of milk consumed in a day, in liters.  You make a food diary, and record the following:\n  Eggs Milk Sick?    0 0.7 Yes  1 0.6 Yes  0 0 No  1 0.7 Yes  1 0 Yes  0 0.4 No    (Example from Mark Schmidt’s CPSC 340)\nA decision/classification stump is a decision on \\(Y\\) based on the value of one of the predictors. You can make one by doing the following steps:\nChoose a predictor (in this case, Milk or Eggs?) Choose a cutpoint on that predictor:  Subset the data having that predictor less than that cutpoint, and take the mode as your decision/classification. Subset the data having that predictor greater than that cutpoint, and take the mode as your decision/classification.   This will get you a decision/classification stump, like so:\n (Image attribution: Hyeju Jang, DSCI 571)\nThe same idea applies in the regression case, except we take the average of the subsetted data, and choose the best one according to the mean squared error.\nSome things to note:\n A decision on a numeric variable is always made based on a threshold (either less than or greater than). Although it makes sense to talk about a stump based on a more complicated decision, allowing for more complicated decisions would increase the search space, and would make fitting these models impractical. Plus, sometimes a more complicated decision can be broken down into several single-threshold decisions. It follows that decisions are always binary. Again, although it makes sense to talk about a stump that has more than two options, we can usually write these as a tree.   Choosing the best stumps Question: What is the error of the above decision stump? If we decide to make a prediction without a decision stump (or any predictors at all), what would the best prediction be, and what’s the error?\nWe have many stumps we can choose from – which one is best?\nClassification:\n The one that gives the least error.  Tends to have a harder time disambiguating several options.  The Gini and Entropy criteria are two of the more commonly used measures.  These are measures of purity.   Regression:\n The one that gives the least mean squared error.    Partitioning of predictor space How does a stump partition the predictor space?\n Trees Notice that a tree is just a bunch of stumps!\nThis means:\n1. For numeric predictors, decisions can only involve one threshold. 2. Decisions are always binary. Optimal tree – computationally infeasible Ideally, we’d examine the error from every possible regression tree, and choose the best one. But there are far too many trees possible! We turn to recursive binary splitting as the next best thing.\n Recursive Binary Splitting The idea to making a tree is to take a greedy approach: keep adding decision stumps, each time choosing the best stump.\nThis is called “greedy” because this method may not end up with the best tree – it’s not computationally feasible to look many steps ahead to see whether a different path is more feasible.\nAdding “Eggs \u0026lt; 1” to the above stump will be the next best iteration – in this case, resulting in 100% prediction accuracy (on the training data). Notice that every time we add a stump, the error on the training set decreases.\nWe keep iterating until we reach some stopping criterion, often based on things such as:\n Tree depth Number of observations in a leaf becomes too small Error is not reduced significantly  Questions:\n How does “tree size” affect the bias-variance tradeoff? How can you choose these parameters?   Pruning Sometimes, a stopping criterion may cause the tree to stop growing prematurely. For example, perhaps we need an insignificant stump to form before getting a more significant one.\nTo avoid this, we take the approach of growing an overly large (overfit) tree, and then pruning it. We won’t get into details of how the tree is pruned back, but one technique is called cost complexity pruning. The general idea is to control a tuning parameter \\(\\alpha\\) to control how much pruning is done – it can be chosen by cross-validation, or the validation set approach.\n  Exercise  Coding a decision tree with tree::tree() in R, using the titanic data. predict() and plot(), and “S3 generics” in R.   Classification and Regression Trees: in R To fit classification and regression trees in R, we use the package tree and the function tree(), which works similarly to lm() and loess():\nsuppressPackageStartupMessages(library(tree)) suppressPackageStartupMessages(library(tidyverse)) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 fit \u0026lt;- tree(Sepal.Width ~ ., data=iris) summary(fit) ## ## Regression tree: ## tree(formula = Sepal.Width ~ ., data = iris) ## Variables actually used in tree construction: ## [1] \u0026quot;Petal.Length\u0026quot; \u0026quot;Sepal.Length\u0026quot; \u0026quot;Petal.Width\u0026quot; ## Number of terminal nodes: 10 ## Residual mean deviance: 0.06268 = 8.776 / 140 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.60560 -0.16780 0.03182 0.00000 0.16280 0.63180 plot(fit) # Plot the tree, without labels text(fit) # Add labels to the tree (Messy – would need to modify the predictor names). Make predictions with predict:\npredict(fit, newdata=iris) %\u0026gt;% head ## 1 2 3 4 5 6 ## 3.635294 3.273913 3.273913 3.273913 3.273913 3.635294 If you want to control the depth of the tree, use the tree.control function in the control argument. Arguments of tree.control that are relevant are:\n mindev: The minimum error reduction acceptable before stopping the tree growth. minsize: The minimum number of observations required in a node in order for a tree to keep growing.  Let’s fit a tree to the max, and check its MSE:\nfitfull \u0026lt;- tree(Sepal.Width ~ ., data=iris, control=tree.control(nrow(iris), mindev=0, minsize=2)) mean((predict(fitfull) - iris$Sepal.Width)^2) ## [1] 0.0008666667 You can investigate the pruning of a tree via cross validation using the cv.tree function. Specify FUN=prune.misclass if you want to prune based on classification error instead of purity measurements, for classification. It returns a list, where the important components are named \u0026quot;size\u0026quot; (number of terminal nodes) and \u0026quot;dev\u0026quot; (the error). Let’s plot those:\nset.seed(4) fitfull_cv \u0026lt;- cv.tree(fitfull) plot(fitfull_cv$size, log(fitfull_cv$dev)) The x-axis represents the number of terminal nodes present in the subtree. The y-axis is the cross-validation error for that subtree.\nFrom the plot, it looks like it’s best to prune the tree to have approximately 10 terminal nodes. Use prune.tree(fitfull, best=10) to prune the tree to 10 terminal nodes.\n*Note: if you encounter an error running prune.tree(fitfull, best=10), it’s not a true error (I believe it’s only an error to the print call, which is called by default). Wrap the code with try:\nfit_pruned \u0026lt;- try(prune.tree(fitfull, best=10)) plot(fit_pruned) text(fit_pruned)  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"68ac929d3dd42455efec2c55dd73e752","permalink":"/post/cart/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/cart/","section":"post","summary":"Outline  Classification and Regression Trees   Motivation Some problems (or at least potential problems) with the local methods introduced last time:\nThey lack interpretation.  It’s not easy to say how the predictors influence the response from the fitted model.  They typically require a data-rich situation so that the estimation variance is acceptable, without compromising the estimation bias.  We’ll look at classification and regression trees as another method (sometimes called CART).","tags":null,"title":"BAIT 509 Class Meeting 05","type":"post"},{"authors":null,"categories":null,"content":" Motivation: why ensembles? Because a classification or regression tree alone tends to be a poor competitor against other machine learning methods. In particular, they tend to be sensitive to the data: if fit to a separate training set, a completely different tree is prone to being fit. This is an embodiment of high variance.\nConsider the hypothetical situation: collect \\(B\\) data sets (of equal size), and fit a tree to all \\(B\\) of them. These \\(B\\) models are called an ensemble. Then if we want to predict on a new case (i.e., we’ve observed the predictors but not the response), take the average predictions of the \\(B\\) trees in the case of regression, or the popular vote of the \\(B\\) trees in the case of classification. This will reduce variance.\nBut collecting \\(B\\) data sets is not practical, and splitting our single data set into \\(B\\) parts would not be a substitute, because we’d only be skyrocketing the variance of each tree before then reducing it in the ensemble.\n Bagging (= Bootstrap Aggregating) We can emulate the above hypothetical situation with a technique called the bootstrap. If your data set has \\(n\\) observations, then we can draw (for all intents and purposes) any number \\(B\\) of data sets we like. To generate one data set, just randomly sample \\(n\\) observations with replacement, and ta-da!\nThe \\(B\\) data sets are related in some sense, so are not as good as having \\(B\\) independent data sets. But it still gives us something useful – in fact, tremendously useful. Go ahead and fit a tree on each data set, and combine the results – that’s bagging in a nutshell.\nNote that we deliberately tend to overfit each tree in the ensemble, to get trees with low bias and high variance – the variance of which will be reduced in the ensemble.\nThis concept of bootstrap is very widespread – it’s not just used for trees, and not even just for machine learning. But for BAIT 509, trees are the only context you’ll see bootstrap in.\nSize of \\(B\\) Note that we can’t overfit by increasing \\(B\\), because this just results in new data sets being generated – not fitting more and more models to a single data set. The error (MSE or error rate) will drop as \\(B\\) increases, until it reaches a stable point where it no longer drops. Once this point is reached, increasing \\(B\\) does not do us much good. This is a common approach for choosing \\(B\\) – no need for cross validation.\n Out of Bag (OOB) error With bagging comes a unique opportunity to calculate the out-of-sample/test error without having to partition the data into training and test sets (either through the validation set method, or cross validation). Here’s how.\nRemember how we obtain a single bootstrap data set: sample with replacement \\(n\\) times. This means that, for each bootstrap sample, there will be some observations that were left out. These are called out-of-bag (OOB) observations, and naturally form a test set!\nNow, obtain an estimate of generalization error (such as MSE or error rate) like so: for each observation in your full data set, consider the trees for which this observation is OOB, and use those to form an ensemble prediction. Compare this against the true value to get the error for this observation. Repeat for all observations.\n Predictor Importance We can use ensembles to determine the importance of certain predictors over others. Recall that the addition of a stump to a tree reduces the training error. We can set up a “points system”, where a reduction in MSE is “awarded” to the predictor responsible for the stump. Do this for all nodes in a tree to come up with a final score for each predictor – the ones with the largest scores are most important.\n  Random Forests One problem with Bagging is that the trees in the ensemble tend to be correlated – that is, they share similarities.\nRandom forests attempt to fix this problem by modifying how a single tree in the ensemble is grown. Recall that, when making a new stump to grow a tree, we choose one predictor out of the total \\(p\\) predictors. The idea behind random forests is to restrict this choice to some random subset of \\(m\\) predictors out of the \\(p\\). A new batch of \\(m\\) predictors is selected each time a stump is to be made.\nThe result is an ensemble of trees that look “more random” – they are said to be decorrelated. This prevents any one predictor from “dominating” the ensemble. And because the trees are less related, combining their predictions results in an overall better result.\n Discussion Questions  Bagging is a special case of random forests under which case? What are the hyperparameters we can control for random forests? Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not? (1,0), (1,2), (1,5) (1,2), (2,0) (1,2), (1,2), (1,5)  For each of the above valid bootstapped data sets, which observations are out-of-bag (OOB)? You make a random forest consisting of four trees. You obtain a new observation of predictors, and would like to predict the response. What would your prediction be in the following cases? Regression: your trees make the following four predictions: 1,1,3,3. Classification: your trees make the following four predictions: “A”, “A”, “B”, “C”.    Boosting Boosting is another method, different from random forests and bagging, but still involves combining predictions of an ensemble.\nThe details are beyond the scope of this course, so we will explain the main ideas. If you truly want a more comprehensive treatment, I suggest reading this Kaggle blog post.\nBasic boosting (Also see the “Motivation” part of the above Kaggle blog – this part is not overly technical).\nLet’s look at a simple two-tree boosting ensemble for regression.\nFit a tree to your data Compute the residuals (actual minus prediction). Fit a second tree to the residuals.  To make a prediction on a new observation, do the following:\nFeed the predictors into the first tree to get a “preliminary” prediction. Feed the predictors into the second tree to get an adjustment. Obtain a final prediction by adding the adjustment to the preliminary prediction.  This second tree captures patterns in the data that the first tree missed, which is why boosting is so useful.\nBoosting, then, is a continuation of this, fitting trees in sequence.\n A conglomerate of “weak learners” Boosting gradually improves predictions by learning on residuals. Because of this, there is no need to build a “strong” model for each iteration – i.e., one that does well at prediction.\nInstead, we deliberately build weak models to slowly get at the structure underyling the data. We therefore build low-depth trees for each member of the ensemble.\n Boosting for Classification The adjustments made here are less interpretable, but do follow a similar logic. Instead of learning on residuals, a consecutive tree leans on classes re-weighted so that observations that are incorrectly classified get a higher weight. This is called adaboost.\n Learning Rate It turns out that we obtain a more powerful prediction if we slow down the “rate of learning”. We introduce a “rate of learning” hyperparameter \\(\\lambda\\) between 0 and 1. Predictions from trees are multiplied by this amount before adjusting the prediction from the previous tree.\n  Ensembles in R randomForest We use the randomForest package and the randomForest function in R to implement random forests (and thus also bagging for classification and regression trees). The syntax follows R’s regression paradigm with randomForest(response~predictors, data). Let’s see an example with the mtcars data (a default data frame in R), predicting mpg (a regression problem):\nsuppressPackageStartupMessages(library(randomForest)) set.seed(40) my_fit \u0026lt;- randomForest(mpg ~ ., data=mtcars) Note that the . stands for “all other variables in the data frame”.\nThe plot function is useful for giving a quick-and-dirty plot of error vs. \\(B\\):\nplot(my_fit) There’s stability after around \\(B=200\\) trees.\nWarning! The predict function works differently than usual. Usually, the following two outputs would be the same:\nyhat1 \u0026lt;- predict(my_fit) yhat2 \u0026lt;- predict(my_fit, newdata=mtcars) But they’re not. Take a look:\nplot(yhat1, yhat2) What’s going on here? It turns out predict(my_fit) without specifying newdata gives the out of bag predictions, whereas the entire ensemble is used to make predictions when the newdata argument is specified. So the OOB error and training errors, respectively, are:\nmean((yhat1 - mtcars$mpg)^2) ## [1] 5.655269 mean((yhat2 - mtcars$mpg)^2) ## [1] 1.49721 Here are two key parameters you can change in the randomForest function:\n mtry: The number of predictors to sample at every stump iteration.  Set equal to the total number of predictors to perform bagging.  ntree: The number of trees to fit.   Boosting Boosting is not on your Assignment 2. For your reference, I’ll indicate some useful implementations here.\n For regression, you can use the gbm function from the gbm R package to do boosting. But you can’t do classification beyond 2 classes. For classification, I recommend the AdaBoostClassifier from the sklearn.ensemble library.  This library also contains RandomForestClassifier for random forest classification. For plain decision trees (again for classification), the sklearn.tree library has a DecisionTreeClassifier method.     ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc67619cd99f5da9dd85e982cfc1e61e","permalink":"/post/ensembles/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/ensembles/","section":"post","summary":"Motivation: why ensembles? Because a classification or regression tree alone tends to be a poor competitor against other machine learning methods. In particular, they tend to be sensitive to the data: if fit to a separate training set, a completely different tree is prone to being fit. This is an embodiment of high variance.\nConsider the hypothetical situation: collect \\(B\\) data sets (of equal size), and fit a tree to all \\(B\\) of them.","tags":null,"title":"BAIT 509 Class Meeting 07","type":"post"},{"authors":null,"categories":null,"content":" Overview  Maximal Margin Classifier, and hyperplanes Support Vector Classifiers (SVC) Support Vector Machines (SVM) Extensions to multiple classes SVM’s in python Feedback on Assignment 1 Lab: work on Assignment 3   The setup Today, we’ll dicuss a new method for binary classification – that is, classification when there are two categories. The method is called Support Vector Machines. We’ll build up to it by considering two special cases:\nThe Maximal Margin Classifier (too restrictive to use in practice) The Support Vector Classifier (linear version of SVM)  We’ll demonstrate concepts when there are two predictors, because it’s more difficult to visualize in higher dimensions. But concepts generalize.\nLet’s start by loading some useful R packages to demonstrate concepts.\nsuppressPackageStartupMessages(library(tidyverse)) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 knitr::opts_chunk$set(fig.width=6, fig.height=3, fig.align=\u0026quot;center\u0026quot;)  Maximal Margin Classifier Setup Consider the following two-predictor example. The response can take on one of two categories: “A” or “B”. Also consider the three classifiers, where above a line we predict “B”, and below, we predict “A”:\nEach line perfectly classifies the training data – which one would you prefer, and why?\n The Method The Maximal Margin Classifier only applies when the two groups can be perfectly separated on the training set by:\n a dividing line (if we have 2 predictors), a dividing plane (if we have 3 predictors), or in general, we need a dividing hyperplane.  We choose the line/hyperplane so that the observations closest to the line/hyperplane are as far away as possible. This minimizes the chance that a new observation will be misclassified.\nWe can say this another way. Notice that, for any given line, we can define the “widest slab” before touching an observation. Here are the widest slabs for the above cases:\nThe Maximal Marginal Classifier chooses the line whose slab width is maximal. Half the “slab width” is called the margin, so this classifier maximizes the margin. There are algorithms to do this maximization.\nWith this in mind, we can order the above three lines from best to worst: Line 2 is the worst, and Line 3 is the best.\nNotice that there are only three observations that define the slab for Line 3 – these are called support vectors.\nBut: a perfect linear classification almost never happens with real data, but this is an important setup before moving on to support vector machines.\n Making Predictions, and Confidence We can classify new points as follows:\nIf \\((x1,x2)\\) lies above the line, predict “B”. If \\((x1,x2)\\) lies below the line, predict “A”.  But, we need a way to automate this, since orientation is not always natural with a hyperplane (for example, we have to switch to “left and right” if the dividing line is vertical).\nThe idea here is to write the equation of the line/hyperplane as \\[ \\beta_0 x_1 + \\beta_1 x_2 + \\beta_3 = 0. \\] We can then classify based on the sign of the left-hand-side of the equation. Note that this can be written in the usual y=mx+b format as \\[ x_2 = \\left(-\\frac{\\beta_0}{\\beta_1}\\right) x_1 + \\left(- \\frac{\\beta_3}{\\beta_1}\\right), \\] but this is only useful for plotting.\nFor the above classifier, the line is \\[ 0.59 x_1 - 0.66 x_2 + 0.46 = 0, \\] and we classify “B” whenever \\[ 0.59 x_1 - 0.66 x_2 + 0.46 \u0026lt; 0, \\] and “A” whenever \\[ 0.59 x_1 - 0.66 x_2 + 0.46 \u0026gt; 0. \\] You can figure out which is which by just trying a point that you know the classification of.\nThis equation also gives us a measure of confidence in our classification – the further the magnitude of the left-hand-side is from 0, the further away from the separating hyperplane it is.\n  Support Vector Classifiers Support Vector Classifiers (or SVC) are more realistic in the sense that they don’t require perfect separability of the data.\nConsider now the following data:\nA SVC fits a line (or in general, a hyperplane) that “best” separates the data in some way. The idea, as before, is to choose the line that results in the biggest margin. The only problem is, there’s no such thing as a maximal margin for a given line anymore. Let’s work on defining that.\nHere’s an example of a line and a margin (not necessarily a “maximal” margin – just some random margin that I chose). The dividing line is the middle one, so above this line is a “B” prediction. We’ll call the upper and lower lines the “margin boundaries”.\nThe idea is to calculate a total penalty associated with each observation with this line-margin combination, like so:\nObservations that are both correctly classified and outside of the “slab” do not receive any penalty. Observations that are correctly classified, but within the margin, receive a penalty equal to the proportion of the way through the margin they are.  This means that observations lying on the classification boundary receive a penalty of 1, because they are entirely one margin width away from its margin boundary.  Step (2) can be generalized to observations that are misclassified – they receive a penalty equal to the number of margin widths they are away from their margin boundary.  This means that an “A” that’s on B’s margin boundary will receive a penalty of 2.   The numbered observations in the above plot receive a penalty:\n Penalty between 0 and 1: observations 2,3,5 Penalty between 1 and 2: observation 6 Penalty greater than 2: 1 and 4.  Add up the penalties to obtain a total penalty.\nIf we choose a maximum allowable total penalty, say \\(C\\), then for any line, there’s a set of margin widths that result in a penalty less than \\(C\\). The maximal margin for that line is the biggest margin.\nAgain, the algorithm chooses the line that has the biggest maximal margin, for a given total penalty \\(C\\).\nNote that, for the above plot, observations 1-6 are called support vectors, because they are the only ones to contribute to a penalty.\n In-class exercises Consider the following data, decision boundary, and margin boundaries.\nConstruct the decision rule according to this classification boundary. How would you classify a new observation that has \\(x_1=6\\) and \\(x_2=10\\)?   If x1\u0026lt;7.5, then “A”. Else “B”. For the above example, since x1=6\u0026lt;7.5, we would classify “A”.\n What size is the margin here?   The margin width is 2 units.\n Which observations receive a penalty? Which observations are the support vectors?   The observations that receive a penalty are 6,7,8,9,10. These are also the support vectors, by definition.\n What is the total penalty here?   The penalty, added in order of observations 6-10, is 0.25+0.25+1.25+1.75+2.25 = 5.75.\n Can I choose a bigger margin if my total allowable penalty is 6?   Yes – increasing the margin will eventually lead to an increase in penalty, which is allowable.\n Are the data separable? If so, what are the support vectors?   The data are separable – as such, we can apply a maximal margin classifier to it. The support vectors would be 4,8,9.\n  Support Vector Machines Quite often, a linear boundary is not useful for classification. Take the below extreme example:\nRecall in linear regression, we can fit other shapes besides lines by transforming the predictors, such as adding powers of the predictor to get polynomials.\nWe can do this to get non-linear decision boundaries on the original predictor space, but it’s usually quite computationally expensive. The way to fix the problem is beyond the scope of this course, but the idea is to use kernel functions. The kernel function typically has a hyperparameter associated with it. Two of the most popular examples:\nThe polynomial kernel, with hyperparameter = the degree of the polynomial. The radial kernel, which has hyperparameter typically denoted “gamma” (a positive number).  Uses nearby training data for classification, where larger values of gamma allow for data to be further.   Note that SVC and Maximum Margin Classification are special cases of SVM.\n Multi-class Prediction If there are more than two classes to predict, we can use one of two approaches to do the classification. Suppose there are \\(K\\) categories.\nApproach 1: One-vs-one, or all-pairs This approach fits an SVM to all pairs of categories. That is:\nSubset the data to only include two of the \\(K\\) categories. Fit SVM Repeat 1-2 for all possible pairs.  Classification is made using the “popular vote”.\n Approach 2: One-vs-all This approach fits an SVM for each category against “other”. That’s \\(K\\) SVM’s in total. That is:\nChoose a category \\(j\\). Lump all other categories into one category called “other”. Fit SVM Repeat 1-2 for all \\(K\\) choices of \\(j\\).  Remember the measure of “confidence” introduced in 4.3, as the absolute value of the left-hand-side of the equation of the hyperplane? We choose the category that results in the highest confidence score.\n  SVM in python The scikit-learn documentation for running SVM’s in python is available here.\nIn general, the machine learning paradigm in python (at least with scikit-learn, the go-to machine learning library) is to\nInitialize the method. Fit the model. Query the fit.  You can run classification in python with the sklearn.svm bundle of methods. From this bundle, the method SVC is useful for SVM’s (despite its name), and LinearSVC is a special case when the classification boundary is linear (what we’ve been calling SVC).\nLoad SVC like so:\nfrom sklearn import svm Let’s use a dataset from the documentation\nfrom sklearn.datasets import make_classification X, y = make_classification(n_features=4, random_state=0) X is a matrix of the predictors (as a list of lists), with predictors in the columns, and observations in the rows. y is a list of labels/categories, with length equal to the number of rows of X.\nLet’s use scikit-learn.\nInitialize the model.  We’ll store the initialization in a variable called my_model. We can use the defaul of SVC, like so:\nmy_model = SVC() Or I could have specified hyperparameters here. For example, specify SVC(C=10, kernel=\u0026quot;rbf\u0026quot;) for a penalty of \\(C=10\\), using the radial basis function kernel (you can use the gamma argument to control gamma, too). More details are in the documentation.\nFit the model.  This is typically done in scikit-learn by appending .fit(X, y) to your initialized model, where X and y are as above.\nmy_model.fit(X, y) Recall that, with python, the above code modifies the object my_model.\nQuery the fit  Now we can go ahead and do things with the fit, such as make predictions:\nmy_model.predict(X_new) If X_new is like X (possibly with a different number of rows), then predictions will be made on this new data set. Note that this above code does not modify my_model, like appending .fit does.\nWe can calculate accuracy:\nmy_model.score(X_new, y_new) where y_new is like y, but for X_new.\nWe can also calculate the distance to the decision boundary by appending .decision_function:\nmy_model.decision_function(X)  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"95d8b807ed53480ca3718eb5d449f0f4","permalink":"/post/svm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/svm/","section":"post","summary":"Overview  Maximal Margin Classifier, and hyperplanes Support Vector Classifiers (SVC) Support Vector Machines (SVM) Extensions to multiple classes SVM’s in python Feedback on Assignment 1 Lab: work on Assignment 3   The setup Today, we’ll dicuss a new method for binary classification – that is, classification when there are two categories. The method is called Support Vector Machines. We’ll build up to it by considering two special cases:","tags":null,"title":"BAIT 509 Class Meeting 09","type":"post"},{"authors":null,"categories":null,"content":" Over the years, I’ve discovered that my ideal career is composed of four interrelated pillars, all with respect to data science. In a roughly “cumulative” order, the pillars are application, research, development, and teaching.\nApplication I’m passionate about addressing real problems with data science.\nCurrently, my main focus is on mentoring MDS students for their capstone projects – a 2-month-long project that students complete in collaboration with various organizations.\nFor the future, I intend to also have long-term collaborations with various organizations, to help them achieve missions that resonate with me.\n Research I’m passionate about discovering better ways to do data science, with application in mind.\nCurrently, my main focus is on reframing statistics from the perspective of applications. For example, why the typical “assumptions” of linear regression are not actually needed, and (rather) what we gain when the reality more closely reflects these assumptions.\nAnother focus is on bringing to light the importance of finding indicators of extreme events by measuring tail index reduction in the regression context, and not just the typical variance-reduction measurements such as the coefficient of determination. One approach of doing this is through quantile regression using vine copulas.\n Development I’m passionate about turning new research into programs and tools that are accessible to the public.\nCurrently, my main focus is on developing the Master of Data Science (MDS) program at The University of British Columbia (Vancouver). I’m involved with bettering the teaching and the content of the program, especially focussing on the statistical perspective, often integrated with machine learning. I’m also working on turning our material and lesson plans into a public-facing resource.\nAnother focus is on developing software, currently as packages for the R programming language. Examples include a package to assist in manipulating copulas, and a package for extreme quantile regression.\nFor the future, I intend to also collaborate with software development teams. I also intend to start an organization whose mission reflects the four pillars I define on this page.\n Teaching I’m passionate about spreading data literacy to the world.\nCurrently, my main focus is on teaching in the MDS program.\nFor the future, I intend to also hold data science workshops at research centers and other organizations. I intend to volunteer for data science outreach programs.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d7c8ee28e95b8f3154e4c22ee78d70ea","permalink":"/goals/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/goals/","section":"","summary":"Over the years, I’ve discovered that my ideal career is composed of four interrelated pillars, all with respect to data science. In a roughly “cumulative” order, the pillars are application, research, development, and teaching.\nApplication I’m passionate about addressing real problems with data science.\nCurrently, my main focus is on mentoring MDS students for their capstone projects – a 2-month-long project that students complete in collaboration with various organizations.","tags":null,"title":"Career Goals","type":"page"},{"authors":null,"categories":null,"content":" Does this happen to you? What about bouncing files back and forth amongst collaborators through email? Or even keeping track of your own files between multiple devices?\nDo you currently have a way of handling these sometimes frustrating scenarios? Any horror stories?\n[I developed and delivered this mini-lesson as part of instructional skills training at UBC]\nLearning Outcomes Version control is a powerful solution to these regular dilemmas. By the end of this very brief lesson, learners are anticipated to:\n Understand the central concepts and values of version control Identify areas of your own work that can benefit from version control Name the prominent version control software, and one prominent cloud-based hosting service  What is Version Control? Git? Version control is a widely used solution for keeping track of files between devices and collaborators over time. The most prominent software is called \u0026ldquo;git\u0026rdquo;.\nI like to think of git as the next level to saving a file. Just like regular file saving has (hopefully) become a regular part of your workflow, version control requires you to regularly make snapshots of your files. These snapshots are called commits.\nSave like crazy. Commit frequently.\nThough, commits aren\u0026rsquo;t really \u0026ldquo;snapshots\u0026rdquo;. Git doesn\u0026rsquo;t actually store an entire copy of your file each time you commit! Only the changes (or diffs) are stored, so there\u0026rsquo;s no need to sweat any storage demands!\nWhat is GitHub? There are cloud-based services that provide both file storage and a nice interface for git built into them. Perhaps the most prominent of these services is GitHub (others are Bitbucket and GitLab). The added step here is to push changes from your computer up to GitHub.\nSave like crazy. Commit frequently. Push before you\u0026rsquo;re interrupted by a fire.\nHow exactly are my versioning and collaboration problems solved? In many ways!\n Easily identify changes: file changes are highlighted by GitHub. No more detective work to see exactly what was changed in a file. Retrievable history: you can easily reconstruct your files as they were at any point along the commit history.  No more need to keep loads of file versions!  No more uncertainty as to who has the most up-to-date files. They\u0026rsquo;re on GitHub!  Personally, I\u0026rsquo;ve been using GitHub more and more, if only for the peace of mind of having a retrievable history, and a record of how my thoughts progress over time. And maybe it\u0026rsquo;s just me, but I also find that dedicating myself to committing keeps me focussed, because they force me to think about what I\u0026rsquo;m doing (instead of just doing it).\nOther tools Although git and GitHub always work, they work best with plain-text files. If you work with the Office suite of tools like Word, you should first of all consider whether you should really be using this. If you still want to use Office, then perhaps consider options built into the Office tools, or Google docs.\nBack to you What areas of your own life could benefit from version control with Git and GitHub?\nCheck your email inbox for emails with attachments to get a sense of the areas of your life that might benefit from version control.\nNext time you go to transfer a file, be mindful of the cognitive load that comes with the file transfer. You might be better off using a version control system like git and GitHub.\nLearning More If you want to learn more, I recommend starting with a cloud-based service like GitHub. It\u0026rsquo;s a gentle introduction to the subject, and is easier to use than plain git, and has plenty of helpful guides.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"36293394a9877ce46034996cb21a6cab","permalink":"/post/2018-08-14-git_github/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/2018-08-14-git_github/","section":"post","summary":"Does this happen to you? What about bouncing files back and forth amongst collaborators through email? Or even keeping track of your own files between multiple devices?\nDo you currently have a way of handling these sometimes frustrating scenarios? Any horror stories?\n[I developed and delivered this mini-lesson as part of instructional skills training at UBC]\nLearning Outcomes Version control is a powerful solution to these regular dilemmas. By the end of this very brief lesson, learners are anticipated to:","tags":["git","github","version control","collaboration","workflow"],"title":"Collaboration with Version Control: Git and GitHub","type":"post"},{"authors":null,"categories":null,"content":" Generalized Additive Models To fit a GAM in R, we could use:\nthe function gam in the mgcv package, or the function gam in the gam package.  Differences between the two functions are discussed in the “Details” section of the gam documentation in the mgcv package. Choose one, but don’t load both! mgcv tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that’s what is used in this tutorial. But the gam package has similar workings.\nThe gam function works similarly to other regression functions, but the formula specification is different. Let’s go through different formula specifications, doing regression on the mtcars dataset in R.\nThe formula mpg ~ disp + wt gives you a linear model. It indicates that disp and wt both enter the model in a linear fashion.\nlibrary(mgcv) ## Loading required package: nlme ## This is mgcv 1.8-24. For overview type \u0026#39;help(\u0026quot;mgcv-package\u0026quot;)\u0026#39;. fit1 \u0026lt;- gam(mpg ~ disp + wt, data=mtcars) fit2 \u0026lt;- lm(mpg ~ disp + wt, data=mtcars) summary(fit1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ disp + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## ## R-sq.(adj) = 0.766 Deviance explained = 78.1% ## GCV = 9.3863 Scale est. = 8.5063 n = 32 summary(fit2) ## ## Call: ## lm(formula = mpg ~ disp + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Notice that the coefficient estimates are the same.\nTo make a term non-parametric, wrap the s function around the term (for splines; comes with the mgcv package). The gam package also has a lo function, for loess smoothing.\nfit3 \u0026lt;- gam(mpg ~ s(disp) + s(wt), data=mtcars) summary(fit3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp) + s(wt) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 20.0906 0.3429 58.59 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp) 6.263 7.386 6.373 0.000164 *** ## s(wt) 1.000 1.000 4.015 0.056434 . ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.896 Deviance explained = 92.1% ## GCV = 5.0715 Scale est. = 3.762 n = 32 Now, each predictor enters the model in a non-parametric, additive form. The nonparametric functions can be accessed by calling plot. For documentation, see ?plot.gam. Let’s plot the “bivariate” scatterplots behind these curves too (these bivariate data actually use partial residuals).\nplot(fit3, residuals=TRUE) Looks like the “weight” variable (wt) is quite linear. We can let it be linear, while the disp variable remains nonparametric. “Wiggliness” of the smoothed fit can be controlled through the k argument of the s function, but this is chosen in a “smart” way by default.\nfit4 \u0026lt;- gam(mpg ~ s(disp, k=3) + wt, data=mtcars) summary(fit4) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp, k = 3) + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 31.1110 3.1336 9.928 1.1e-10 *** ## wt -3.4254 0.9649 -3.550 0.00138 ** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp) 1.93 1.995 9.724 0.000758 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.839 Deviance explained = 85.4% ## GCV = 6.659 Scale est. = 5.8413 n = 32 plot(fit4, residuals=TRUE) You can even combine predictors into a common smooth function:\nfit5 \u0026lt;- gam(mpg ~ s(disp, qsec) + wt, data=mtcars) summary(fit5) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## mpg ~ s(disp, qsec) + wt ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 32.181 4.340 7.415 1.83e-07 *** ## wt -3.758 1.345 -2.794 0.0105 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(disp,qsec) 7.634 9.694 5.353 0.000312 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## R-sq.(adj) = 0.903 Deviance explained = 93% ## GCV = 5.0335 Scale est. = 3.5181 n = 32 For each, the predict and residuals functions work in the same old way. Let’s use them to make a residual plot:\nqplot(predict(fit5), residuals(fit5)) + geom_abline(intercept=0, slope=0, linetype=\u0026quot;dashed\u0026quot;) + xlab(\u0026quot;Prediction (mean)\u0026quot;) + ylab(\u0026quot;Residuals\u0026quot;) For their documentation, see ?predict.gam and ?residuals.gam.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"15bc441f2ba1534281e7873a7da1adae","permalink":"/post/gam_in_r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/gam_in_r/","section":"post","summary":"Generalized Additive Models To fit a GAM in R, we could use:\nthe function gam in the mgcv package, or the function gam in the gam package.  Differences between the two functions are discussed in the “Details” section of the gam documentation in the mgcv package. Choose one, but don’t load both! mgcv tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that’s what is used in this tutorial.","tags":null,"title":"DSCI 562 Lab 4 Tutorial: GAM","type":"post"},{"authors":null,"categories":null,"content":" This tutorial discusses the implementation of \\(k\\)-means (and variants) in R.\nLet’s use a simulated dataset. Throughout, we’ll take \\(k=3\\) groups.\nsuppressMessages(library(ggplot2)) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 set.seed(345) x \u0026lt;- rnorm(250) y \u0026lt;- rnorm(250) ## Shift group 2: x[51:150] \u0026lt;- x[51:150] + 2.5 y[51:150] \u0026lt;- y[51:150] - 1 ## Shift group 3: x[151:250] \u0026lt;- x[151:250] - 1.5 y[151:250] \u0026lt;- y[151:250] - 1.5 ## Plot: dat \u0026lt;- data.frame(x=x, y=y) qplot(x, y, alpha=I(0.5)) \\(k\\)-means Basics Let’s fit a \\(k\\)-means algorithm to the data with the kmeans function, to \\(k=3\\) groups. Indicate the data first, then \\(k\\) in the second (centers) argument.\nset.seed(22) (fit1 \u0026lt;- kmeans(dat, 3)) ## K-means clustering with 3 clusters of sizes 60, 87, 103 ## ## Cluster means: ## x y ## 1 0.3725131 0.2750934 ## 2 2.5109499 -1.2199721 ## 3 -1.5793637 -1.5174017 ## ## Clustering vector: ## [1] 1 3 3 3 1 3 3 1 2 1 1 1 1 1 1 1 1 2 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1 3 1 ## [36] 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2 2 2 2 2 2 1 3 2 2 2 2 2 1 2 2 2 2 2 2 ## [71] 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 ## [106] 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 ## [141] 2 2 2 2 2 2 2 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 ## [176] 3 3 1 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 ## [211] 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [246] 3 3 3 3 3 ## ## Within cluster sum of squares by cluster: ## [1] 100.3076 154.1080 170.9162 ## (between_SS / total_SS = 68.4 %) ## ## Available components: ## ## [1] \u0026quot;cluster\u0026quot; \u0026quot;centers\u0026quot; \u0026quot;totss\u0026quot; \u0026quot;withinss\u0026quot; ## [5] \u0026quot;tot.withinss\u0026quot; \u0026quot;betweenss\u0026quot; \u0026quot;size\u0026quot; \u0026quot;iter\u0026quot; ## [9] \u0026quot;ifault\u0026quot; The output tells you what you can extract, under “Available components”. You can extract them using the $ symbol. Let’s look at some of them.\nHere are the assigned clusters:\nfit1$cluster ## [1] 1 3 3 3 1 3 3 1 2 1 1 1 1 1 1 1 1 2 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1 3 1 ## [36] 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2 2 2 2 2 2 1 3 2 2 2 2 2 1 2 2 2 2 2 2 ## [71] 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 ## [106] 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 ## [141] 2 2 2 2 2 2 2 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 ## [176] 3 3 1 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 ## [211] 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [246] 3 3 3 3 3 qplot(x, y, alpha=I(0.5), colour=factor(fit1$cluster)) Here are the three means/centers of each group:\nfit1$centers ## x y ## 1 0.3725131 0.2750934 ## 2 2.5109499 -1.2199721 ## 3 -1.5793637 -1.5174017 We can also extract the squared distances.\nfit1$totss # Total sum of squares. ## [1] 1343.956 fit1$withinss # Within group sum of squares. ## [1] 100.3076 154.1080 170.9162 fit1$tot.withinss # Total within group. **Objective function** ## [1] 425.3318 fit1$betweenss # Between group sum of squares. ## [1] 918.6239  Tweaks In addition to running basic \\(k\\)-means, we can also run multiple \\(k\\)-means with the nstart argument. For each run, different initial values are used (different centroids). The run with the best fit is the one that is reported. What is the “best fit” anyway? It’s the one with the smallest total within group sum of squares – that is, $tot.withinss.\nLet’s take the best of 20 runs:\n(fit2 \u0026lt;- kmeans(dat, 3, nstart=20)) ## K-means clustering with 3 clusters of sizes 60, 103, 87 ## ## Cluster means: ## x y ## 1 0.3725131 0.2750934 ## 2 -1.5793637 -1.5174017 ## 3 2.5109499 -1.2199721 ## ## Clustering vector: ## [1] 1 2 2 2 1 2 2 1 3 1 1 1 1 1 1 1 1 3 2 1 1 1 2 2 1 1 2 1 1 1 1 1 1 2 1 ## [36] 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 3 3 3 3 3 3 1 2 3 3 3 3 3 1 3 3 3 3 3 3 ## [71] 3 3 3 3 1 3 3 3 3 3 3 3 1 1 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 ## [106] 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 1 3 3 1 1 3 3 3 3 3 3 3 3 3 ## [141] 3 3 3 3 3 3 3 1 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 ## [176] 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 ## [211] 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [246] 2 2 2 2 2 ## ## Within cluster sum of squares by cluster: ## [1] 100.3076 170.9162 154.1080 ## (between_SS / total_SS = 68.4 %) ## ## Available components: ## ## [1] \u0026quot;cluster\u0026quot; \u0026quot;centers\u0026quot; \u0026quot;totss\u0026quot; \u0026quot;withinss\u0026quot; ## [5] \u0026quot;tot.withinss\u0026quot; \u0026quot;betweenss\u0026quot; \u0026quot;size\u0026quot; \u0026quot;iter\u0026quot; ## [9] \u0026quot;ifault\u0026quot; The structure of the output is no different from before.\nYou may also choose the centroids to start the algorithm with. To do this, instead of indicating \\(k\\) for the centers argument, use a matrix of the centroids you wish to begin with.\nLet’s start with the true means: (0,0), (2.5,-1), and (-1.5, -1.5):\n(xstart \u0026lt;- matrix(c(0,0, 2.5,-1, -1.5,-1.5), ncol=2, byrow=TRUE)) ## [,1] [,2] ## [1,] 0.0 0.0 ## [2,] 2.5 -1.0 ## [3,] -1.5 -1.5 (fit3 \u0026lt;- kmeans(dat, xstart)) ## K-means clustering with 3 clusters of sizes 60, 87, 103 ## ## Cluster means: ## x y ## 1 0.3725131 0.2750934 ## 2 2.5109499 -1.2199721 ## 3 -1.5793637 -1.5174017 ## ## Clustering vector: ## [1] 1 3 3 3 1 3 3 1 2 1 1 1 1 1 1 1 1 2 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1 3 1 ## [36] 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 2 2 2 2 2 2 1 3 2 2 2 2 2 1 2 2 2 2 2 2 ## [71] 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 ## [106] 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 ## [141] 2 2 2 2 2 2 2 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 ## [176] 3 3 1 3 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 ## [211] 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [246] 3 3 3 3 3 ## ## Within cluster sum of squares by cluster: ## [1] 100.3076 154.1080 170.9162 ## (between_SS / total_SS = 68.4 %) ## ## Available components: ## ## [1] \u0026quot;cluster\u0026quot; \u0026quot;centers\u0026quot; \u0026quot;totss\u0026quot; \u0026quot;withinss\u0026quot; ## [5] \u0026quot;tot.withinss\u0026quot; \u0026quot;betweenss\u0026quot; \u0026quot;size\u0026quot; \u0026quot;iter\u0026quot; ## [9] \u0026quot;ifault\u0026quot;   \\(k\\)-means++ Basics The kmeans function is quite specific – it only looks at squared distances. There’s a package called flexclust that allows for more flexible cluster analysis. The kcca function is particularly useful.\nAt its basic, kcca works similarly to kmeans. In fact, this will run a standard \\(k\\)-means algorithm:\nsuppressMessages(library(flexclust)) (fit4 \u0026lt;- kcca(dat, 3)) ## kcca object of family \u0026#39;kmeans\u0026#39; ## ## call: ## kcca(x = dat, k = 3) ## ## cluster sizes: ## ## 1 2 3 ## 92 59 99 But, the output is less friendly than kmeans (because kcca uses a formal S4 object-oriented format). The multitude of output can be extracted by @ instead of $. For example, here are the cluster assignments, and the final three centers:\nfit4@cluster ## [1] 2 3 3 3 2 3 3 2 1 2 1 2 2 2 2 2 2 1 3 2 2 2 3 3 2 2 2 2 2 2 2 2 2 3 2 ## [36] 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 1 1 1 1 1 1 2 3 1 1 1 1 1 2 1 1 1 1 1 1 ## [71] 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 ## [106] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 1 1 2 2 1 1 1 1 1 1 1 1 1 ## [141] 1 1 1 1 1 1 1 2 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 ## [176] 3 3 2 3 3 3 3 3 3 3 3 1 2 3 3 3 3 2 3 3 2 3 3 3 2 3 3 3 3 2 3 3 3 3 3 ## [211] 3 3 3 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [246] 3 3 3 3 3 fit4@centers ## x y ## [1,] 2.4487745 -1.1781922 ## [2,] 0.1910489 0.3025144 ## [3,] -1.6003047 -1.5694850 Notice that there is no sum of squares output, like there is in kmeans – this is because kcca doesn’t necessarily use the euclidean distance. However, there is a way to convert a kcca output to a kmeans output, though this output is more limited than usual (the only ss available is $withinss):\nfit4b \u0026lt;- as(fit4, \u0026quot;kmeans\u0026quot;) fit4b$withinss ## [1] 167.92353 96.14623 161.87687  Tweaks The most useful thing about kcca regarding Lab 1 is kcca’s ability to do \\(k\\)-means++. This can be done through the control argument.\nThe control argument should be a named list, with each name corresponding to some property you’d like to indicate. See ?cclustControl for the various options. But the one we’re interested in is “initcent”, which controls how the initial centers are chosen. From the documentation, this should be:\n Character string, name of function for initial centroids, currently “randomcent” (the default) and “kmeanspp” are available.\n So to do \\(k\\)-means++, this amounts to:\n(fit5 \u0026lt;- kcca(dat, 3, control=list(initcent=\u0026quot;kmeanspp\u0026quot;))) ## kcca object of family \u0026#39;kmeans\u0026#39; ## ## call: ## kcca(x = dat, k = 3, control = list(initcent = \u0026quot;kmeanspp\u0026quot;)) ## ## cluster sizes: ## ## 1 2 3 ## 95 68 87 Another feature of kcca is that it allows for different distance metrics besides euclidean. This can be indicated through the family argument. Take a look at the kcca documentation under “Predefined Families” to see what distance metrics you can use.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d68caa4840f5ac4dbe50c373f26c182","permalink":"/post/kmeans_in_r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/kmeans_in_r/","section":"post","summary":"This tutorial discusses the implementation of \\(k\\)-means (and variants) in R.\nLet’s use a simulated dataset. Throughout, we’ll take \\(k=3\\) groups.\nsuppressMessages(library(ggplot2)) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 set.seed(345) x \u0026lt;- rnorm(250) y \u0026lt;- rnorm(250) ## Shift group 2: x[51:150] \u0026lt;- x[51:150] + 2.5 y[51:150] \u0026lt;- y[51:150] - 1 ## Shift group 3: x[151:250] \u0026lt;- x[151:250] - 1.5 y[151:250] \u0026lt;- y[151:250] - 1.5 ## Plot: dat \u0026lt;- data.","tags":null,"title":"DSCI 563 Lab1 Tutorial: k-means","type":"post"},{"authors":null,"categories":null,"content":" Who’s seen improv shows? Knows lessons?\nLearning Objective By the end of today’s lesson, using two activities based on improv theatre, learners are anticipated to\n Demonstrate how saying yes to ideas opens up new possibilities in work and life.   Activity #1: Yes, and…; Yeah, but… Part 1:\n Pair up. Someone starts: “Hey! Let’s [activity]!!” Go back and forth: “Yes! And, […]”. I’ll stop you after 1 minute. Rule: you must give an overly enthusiastic “yes”.  Part 2:\nSame thing, but say “yeah, but…”\n Rule: appear displeased with the others’ suggestion.  Discussion:\n Any take-aways? When is either useful? Which type of person are you?   Activity #2: Story time Teams: one big team of 5.\nGoal: Build a cohesive story in a team, using 5 parts.\nEach person gets a component of the story:\n Setting characters problem stakes solution  Discussion:\n Any take-aways?   Summary Saying yes in group settings leads to new opportunities!\nUse “yeah, but…” to refine ideas.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d3470388ff8345704b7d858c114cfbfb","permalink":"/notes/yes_and/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/yes_and/","section":"notes","summary":"Who’s seen improv shows? Knows lessons?\nLearning Objective By the end of today’s lesson, using two activities based on improv theatre, learners are anticipated to\n Demonstrate how saying yes to ideas opens up new possibilities in work and life.   Activity #1: Yes, and…; Yeah, but… Part 1:\n Pair up. Someone starts: “Hey! Let’s [activity]!!” Go back and forth: “Yes! And, […]”. I’ll stop you after 1 minute.","tags":null,"title":"Improv and Life","type":"notes"},{"authors":null,"categories":null,"content":" JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.\nYou’ll need to download and install JAGS. You can interact with JAGS through one of three R packages:\n runjags (recommended for this course)  Model written as a single string in R; possibly also allows you to input from file. Quick-start guide vignette: vignette(\u0026quot;quickjags\u0026quot;, package=\u0026quot;runjags\u0026quot;) Full user guide vignette: vignette(\u0026quot;UserGuide\u0026quot;, package=\u0026quot;runjags\u0026quot;)  rjags (sample code here)  Model read in from plain text file.  R2jags  this is dependent on R2winbugs, which I find doesn’t work well outside of Windows machines, so I’m more hesitant to use this package. Model written in R as a function, but using JAGS language; or inputted from file.   Also, the coda package is useful for working with the output of at least runjags.\nAbout the JAGS language:\n Generates samples of parameters based on the prior and statistical model. Need to specify which parameters you want to include in the output (aka “track” or “monitor”). Specify probability distributions similarly to R, except:  Draw samples using calls like dexp and dnorm, not rexp and rnorm. The JAGS version of rnorm uses the precision (=1/variance) instead of standard deviation. The documentation of JAGS code is not as nice as R. You have to look things up from a table-of-contents-style search from this document. Page 29 shows the aliases for various distributions.   For this week’s lab assignment (3), you’ll only be using it to generate observations from a distribution. Let’s generate data from a N(0,2) distribution (that is, variance=2), and ignore the warning messages for this week.\nsuppressPackageStartupMessages(library(runjags)) my_model \u0026lt;- \u0026quot; model{ # This is a comment. theta ~ dnorm(0, 1/2) } \u0026quot; fit \u0026lt;- run.jags(my_model, monitor=\u0026quot;theta\u0026quot;, n.chains=1, sample=1000) ## Warning: No data was specified or found in the model file so the simulation ## was run withut data ## Calling the simulation... ## Welcome to JAGS 4.3.0 on Sun Aug 25 20:24:33 2019 ## JAGS is free software and comes with ABSOLUTELY NO WARRANTY ## Loading module: basemod: ok ## Loading module: bugs: ok ## . . Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 0 ## Unobserved stochastic nodes: 1 ## Total graph size: 5 ## . Initializing model ## . Adaptation skipped: model is not in adaptive mode. ## . Updating 4000 ## -------------------------------------------------| 4000 ## ************************************************** 100% ## . . Updating 1000 ## -------------------------------------------------| 1000 ## ************************************************** 100% ## . . . . Updating 0 ## . Deleting model ## . ## Note: the model did not require adaptation ## Simulation complete. Reading coda files... ## Coda files loaded successfully ## Calculating summary statistics... ## Warning: Convergence cannot be assessed with only 1 chain ## Finished running the simulation theta \u0026lt;- coda::as.mcmc(fit) head(theta) ## Markov Chain Monte Carlo (MCMC) output: ## Start = 5001 ## End = 5007 ## Thinning interval = 1 ## theta ## 5001 0.0876587 ## 5002 1.1282000 ## 5003 -1.9906600 ## 5004 0.1476260 ## 5005 0.6665350 ## 5006 2.1165500 ## 5007 -0.0731193 plot(theta) More sample code library(tidyverse) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.2.0 ✔ purrr 0.2.5 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ tidyr::extract() masks runjags::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(runjags) n \u0026lt;- 50 dat \u0026lt;- tibble(x=rnorm(n), y=x + rnorm(n)) jagsdat \u0026lt;- c(as.list(dat), n=nrow(dat)) model \u0026lt;- \u0026quot;model{ for (i in 1:n) { y[i] ~ dnorm(beta*x[i], tau) } tau \u0026lt;- pow(sigma, -2) sigma ~ dunif(0, 100) beta ~ dnorm(0, 0.001) }\u0026quot; foo \u0026lt;- run.jags( model=model, monitor=c(\u0026quot;beta0\u0026quot;, \u0026quot;beta1\u0026quot;, \u0026quot;sigma\u0026quot;), data=jagsdat ) ## Warning: No initial value blocks found and n.chains not specified: 2 chains ## were used ## Warning: No initial values were provided - JAGS will use the same initial ## values for all chains ## Calling the simulation... ## Welcome to JAGS 4.3.0 on Sun Aug 25 20:24:36 2019 ## JAGS is free software and comes with ABSOLUTELY NO WARRANTY ## Loading module: basemod: ok ## Loading module: bugs: ok ## . . Reading data file data.txt ## . Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 50 ## Unobserved stochastic nodes: 2 ## Total graph size: 159 ## . Initializing model ## . Adapting 1000 ## -------------------------------------------------| 1000 ## ++++++++++++++++++++++++++++++++++++++++++++++++++ 100% ## Adaptation successful ## . Updating 4000 ## -------------------------------------------------| 4000 ## ************************************************** 100% ## . Failed to set trace monitor for beta0 ## Variable beta0 not found ## . Failed to set trace monitor for beta1 ## Variable beta1 not found ## . . Updating 10000 ## -------------------------------------------------| 10000 ## ************************************************** 100% ## . . . . . Updating 0 ## . Deleting model ## . ## Simulation complete. Reading coda files... ## Coda files loaded successfully ## Calculating summary statistics... ## Calculating the Gelman-Rubin statistic for 1 variables.... ## Finished running the simulation plot(coda::as.mcmc(foo)) ## Warning in as.mcmc.runjags(foo): Combining the 2 mcmc chains together  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e813129d9dd3d8ec5e743531ae949b23","permalink":"/post/jags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/jags/","section":"post","summary":"JAGS is a language that allows you to run Bayesian analyses. It gets at the posterior by generating samples based on the posterior and statistical model.\nYou’ll need to download and install JAGS. You can interact with JAGS through one of three R packages:\n runjags (recommended for this course)  Model written as a single string in R; possibly also allows you to input from file. Quick-start guide vignette: vignette(\u0026quot;quickjags\u0026quot;, package=\u0026quot;runjags\u0026quot;) Full user guide vignette: vignette(\u0026quot;UserGuide\u0026quot;, package=\u0026quot;runjags\u0026quot;)  rjags (sample code here)  Model read in from plain text file.","tags":null,"title":"JAGS Tutorial","type":"post"},{"authors":null,"categories":null,"content":"  Making effective plots can tell you a LOT about data. Its hard! Its an under-rated but very powerful skill to develop.\n - Di Cook\nsuppressPackageStartupMessages(library(tidyverse)) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 library(gapminder) knitr::opts_chunk$set(fig.width=5, fig.height=3) Agenda Tips for effective graphing\nAt least two exercises related to content and http://viz.wtf/ (see the worksheet).\n Resources These resources are listed on the syllabus in the lecture table. They provide a good overview of tips for effective plotting.\n Geckoboard’s data vis tips Jenny’s STAT545 lecture notes: do’s and don’ts  Here are some resources that dive a little deeper:\n Di Cook’s Rookie Mistakes.  Especially focusses on categorical data.  Richard Hollins’ Three reasons why pie charts suck  An entertaining but inspiring resource:\n Gallery of poor data vis: http://viz.wtf/  If you want to spend more time on this and/or dig deeper, take a look at the following books:\n Visualization Analysis and Design – Tamara Munzner Creating more effective graphs – Naomi Robbins.   Preface  Effectiveness: how well a graph conveys the information conveyed in data. != publication quality, although sometimes the line between these two can be blurry (no pun intended). Take solace in the fact that we can’t make perfectly effective graphics. Is a topic in human psychology. Don’t know human psychology? Luckily, you’re a human, and have an innate knowledge of the relative effectiveness of a plot. Use that as your guide!  From Gelman et al, As a guiding point, ask yourself: how can I modify my graph to better: faciliate comparisons, and reveal trends?    Disclaimer: The tips you see here and online hold true for most cases. There might be some rare cases where the tips don’t hold – the key is to be intentional about every component of the graph.\n“Let’s Practice What We Preach: Turning Tables into Graphs” by Gelman A, Pasarica C, Dodhia R. The American Statistician, Volume 56, Number 2, 1 May 2002 , pp. 121-130(10).\n Learning Objectives From today’s lecture, students are expected to:\n be intentional with your choice of graph components. be able to spot bad graphs, and avoid making them. internalize some tips of plotting effectiveness provided in class.  For the quiz, you aren’t expected to know/memorize all of the tips, but you are expected to have internalized some of them.\n Consider Information Density Sometimes called overplotting.\n Scatterplot too dense?  Do you need a log transform? Try alpha transparency Change geom: geom_hex() or geom_bin2d() Spread the data into separate panels: facet by a grouping variable or two.   gapxy \u0026lt;- ggplot(gapminder, aes(lifeExp, gdpPercap)) + theme_bw() gapxy + geom_point() gapxy \u0026lt;- gapxy + scale_y_log10() gapxy + geom_point()  gapxy + geom_point(alpha=0.2) gapxy + geom_hex()  gapxy + geom_density2d() gapxy + facet_wrap(~continent) + geom_point(alpha=0.2)   Using too many geom’s? Don’t.  ggplot(gapminder, aes(continent, lifeExp)) + geom_violin(fill=\u0026quot;red\u0026quot;, alpha=0.2) + geom_boxplot(fill=\u0026quot;blue\u0026quot;, alpha=0.2) + geom_jitter(alpha=0.2)  Find the Goldilocks Plot Display just the right amount of content: not too much, not too little.\nIn particular: reveal as much relevant information as possible; trim irrelevant and redundant information.\nReveal as much relevant information as possible Because hiding your data is not effective at conveying information!\n jitter + violin, not pinhead plots. mosaic plots   Trim Irrelevant Information Only use as much data as is required for answering a data analytic question.\n Di Cook’s example in Rookie Mistakes: reduce complexity section.\n Remove the special effects (sorry, Excel). Great demo: Darkhorse analytic’s Less is more gif.\n More examples of extraneous information:\n  map_data(\u0026quot;france\u0026quot;) %\u0026gt;% ggplot(aes(long, lat)) + geom_polygon(aes(group=group), fill=NA, colour=\u0026quot;black\u0026quot;) + theme_bw() + ggtitle(\u0026quot;Are lat and long really needed?\u0026quot;) ggplot(gapminder, aes(year, lifeExp)) + geom_line(aes(group=country, colour=country), alpha=0.2) + guides(colour=FALSE) + theme_bw() + ggtitle(\u0026quot;Is colouring by country really necessary here?\\nNevermind fitting the legend!\u0026quot;)  Trim Redundant Information Don’t redundantly map variables to aesthetics/facets.\n Common example: colouring/filling and facetting by the same variable.  HairEyeColor %\u0026gt;% as_tibble() %\u0026gt;% uncount(n) %\u0026gt;% ggplot(aes(Hair)) + facet_wrap(~Sex) + geom_bar(aes(fill=Sex)) + theme_bw() + ggtitle(\u0026quot;Don\u0026#39;t do this.\u0026quot;) Really want to use colour? No problem, colours are fun! Try this:\nHairEyeColor %\u0026gt;% as_tibble() %\u0026gt;% uncount(n) %\u0026gt;% ggplot(aes(Hair)) + facet_wrap(~Sex) + geom_bar(fill=\u0026quot;#D95F02\u0026quot;) + theme_bw() + ggtitle(\u0026quot;Do this.\u0026quot;)  Delegate numeric details to an appendix, not the graph (or omit entirely).  HairEyeColor %\u0026gt;% as_tibble() %\u0026gt;% uncount(n) %\u0026gt;% count(Hair) %\u0026gt;% ggplot(aes(Hair, n)) + geom_col() + geom_text(aes(label=n), vjust=-0.1) + theme_bw() + labs(x=\u0026quot;Hair colour\u0026quot;, y=\u0026quot;count\u0026quot;, title=\u0026quot;Are the bar numbers AND y-axis really needed?\u0026quot;)   Choose Human-Interpretable Aesthetic Mappings and Geom’s  Here’s an iconic fail in Henrik Lindberg’s tweet: the “depeche plot”.\n Don’t use pie charts – use bar charts instead. 3 reasons why they suck.\n To bar, or not to bar? Not if zero doesn’t matter! As a general rule, I like to err on the side of points over bars.\n  plot_beav2 \u0026lt;- bind_rows( mutate(beaver1, beaver = \u0026quot;Beaver 1\u0026quot;), mutate(beaver2, beaver = \u0026quot;Beaver 2\u0026quot;) ) %\u0026gt;% group_by(beaver) %\u0026gt;% summarize(med = median(temp)) %\u0026gt;% ggplot(aes(beaver, med)) + theme_bw() + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;Body Temperature\\n(Celsius)\u0026quot;) cowplot::plot_grid( plot_beav2 + geom_col() + ggtitle(\u0026quot;Don\u0026#39;t do this.\u0026quot;), plot_beav2 + geom_point() + ggtitle(\u0026quot;Do this.\u0026quot;) ) (Yes, that’s really all the info you’re conveying. Own it.)\n Colour your groups so that not one group unintentionally stands out.  plot_iris \u0026lt;- ggplot(iris, aes(Sepal.Width, Sepal.Length)) + geom_jitter(aes(colour=Species)) + theme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;) cowplot::plot_grid( plot_iris + scale_colour_manual(values=c(\u0026quot;brown\u0026quot;, \u0026quot;gray\u0026quot;, \u0026quot;yellow\u0026quot;)) + ggtitle(\u0026quot;Don\u0026#39;t do this.\u0026quot;), plot_iris + scale_colour_brewer(palette=\u0026quot;Dark2\u0026quot;) + ggtitle(\u0026quot;Leave it to an expert.\\nDo this.\u0026quot;) )  Consider Zero Are you comparing data across groups? Consider what a meaningful distance measure might be between two groups.\nAre differences meaningful, and proportions not? Example: temperature. Zero doesn’t matter.\nplot_beav \u0026lt;- bind_rows( mutate(beaver1, beaver = \u0026quot;Beaver 1\u0026quot;), mutate(beaver2, beaver = \u0026quot;Beaver 2\u0026quot;) ) %\u0026gt;% ggplot(aes(beaver, temp)) + geom_violin() + geom_jitter(alpha=0.25) + theme_bw() + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;Body Temperature\\n(Celsius)\u0026quot;) cowplot::plot_grid( plot_beav + ggtitle(\u0026quot;This.\u0026quot;), plot_beav + ylim(c(0,NA)) + ggtitle(\u0026quot;Not This.\u0026quot;) ) Are proportions meaningful, and differences not? Example: counts.\nHairEyeColor %\u0026gt;% as_tibble() %\u0026gt;% uncount(n) %\u0026gt;% ggplot(aes(Hair)) + geom_bar() + theme_bw() + ggtitle(\u0026quot;Keep this starting from 0.\u0026quot;) Want to convey absolute life expectancies, in addition to relative life expectancies? Show 0.\nggplot(gapminder, aes(continent, lifeExp)) + geom_boxplot() + ylim(c(0, NA)) + geom_hline(yintercept = 0, linetype = \u0026quot;dashed\u0026quot;)  Order factors It’s easier to see rankings. See this STAT 545 example by Jenny Bryan. Use forcats!\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"10833552f8f52c91bc7c60b8e3aa1c3a","permalink":"/post/effective_plotting/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/effective_plotting/","section":"post","summary":"Making effective plots can tell you a LOT about data. Its hard! Its an under-rated but very powerful skill to develop.\n - Di Cook\nsuppressPackageStartupMessages(library(tidyverse)) ## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;tibble\u0026#39; was built under R version 3.5.2 ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 library(gapminder) knitr::opts_chunk$set(fig.width=5, fig.height=3) Agenda Tips for effective graphing\nAt least two exercises related to content and http://viz.","tags":null,"title":"Lecture 5: Plotting for Humans","type":"post"},{"authors":null,"categories":null,"content":" This document outlines some Linear Algebra results that you’ll need to know for the DSCI 563 Lab4 assignment. You can take these as given – you don’t have to prove them.\nCovariance Rule The variance-covariance matrix (or, just covariance matrix for short) of a random vector \\(\\boldsymbol{X}\\) is denoted \\(\\text{Cov}(\\boldsymbol{X})\\). Now, if \\(A\\) is a matrix/vector, then the following rule holds: \\[ \\text{Cov}(A\\boldsymbol{X}) = A \\text{Cov}(\\boldsymbol{X}) A ^{\\top}, \\] where \\(A ^ {\\top}\\) denotes the transpose of \\(A\\).\n Orthogonal Matrices An orthogonal matrix \\(A\\) is defined such that \\(A\\) is square, and \\[ A^{\\top} A = A A^{\\top} = I, \\] where \\(I\\) is the identity matrix. That is, \\(A\\) and \\(A^{\\top}\\) are inverses.\n Spectral Decomposition Let \\(\\Sigma\\) be a covariance matrix. That is, it’s a square matrix that’s positive definite. Then, \\(\\Sigma\\) can be decomposed/reconstructed by its eigenvalues and eigenvectors. Specifically, if \\(U\\) is a matrix whose columns are the eigenvectors of \\(\\Sigma\\), and \\(\\Lambda\\) is a diagonal matrix with the corresponding eigenvectors along the diagonal, then \\[ \\Sigma = U \\Lambda U^{\\top}. \\]\nFurther, \\(U\\) is orthogonal.\n Diagonal Matrices Suppose \\(A\\) is a square diagonal matrix. Then \\(A^{\\top} = A\\). Also, if \\(a \\in \\mathbb{R}\\), then \\(A^a\\) is a diagonal matrix where the \\(i\\)’th diagonal entry is the \\(i\\)’th diagonal entry of \\(A\\) raised to the power of \\(a\\).\n Transpose of a Matrix Product Suppose \\(A_1, \\ldots, A_k\\) are \\(k\\) matrices. Then, the transpose of their product can be found by reversing the order of the product, and transposing each matrix: \\[ \\left(\\prod_{i=1}^{k}A_{i}\\right)^{\\top}=\\prod_{i=1}^{k}A_{k-i+1}^{\\top}. \\]\n Summability of Powers Let \\(a_1, \\ldots, a_k\\) be real numbers, and \\(A\\) a square matrix. If \\(A^{a_i}\\) for \\(i=1,\\ldots,k\\) exist, then \\[ \\prod_{i=1}^{k}A^{a_{i}}=A^{\\sum_{i=1}^{k}a_{i}}. \\]\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"372c989bf3d4310b9c1375c135e22b26","permalink":"/post/useful_linalg_results/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/useful_linalg_results/","section":"post","summary":"This document outlines some Linear Algebra results that you’ll need to know for the DSCI 563 Lab4 assignment. You can take these as given – you don’t have to prove them.\nCovariance Rule The variance-covariance matrix (or, just covariance matrix for short) of a random vector \\(\\boldsymbol{X}\\) is denoted \\(\\text{Cov}(\\boldsymbol{X})\\). Now, if \\(A\\) is a matrix/vector, then the following rule holds: \\[ \\text{Cov}(A\\boldsymbol{X}) = A \\text{Cov}(\\boldsymbol{X}) A ^{\\top}, \\] where \\(A ^ {\\top}\\) denotes the transpose of \\(A\\).","tags":null,"title":"Some Linear Algebra For DSCI 563 Lab4","type":"post"},{"authors":null,"categories":null,"content":" This page indicates my teaching philosophy.\nMy teaching takes to heart what I learned from Greg Wilson, that teaching is much more about motivating students than it is knowledge transfer. Students have access to any information they want through the internet, but a classroom environment has the powerful advantage of the presence of peers and a knowledgeable instructor. I take a semi-spontaneous approach to teaching, as inspired by improv comedy – I have a rough guideline that I abide to, but am not afraid to deviate from this where necessary, responding to how the class is feeling about a topic. I use various methods to keep constant awareness of how the class is feeling, through simple methods such as asking questions, to more modern active learning strategies such as think-pair-share, which leverages the knowledge of peers.\nI believe in quality over quantity, and this is especially important for a program as time-sensitive as the Master of Data Science program at UBC that I currently teach in. Here, I focus on fundamental concepts, asking the question “what must the students absolutely know by the end of this course?” To help answer this, I look to fundamental concepts as they relate to applications, not necessarily how they are developed in academia. I stick to these core concepts, and show students just how far they can go by exploring deeper concepts and data science methods – again, going back to motivation over knowledge transfer.\nI believe that teaching is far less effective when done “in a vacuum”, as opposed to collaboratively with the input and feedback from peers. I’m lucky to be involved with open and communicative colleagues who can share their input, to build world-class courses in data science. And I’m happy to be on the other end as well, providing input and feedback to my colleagues.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb737296956c3f9cb1ea13f787e98de0","permalink":"/teaching/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/","section":"","summary":"This page indicates my teaching philosophy.\nMy teaching takes to heart what I learned from Greg Wilson, that teaching is much more about motivating students than it is knowledge transfer. Students have access to any information they want through the internet, but a classroom environment has the powerful advantage of the presence of peers and a knowledgeable instructor. I take a semi-spontaneous approach to teaching, as inspired by improv comedy – I have a rough guideline that I abide to, but am not afraid to deviate from this where necessary, responding to how the class is feeling about a topic.","tags":null,"title":"Teaching Philosophy","type":"page"},{"authors":null,"categories":null,"content":" [ ] Materials: Paper and pen/pencil.\nPre-quiz Suppose you have 6-sided and fair dice. What’s the probability of rolling: A 3 on the roll of a die? Snake-eyes? (i.e., two 1’s after rolling two dice).  On a given day, there’s a 50% chance that your toaster works. What’s the probability that… your toaster works exactly one day in a given week? BONUS: your toaster works at least one day in a given week?   [ ] Pre-quiz done!\n Share your answers What did you all get?\nHow did you come up with those numbers?\nThese can all be calculated using the __Binomial distribution_!\nAnyone familiar?\n[ ] Sharing done!\n Learning Objectives By the end of the lesson, when faced with the task of computing a probability, learners are anticipated to be able to:\n Identify whether the probability can be computed using the binomial distribution. Apply the binomial distribution formula to compute the probability.  [ ] Objectives done!\n 1. Can the Binomial distribution be used to compute a probability? You need two ingredients.\nIngredient #1: Outcome = number of “wins” of independent “games”.\nQuestion: What are the “games” in the pre-quiz questions? What’s a “win”?\n [ ] I’ll do Q1 [ ] Can you do Q2?  [ ] Ingredient 1 done!\n Ingredient #2: The probability of winning a game stays the same across games.\n[ ] Question: What are the probabilities of winning a game for Q1 and Q2? Are they the same across games?\n[ ] Ingredient 2 done! All ingredients done!\n  2. How to use the Binomial distribution to compute a probability Formula for \\(x\\) wins out of \\(n\\) games. Probability of success \\(p\\):\n\\[\\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}\\]\nActivity:\n[ ] Let’s work through Q1 together [ ] You do Q2 on your own. Write it down! [ ] Share/revise answers with peer. [ ] Communicate answers (answers below)  [ ] Using the formula = done!\n Summary  Use the binomial distribution to compute probability of number of “wins”. A “win” can be so many things!  rain (vs. not rain) getting to work in less than 30 minutes (vs. not) passing a course (vs. not) etc…    Answers … 1/6 = 0.1667 1/36 = 0.02778  … 0.0546875 (1 - prob of never works) = 1 - 0.0078125 = 0.9921875    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"667f6f145479bb57092284c1e9c8ae08","permalink":"/notes/binomial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/notes/binomial/","section":"notes","summary":"[ ] Materials: Paper and pen/pencil.\nPre-quiz Suppose you have 6-sided and fair dice. What’s the probability of rolling: A 3 on the roll of a die? Snake-eyes? (i.e., two 1’s after rolling two dice).  On a given day, there’s a 50% chance that your toaster works. What’s the probability that… your toaster works exactly one day in a given week? BONUS: your toaster works at least one day in a given week?","tags":null,"title":"The Binomial Distribution","type":"notes"},{"authors":null,"categories":null,"content":" What is Varimax? In 1(b), you saw that there are many loadings matrices that we can take so that the Factor Analysis assumptions are still satisfied (i.e., more than one loadings matrix “works”). We explored some of the possible loadings matrices by multiplying by a rotation matrix, denoted \\(R\\) in the lab assignment.\nThe varimax rotation is one such rotation that leads to loadings vectors that have a desirable property (in terms of interpretation): it “polarizes” the loadings vectors so that the values within a vector are either close to zero or are large in magnitude. A nicely “polarized” loadings vector is useful for interpretation, because features that are relevant to the underlying factor become easier to identify. For example, if the loadings vector for Factor 1 has loadings close to 1 for Features A and B, and close to zero for all other features, then we can look for something common between Features A and B (for example, “bitter flavour”) that describes the factor underlying the data.\nHow can one achieve a loadings matrix whose vectors are polarized as much as possible? We need some concrete measure of “polarity” to maximize. One trick is to maximize the variance of the squared loadings (squaring the loadings allows us to disregard sign). This is what varimax does.\nThe specific formula can be found on Slide 29 in Lecture 6.\n Your task Write a function in R that takes a value of \\(\\theta\\), and does the following:\nComputes the rotation matrix \\(R\\). Computes the rotated loadings \\(L_R = LR\\). Computes sum of variances of the squared loadings in each column of \\(L_R\\).  This function returns a measure of “polarity” in your loadings vectors (in terms of the total variance) – the larger the total variance, the more polarized your loadings vectors are. As such, you should find the value of \\(\\theta\\) that maximizes this “polarity” (maximizes this function). Use this \\(\\theta\\) to compute your rotated loadings.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"67f23d6b50db735870d8b242e0ac667f","permalink":"/post/varimax/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/varimax/","section":"post","summary":"What is Varimax? In 1(b), you saw that there are many loadings matrices that we can take so that the Factor Analysis assumptions are still satisfied (i.e., more than one loadings matrix “works”). We explored some of the possible loadings matrices by multiplying by a rotation matrix, denoted \\(R\\) in the lab assignment.\nThe varimax rotation is one such rotation that leads to loadings vectors that have a desirable property (in terms of interpretation): it “polarizes” the loadings vectors so that the values within a vector are either close to zero or are large in magnitude.","tags":null,"title":"Varimax Rotation","type":"post"},{"authors":null,"categories":null,"content":" email: vincenzo.coia@gmail.com\nWork Experience 2017/01 - present\nPostdoctoral Teaching and Learning Fellow\nMasters of Data Science Program, the University of British Columbia\nVancouver, BC\n2009/05 - 2014/05\nShort-term statistical consulting (6 projects)\nUBC and private\n Education PhD in Statistics\n2012/09 - 2017/02\nThe University of British Columbia\nConferred May 29, 2017\nMSc in Mathematics and Statistics (Statistics)\n2011/09 - 2012/08\nBrock University\nConferred on October 13, 2012\nBSc (3-year) in Biological Sciences\nMinor in Earth Sciences\n2005/09 - 2011/04\nBrock University\nConferred “With Distinction” on October 22, 2011\nBSc (Honours) Mathematics Integrated with Computers and Applications\nConcentration in Statistics\n2005/09 - 2011/04\nBrock University\nConferred “With First-Class Standing” on June 7, 2011\nVolunteer Positions 2016/09 - 2016/02\nScience World at TELUS World of Science \nVancouver, BC\n78.15 hours\n2013/10 - 2014/05\nBeaty Biodiversity Museum: Events Volunteer\nVancouver, BC\n35.0 hours\n2013/04 - 2013/09\nUBC Farm\nVancouver, BC\n102.5 hours\n2011/06 - 2011/08\nProject S.H.A.R.E. community garden\nNiagara Falls, ON\n15.0 hours\n Research Assistantships 2013/05 - 2013/08\nRobust penalized regression\nSupervisor: Dr. Gabriela Cohen-Freue\nDepartment of Statistics\nThe University of British Columbia\nVancouver, BC\n2012/05 - 2012/08\n2011/05 - 2011/08\n2010/05 - 2010/08\nExtreme value modelling\nSupervisor: Dr. Mei Ling Huang\nDepartment of Mathematics\nBrock University\nSt. Catharines, ON\n2010/09 - 2011/06\nQuantum monte carlo simulations\nSupervisors: Dr. Stuart Rothstein; Dr. Wai Kong (John) Yuen\nDepartment of Chemistry and Department of Mathematics\nBrock University\nSt. Catharines, ON\n Teaching Assistantships Duration: From the latter part of my undergrad, to the end of my PhD.\nUBC:\n SCIE 300: Communicating Science (5x)  Brock University:\n MATH 4P82/5P82: Non-parametric Statistics MATH 3P82: Regression Analysis MATH 4P81/5P81: Sampling Theory MATH 3P81: Experimental Design (2x) MATH 2F40: Mathematics Integrated w/ Computers and Applications II    Publications and Talks Articles Submitted to Refereed Journals  Huang, M.L., Coia, V., and Brill, P.H. (2013) A cluster truncated Pareto distribution and its applications. ISRN Probability and Statistics 2013: Article ID 265373.\n Ayad, M., Coia, V., and Kihel, O. (2014) The number of relatively prime subsets of a finite union of sets of consecutive integers. Journal of Integer Sequences 17: Article 14.3.7\n Coia, V., and Huang, M.L. (2014) A sieve model for extreme values. Journal of Statistical Computation and Simulation. 84(8):16921710.\n   Articles Submitted to Conference Proceedings  Huang, M. L., Coia, V., and Brill, P.H., A mixture truncated Pareto distribution, In JSM Proceedings 2012, Statistical Computing Section, Alexandria, VA: American Statistical Association, pp. 24882498.   Conference and Roundtable Contributions  Coia, V., Nolde, N., and Joe, H. Forecasting Extremes for Flooding (Invited Talk). The 44th Annual Meeting of the Statistical Society of Canada. May 29June 1, 2016 at Brock University, St. Catharines, ON.\n Coia, V., and Jeanniard du Dot, T. (Invited Demonstration) “Using the Grammar of Graphics and Interactivity to explore Biologging Data in R”. May 6, 2015. Building a Bioanalytical Theory for Analysis of Marine Mammal Movements: A Peter Wall International Research Roundtable. The University of British Columbia, Vancouver, BC.\n Coia, V. “Flood Warning: An Application of High-Quantile Regression” (Contributed Talk). SFU/UBC Joint Graduate Student Seminar (Winter). February 28, 2015 at the SFU Harbour Centre, Vancouver, BC.\n Coia, V. “A New Sieve Model for Extreme Values” (Contributed Talk). SFU/UBC Joint Graduate Student Seminar (Fall). September 29, 2012 at the SFU Harbour Centre, Vancouver, BC.\n Coia, V., and Huang, M.L. “On Estimation of Heavy Tailed Distributions” (Contributed Talk). The 40th Annual Meeting of the Statistical Society of Canada. June 36, 2012 at the University of Guelph, Guelph, ON.\n Huang, M.L., Coia, V., and Brill, P.H. “A Mixture Truncated Pareto Distribution” (Contributed Talk). The 2012 Joint Statistical Meetings. July 28August 2, 2012 at San Diego, California\n    Awards University Issued  2012/09 - 2016/08: Four-Year Fellowship 2012/09 - 2016/08: Faculty of Science Graduate Award 2011/09: Dean of Graduate Studies Excellence Scholarship 2011/06/07: Dean’s Gold Medal 2011/06/07: Distinguished Undergraduate Student Award in Mathematics 2011/03: President’s Surgite Award   Nationally Recognized  2013/06: Governor General of Canada’s Gold Medal 2012/09 - 2015/08: NSERC Postgraduate Award (Doctoral, 3-year) 2011/09 - 2012/08: NSERC Alexander Graham Bell Canada Graduate Scholarship (Masters) 2010/05 - 2010/08: NSERC Undergraduate Student Research Award   Individual Donors  2012/04: Dr. Jack Lightstone \u0026amp; Dorothy Markiewicz Scholarship 2012/04: Dr. Raymond \u0026amp; Mrs. Sachi Moriyama Grad. Fellowship 2012/03: Tomlinson Entrance Scholarship for Excellence in Mathematics and Science 2011/06: John and Roslyn Reed Book Prize 2010/09: Art Bicknell Scholarship in Mathematics 2010/09: Ian D. Beddis Family Scholarship 2010/09: Terry and Sue White Mathematics and Science Scholarship 2007/09: M.J. (“Mel”) Farquharson Scholarship 2006/09: Scholler Foundation Scholarship in Chemistry   Athletic Awards  2016/04/03: Sportsmanship Award, UBC Thunderbirds Sport Clubs (Fencing) 2016/02/28: Bronze Medal in Senior Mixed Epee (Intercollegiate Tournament, UBC) 2015/11/16: Bronze Medal in Senior Mixed Epee (Remembrance Day Tournament, UBC) 2012/03/28: RM Davis Surgite Award 2009/12/12: First Place Award Winter Epeedemic, Toronto Fencing Club 2009/03: Varsity Fencing Rookie of the Year Award   Declined Awards  2012/04: Ontario Graduate Scholarship (Doctoral) 2011/05: Ontario Graduate Scholarship (Masters)    Professional Activities Dept. of Statistics, The University of British Columbia:\n 2016/05 - 2016/06: Search committee member (for CRC 2 faculty position) 2015/04/30: Organizer of the “How to Write an Awesome Abstract” workshop 2014/06 - 2015/05: Organizer of the SFU/UBC Joint Seminar 2014/06 - 2014/09: Organizer of the Graduate Student Trip 2014/04 - 2015/05: Co-founder of the Graduate Writing Forums 2013/06 - 2014/05: Statistics Graduate Student Representative   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"83a254add626bb2e90c041edccf73cf9","permalink":"/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/","section":"","summary":"email: vincenzo.coia@gmail.com\nWork Experience 2017/01 - present\nPostdoctoral Teaching and Learning Fellow\nMasters of Data Science Program, the University of British Columbia\nVancouver, BC\n2009/05 - 2014/05\nShort-term statistical consulting (6 projects)\nUBC and private\n Education PhD in Statistics\n2012/09 - 2017/02\nThe University of British Columbia\nConferred May 29, 2017\nMSc in Mathematics and Statistics (Statistics)\n2011/09 - 2012/08\nBrock University\nConferred on October 13, 2012\nBSc (3-year) in Biological Sciences","tags":null,"title":"Vincenzo Coia's CV","type":"page"}]