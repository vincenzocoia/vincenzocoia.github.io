---
title: 'BAIT 509 Class Meeting 05'
subtitle: "Classification and Regression Trees (CART)"
date: "January 16, 2019"
output: 
    html_document:
        keep_md: true
        toc: true
        toc_depth: 2
        number_sections: true
        theme: cerulean
        toc_float: true
---



<div id="outline" class="section level1">
<h1>Outline</h1>
<ul>
<li>Classification and Regression Trees</li>
</ul>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<p>Some problems (or at least potential problems) with the local methods introduced last time:</p>
<ol style="list-style-type: decimal">
<li>They lack interpretation.
<ul>
<li>It’s not easy to say how the predictors influence the response from the fitted model.</li>
</ul></li>
<li>They typically require a data-rich situation so that the estimation variance is acceptable, without compromising the estimation bias.</li>
</ol>
<p>We’ll look at classification and regression trees as another method (sometimes called CART). CARTs are not necessarily better, and in fact, tend to be poor competitors to other methods – but ensemble extensions of these tend to perform well (a topic for next Monday).</p>
<p>Our setting this time is the usual: we have a response <span class="math inline">\(Y\)</span> (either categorical or numeric), and hope to predict this response using <span class="math inline">\(p\)</span> predictors <span class="math inline">\(X_1,\ldots,X_p\)</span>.</p>
<ul>
<li>When the response is categorical, we aim to estimate the mode and take that as our prediction.</li>
<li>When the response is numeric, we aim to estimate the mean, and take that as our prediction.</li>
</ul>
</div>
<div id="stumps-a-preliminary-concept" class="section level1">
<h1>Stumps: A preliminary concept</h1>
<div id="what-are-they" class="section level2">
<h2>What are they?</h2>
<p>Let’s say I get an upset stomach once in a while, and I suspect certain foods might be responsible. My response and predictors are:</p>
<ul>
<li><span class="math inline">\(Y\)</span>: sick or not sick (categorical)</li>
<li><span class="math inline">\(X_1\)</span>: amount of eggs consumed in a day.</li>
<li><span class="math inline">\(X_2\)</span>: amount of milk consumed in a day, in liters.</li>
</ul>
<p>You make a food diary, and record the following:</p>
<table>
<thead>
<tr class="header">
<th>Eggs</th>
<th>Milk</th>
<th>Sick?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.7</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.6</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>No</td>
</tr>
<tr class="even">
<td>1</td>
<td>0.7</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>0</td>
<td>0.4</td>
<td>No</td>
</tr>
</tbody>
</table>
<p>(Example from Mark Schmidt’s CPSC 340)</p>
<p>A <strong>decision/classification stump</strong> is a decision on <span class="math inline">\(Y\)</span> based on the value of <em>one</em> of the predictors. You can make one by doing the following steps:</p>
<ol style="list-style-type: decimal">
<li>Choose a predictor (in this case, Milk or Eggs?)</li>
<li>Choose a cutpoint on that predictor:
<ul>
<li>Subset the data having that predictor <em>less than</em> that cutpoint, and take the mode as your decision/classification.</li>
<li>Subset the data having that predictor <em>greater than</em> that cutpoint, and take the mode as your decision/classification.</li>
</ul></li>
</ol>
<p>This will get you a decision/classification stump, like so:</p>
<div class="figure">
<img src="cm05-trees_files/stump.png" />

</div>
<p>(Image attribution: Hyeju Jang, DSCI 571)</p>
<p>The same idea applies in the regression case, except we take the average of the subsetted data, and choose the best one according to the mean squared error.</p>
<p>Some things to note:</p>
<ul>
<li>A decision on a numeric variable is always made based on a threshold (either less than or greater than). Although it makes sense to talk about a stump based on a more complicated decision, allowing for more complicated decisions would increase the search space, and would make fitting these models impractical. Plus, sometimes a more complicated decision can be broken down into several single-threshold decisions.</li>
<li>It follows that decisions are always binary. Again, although it makes sense to talk about a stump that has more than two options, we can usually write these as a tree.</li>
</ul>
</div>
<div id="choosing-the-best-stumps" class="section level2">
<h2>Choosing the best stumps</h2>
<p><strong>Question</strong>: What is the error of the above decision stump? If we decide to make a prediction without a decision stump (or any predictors at all), what would the best prediction be, and what’s the error?</p>
<p>We have many stumps we can choose from – which one is best?</p>
<p><strong>Classification</strong>:</p>
<ul>
<li>The one that gives the least error.
<ul>
<li>Tends to have a harder time disambiguating several options.</li>
</ul></li>
<li>The Gini and Entropy criteria are two of the more commonly used measures.
<ul>
<li>These are measures of <strong>purity</strong>.</li>
</ul></li>
</ul>
<p><strong>Regression</strong>:</p>
<ul>
<li>The one that gives the least mean squared error.</li>
</ul>
</div>
</div>
<div id="partitioning-of-predictor-space" class="section level1">
<h1>Partitioning of predictor space</h1>
<p>How does a stump partition the predictor space?</p>
</div>
<div id="trees" class="section level1">
<h1>Trees</h1>
<p>Notice that a tree is just a bunch of stumps!</p>
<p>This means:</p>
<pre><code>1. For numeric predictors, decisions can only involve one threshold.
2. Decisions are always binary.</code></pre>
<div id="optimal-tree-computationally-infeasible" class="section level2">
<h2>Optimal tree – computationally infeasible</h2>
<p>Ideally, we’d examine the error from every possible regression tree, and choose the best one. But there are far too many trees possible! We turn to <em>recursive binary splitting</em> as the next best thing.</p>
</div>
<div id="recursive-binary-splitting" class="section level2">
<h2>Recursive Binary Splitting</h2>
<p>The idea to making a tree is to take a <strong>greedy</strong> approach: keep adding decision stumps, each time choosing the best stump.</p>
<p>This is called “greedy” because this method may not end up with the best tree – it’s not computationally feasible to look many steps ahead to see whether a different path is more feasible.</p>
<p>Adding “Eggs &lt; 1” to the above stump will be the next best iteration – in this case, resulting in 100% prediction accuracy (on the training data). Notice that every time we add a stump, the error on the training set decreases.</p>
<p>We keep iterating until we reach some <strong>stopping criterion</strong>, often based on things such as:</p>
<ul>
<li>Tree depth</li>
<li>Number of observations in a leaf becomes too small</li>
<li>Error is not reduced significantly</li>
</ul>
<p><strong>Questions</strong>:</p>
<ul>
<li>How does “tree size” affect the bias-variance tradeoff?</li>
<li>How can you choose these parameters?</li>
</ul>
</div>
<div id="pruning" class="section level2">
<h2>Pruning</h2>
<p>Sometimes, a stopping criterion may cause the tree to stop growing prematurely. For example, perhaps we need an insignificant stump to form before getting a more significant one.</p>
<p>To avoid this, we take the approach of growing an overly large (overfit) tree, and then pruning it. We won’t get into details of how the tree is pruned back, but one technique is called <em>cost complexity pruning</em>. The general idea is to control a tuning parameter <span class="math inline">\(\alpha\)</span> to control how much pruning is done – it can be chosen by cross-validation, or the validation set approach.</p>
</div>
</div>
<div id="exercise" class="section level1">
<h1>Exercise</h1>
<ul>
<li>Coding a decision tree with <code>tree::tree()</code> in R, using the <a href="https://raw.githubusercontent.com/vincenzocoia/BAIT509/master/assessments/assignment1/data/titanic.csv">titanic data</a>.</li>
<li><code>predict()</code> and <code>plot()</code>, and “S3 generics” in R.</li>
</ul>
</div>
<div id="classification-and-regression-trees-in-r" class="section level1">
<h1>Classification and Regression Trees: in R</h1>
<p>To fit classification and regression trees in R, we use the package <code>tree</code> and the function <code>tree()</code>, which works similarly to <code>lm()</code> and <code>loess()</code>:</p>
<pre class="r"><code>suppressPackageStartupMessages(library(tree))
suppressPackageStartupMessages(library(tidyverse))</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.5.2</code></pre>
<pre class="r"><code>fit &lt;- tree(Sepal.Width ~ ., data=iris)
summary(fit)</code></pre>
<pre><code>## 
## Regression tree:
## tree(formula = Sepal.Width ~ ., data = iris)
## Variables actually used in tree construction:
## [1] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; &quot;Petal.Width&quot; 
## Number of terminal nodes:  10 
## Residual mean deviance:  0.06268 = 8.776 / 140 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -0.60560 -0.16780  0.03182  0.00000  0.16280  0.63180</code></pre>
<pre class="r"><code>plot(fit)  # Plot the tree, without labels
text(fit)  # Add labels to the tree</code></pre>
<p><img src="/tutorials/cart_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
<p>(Messy – would need to modify the predictor names). Make predictions with <code>predict</code>:</p>
<pre class="r"><code>predict(fit, newdata=iris) %&gt;% 
    head</code></pre>
<pre><code>##        1        2        3        4        5        6 
## 3.635294 3.273913 3.273913 3.273913 3.273913 3.635294</code></pre>
<p>If you want to control the depth of the tree, use the <code>tree.control</code> function in the <code>control</code> argument. Arguments of <code>tree.control</code> that are relevant are:</p>
<ul>
<li><code>mindev</code>: The minimum error reduction acceptable before stopping the tree growth.</li>
<li><code>minsize</code>: The minimum number of observations required in a node in order for a tree to keep growing.</li>
</ul>
<p>Let’s fit a tree to the max, and check its MSE:</p>
<pre class="r"><code>fitfull &lt;- tree(Sepal.Width ~ ., data=iris, 
                control=tree.control(nrow(iris), 
                                     mindev=0, minsize=2))
mean((predict(fitfull) - iris$Sepal.Width)^2)</code></pre>
<pre><code>## [1] 0.0008666667</code></pre>
<p>You can investigate the pruning of a tree via cross validation using the <code>cv.tree</code> function. Specify <code>FUN=prune.misclass</code> if you want to prune based on classification error instead of purity measurements, for classification. It returns a list, where the important components are named <code>&quot;size&quot;</code> (number of terminal nodes) and <code>&quot;dev&quot;</code> (the error). Let’s plot those:</p>
<pre class="r"><code>set.seed(4)
fitfull_cv &lt;- cv.tree(fitfull)
plot(fitfull_cv$size, log(fitfull_cv$dev))</code></pre>
<p><img src="/tutorials/cart_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The x-axis represents the number of terminal nodes present in the subtree. The y-axis is the cross-validation error for that subtree.</p>
<p>From the plot, it looks like it’s best to prune the tree to have approximately 10 terminal nodes. Use <code>prune.tree(fitfull, best=10)</code> to prune the tree to 10 terminal nodes.</p>
<p>*Note: if you encounter an error running <code>prune.tree(fitfull, best=10)</code>, it’s not a true error (I believe it’s only an error to the <code>print</code> call, which is called by default). Wrap the code with <code>try</code>:</p>
<pre class="r"><code>fit_pruned &lt;- try(prune.tree(fitfull, best=10))
plot(fit_pruned)
text(fit_pruned)</code></pre>
<p><img src="/tutorials/cart_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
