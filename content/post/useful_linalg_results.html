---
title: "Some Linear Algebra For DSCI 563 Lab4"
date: "March 10, 2017"
output: html_document
---



<p>This document outlines some Linear Algebra results that you’ll need to know for the DSCI 563 Lab4 assignment. You can take these as given – you don’t have to prove them.</p>
<div id="covariance-rule" class="section level2">
<h2>Covariance Rule</h2>
<p>The variance-covariance matrix (or, just <em>covariance matrix</em> for short) of a random vector <span class="math inline">\(\boldsymbol{X}\)</span> is denoted <span class="math inline">\(\text{Cov}(\boldsymbol{X})\)</span>. Now, if <span class="math inline">\(A\)</span> is a matrix/vector, then the following rule holds: <span class="math display">\[ \text{Cov}(A\boldsymbol{X}) = A \text{Cov}(\boldsymbol{X}) A ^{\top}, \]</span> where <span class="math inline">\(A ^ {\top}\)</span> denotes the transpose of <span class="math inline">\(A\)</span>.</p>
</div>
<div id="orthogonal-matrices" class="section level2">
<h2>Orthogonal Matrices</h2>
<p>An orthogonal matrix <span class="math inline">\(A\)</span> is defined such that <span class="math inline">\(A\)</span> is square, and <span class="math display">\[ A^{\top} A = A A^{\top} = I, \]</span> where <span class="math inline">\(I\)</span> is the identity matrix. That is, <span class="math inline">\(A\)</span> and <span class="math inline">\(A^{\top}\)</span> are inverses.</p>
</div>
<div id="spectral-decomposition" class="section level2">
<h2>Spectral Decomposition</h2>
<p>Let <span class="math inline">\(\Sigma\)</span> be a covariance matrix. That is, it’s a square matrix that’s positive definite. Then, <span class="math inline">\(\Sigma\)</span> can be decomposed/reconstructed by its eigenvalues and eigenvectors. Specifically, if <span class="math inline">\(U\)</span> is a matrix whose columns are the eigenvectors of <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix with the corresponding eigenvectors along the diagonal, then <span class="math display">\[ \Sigma = U \Lambda U^{\top}. \]</span></p>
<p>Further, <span class="math inline">\(U\)</span> is orthogonal.</p>
</div>
<div id="diagonal-matrices" class="section level2">
<h2>Diagonal Matrices</h2>
<p>Suppose <span class="math inline">\(A\)</span> is a square diagonal matrix. Then <span class="math inline">\(A^{\top} = A\)</span>. Also, if <span class="math inline">\(a \in \mathbb{R}\)</span>, then <span class="math inline">\(A^a\)</span> is a diagonal matrix where the <span class="math inline">\(i\)</span>’th diagonal entry is the <span class="math inline">\(i\)</span>’th diagonal entry of <span class="math inline">\(A\)</span> raised to the power of <span class="math inline">\(a\)</span>.</p>
</div>
<div id="transpose-of-a-matrix-product" class="section level2">
<h2>Transpose of a Matrix Product</h2>
<p>Suppose <span class="math inline">\(A_1, \ldots, A_k\)</span> are <span class="math inline">\(k\)</span> matrices. Then, the transpose of their product can be found by reversing the order of the product, and transposing each matrix: <span class="math display">\[ \left(\prod_{i=1}^{k}A_{i}\right)^{\top}=\prod_{i=1}^{k}A_{k-i+1}^{\top}. \]</span></p>
</div>
<div id="summability-of-powers" class="section level2">
<h2>Summability of Powers</h2>
<p>Let <span class="math inline">\(a_1, \ldots, a_k\)</span> be real numbers, and <span class="math inline">\(A\)</span> a square matrix. If <span class="math inline">\(A^{a_i}\)</span> for <span class="math inline">\(i=1,\ldots,k\)</span> exist, then <span class="math display">\[ \prod_{i=1}^{k}A^{a_{i}}=A^{\sum_{i=1}^{k}a_{i}}. \]</span></p>
</div>
